{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import CFG, ChartParser\n",
    "from random import choice\n",
    "from nltk.parse.generate import generate\n",
    "import gensim\n",
    "import os\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "import random\n",
    "from matplotlib import pyplot\n",
    "import copy\n",
    "import io\n",
    "import pdb\n",
    "NUM_ANALOGS = 4\n",
    "N_COMPONENTS = 10\n",
    "PER_ANALOG = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_word_embeddings(vec_dict,n_components=10):\n",
    "    X_train = []\n",
    "    X_train_names = []\n",
    "    for x in vec_dict:\n",
    "            X_train.append(vec_dict[x])\n",
    "            X_train_names.append(x)\n",
    "\n",
    "    X_train = np.asarray(X_train)\n",
    "    pca_embeddings = {}\n",
    "\n",
    "    # PCA to get Top Components\n",
    "    pca =  PCA(n_components = len(X_train_names))\n",
    "    X_train = X_train - np.mean(X_train)\n",
    "    X_fit = pca.fit_transform(X_train)\n",
    "    U1 = pca.components_\n",
    "\n",
    "    z = []\n",
    "\n",
    "    # Removing Projections on Top Components\n",
    "    for i, x in enumerate(X_train):\n",
    "        for u in U1[0:7]:        \n",
    "                x = x - np.dot(u.transpose(),x) * u \n",
    "        z.append(x)\n",
    "\n",
    "    z = np.asarray(z)\n",
    "\n",
    "    # PCA Dim Reduction\n",
    "    pca =  PCA(n_components = n_components)\n",
    "    X_train = z - np.mean(z)\n",
    "    X_new_final = pca.fit_transform(X_train)\n",
    "\n",
    "\n",
    "    # PCA to do Post-Processing Again\n",
    "    pca =  PCA(n_components = n_components)\n",
    "    X_new = X_new_final - np.mean(X_new_final)\n",
    "    X_new = pca.fit_transform(X_new)\n",
    "    Ufit = pca.components_\n",
    "\n",
    "    X_new_final = X_new_final - np.mean(X_new_final)\n",
    "\n",
    "    final_pca_embeddings = {}\n",
    "\n",
    "    for i, x in enumerate(X_train_names):\n",
    "            final_pca_embeddings[x] = X_new_final[i]\n",
    "            #for u in Ufit[0:7]:\n",
    "            #    final_pca_embeddings[x] = final_pca_embeddings[x] - np.dot(u.transpose(),final_pca_embeddings[x]) * u \n",
    "\n",
    "    return final_pca_embeddings#X_new_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is present in variable - stimuli\n",
    "stimuli_path = str(os.path.dirname(os.getcwd())) + '\\\\data\\\\analog_3_stimuli.py'\n",
    "f = open(stimuli_path, 'r')\n",
    "f.seek(0)  # to get to the beginning of the file.\n",
    "symstring = ''\n",
    "for line in f:\n",
    "    symstring += line\n",
    "exec symstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_embeddings(vecs, names,save_path=None):\n",
    "    X = vecs\n",
    "    #pca = PCA(n_components=2)\n",
    "    #result = pca.fit_transform(X)\n",
    "    ## create a scatter plot of the projection\n",
    "    #pyplot.scatter(result[:, 0], result[:, 1])\n",
    "    \n",
    "    #for i, word in enumerate(names):\n",
    "    #    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "    #pyplot.show()\n",
    "    \n",
    "    #cos_sim = np.dot(X, np.transpose(X))/(np.linalg.norm(X)*np.linalg.norm(X))\n",
    "    cos_sim = np.corrcoef(X)\n",
    "    sns_heatmap = sns.heatmap(cos_sim, linewidth=0.5,xticklabels=names, yticklabels=names)\n",
    "    fig = sns_heatmap.get_figure()\n",
    "    fig.savefig(save_path)\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddings(embed_loc, wrd_list, embed_dims):\n",
    "    \"\"\"\n",
    "    Gives embedding for each word in wrd_list\n",
    "    Parameters\n",
    "    ----------\n",
    "    model:\t\tWord2vec model\n",
    "    wrd_list:\tList of words for which embedding is required\n",
    "    embed_dims:\tDimension of the embedding\n",
    "    Returns\n",
    "    -------\n",
    "    embed_matrix:\t(len(wrd_list) x embed_dims) matrix containing embedding for each word in wrd_list in the same order\n",
    "    \"\"\"\n",
    "    embed_list = []\n",
    "\n",
    "    wrd2embed = {}\n",
    "    for line in io.open(embed_loc, encoding='utf-8'):\n",
    "        data = line.strip().split(' ')\n",
    "\n",
    "        # wrd, embed = data[0], data[1:]\n",
    "\n",
    "        # Some words may be separated by space (telephone numbers, for example).\n",
    "        # It's more robust to load data as follows.\n",
    "        embed = data[-1*embed_dims: ] \n",
    "        wrd   = ' '.join(data[: -1*embed_dims])\n",
    "\n",
    "        embed = list(map(float, embed))\n",
    "        wrd2embed[wrd] = embed\n",
    "\n",
    "    embeddings = {}\n",
    "    for wrd in wrd_list:\n",
    "        embeddings[wrd] = wrd2embed[wrd]\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\karkau\\\\Documents\\\\karthikeya\\\\Dora\\\\BrPong_1\\\\workspace\\\\karthikeya'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.dirname(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec(sentences,model_type='custom', reduced_dim=10, normalised=True):\n",
    "    # first split all sentences and get unique words\n",
    "    unique_words = [sentence.split(' ') for sentence in sentences]\n",
    "    unique_words = [word.lower() for sentence in unique_words for word in sentence]\n",
    "    unique_words = list(set(unique_words))\n",
    "    # put these words into model and get embeddings\n",
    "    if model_type == 'custom':\n",
    "        # for custom model, give list of sentences\n",
    "        all_sentences = [sentence.split(' ') for sentence in sentences]\n",
    "        model_custom = gensim.models.Word2Vec(all_sentences, min_count=1,size=reduced_dim,sg=0) # sg= 1 - Skipgram, 0 - CBOW\n",
    "        vec_dict = {}\n",
    "        for word in unique_words:\n",
    "            vec_dict[word] = model_custom[word]\n",
    "    elif model_type == 'google':\n",
    "        # for google word2vec give all unique words\n",
    "        google_word2vec_path = 'C:\\\\Users\\\\karkau\\\\Documents\\\\karthikeya\\\\GoogleNews-vectors-negative300.bin'\n",
    "        model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "            google_word2vec_path, binary=True)\n",
    "        vec_dict = {}\n",
    "        for word in unique_words:\n",
    "            vec_dict[word] = model[word]\n",
    "        # reduce dimension size for the google 300b model\n",
    "        vec_dict = reduce_word_embeddings(vec_dict,n_components=reduced_dim)\n",
    "    elif model_type == 'WordGCN':\n",
    "        model_path = 'C:\\\\Users\\\\karkau\\\\Documents\\\\karthikeya\\\\syngcn_embeddings.txt'\n",
    "        vec_dict = getEmbeddings(model_path, unique_words, 300)\n",
    "        vec_dict = reduce_word_embeddings(vec_dict, n_components=reduced_dim)\n",
    "        \n",
    "    # power normalise vectors, then put them into 0,1 range\n",
    "    word2vec_mat = np.zeros((len(unique_words),reduced_dim))\n",
    "    for ind,word in enumerate(unique_words):\n",
    "        word2vec_mat[ind,:] = vec_dict[word]#/np.linalg.norm(vec_dict[word])\n",
    "    # now 0-1 normalise by dimension\n",
    "    for ind in range(word2vec_mat.shape[1]):\n",
    "        word2vec_mat[:,ind] = (word2vec_mat[:,ind] - min(word2vec_mat[:,ind]))/(max(word2vec_mat[:,ind]) - min(word2vec_mat[:,ind]))\n",
    "    # power normalise again\n",
    "    for ind,word in enumerate(unique_words):\n",
    "        word2vec_mat[ind,:] = word2vec_mat[ind,:]/np.linalg.norm(word2vec_mat[ind,:])\n",
    "    \n",
    "    # convert back to dict\n",
    "    vec_dict = {}\n",
    "    for ind,word in enumerate(unique_words):\n",
    "        vec_dict[word] = word2vec_mat[ind,:]\n",
    "    return [vec_dict, word2vec_mat, unique_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dogs chase cats',\n",
       " 'hungry dogs bark',\n",
       " 'dogs growl',\n",
       " 'dogs think big dogs bite cats',\n",
       " 'big dogs bite small cats',\n",
       " 'angry hungry dogs growl at strangers',\n",
       " 'kitten stay away from water',\n",
       " 'cute fluffy cats purr softly',\n",
       " 'lions hunt deer',\n",
       " 'angry lions roar',\n",
       " 'deer jump',\n",
       " 'lions know big lions hunt deer',\n",
       " 'bulky lions stalk fast deer',\n",
       " 'majestic male lions roar in anger',\n",
       " 'deer run quickly into darkness',\n",
       " 'beautiful timid deer graze happily',\n",
       " 'eagles eat worms',\n",
       " 'majestic eagles glide',\n",
       " 'eagles fly',\n",
       " 'worms say early worms spot eagles',\n",
       " 'wise eagles catch easy worms',\n",
       " 'old bald eagles scan the sky',\n",
       " 'worms crawl slowly from under',\n",
       " 'slow dull worms wander aimlessly',\n",
       " 'snakes eat rats',\n",
       " 'angry snakes hiss',\n",
       " 'snakes slither',\n",
       " 'snakes think big snakes eat rats',\n",
       " 'cunning snakes love juicy rats',\n",
       " 'slender spotted snakes rattle at danger',\n",
       " 'rats wobble quietly towards traps',\n",
       " 'fat hairy rats scurry about']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stimuli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8968908589299879, 0.9919208743324607, 0.9017976931433493, 0.9016167440826438, 0.8290740386611384, 0.808014109088545)\n"
     ]
    }
   ],
   "source": [
    "[word2vec_dict, word2vec_mat, unique_words] = get_word2vec(sentences=stimuli,model_type='WordGCN',reduced_dim=N_COMPONENTS)\n",
    "reduced_embeddings = word2vec_dict\n",
    "sentences = stimuli\n",
    "print(np.dot(reduced_embeddings['hungry'],reduced_embeddings['dogs']),\n",
    "np.dot(reduced_embeddings['dogs'],reduced_embeddings['cats']),\n",
    "np.dot(reduced_embeddings['dogs'],reduced_embeddings['jump']),\n",
    "np.dot(reduced_embeddings['dogs'],reduced_embeddings['lions']),\n",
    "np.dot(reduced_embeddings['bite'],reduced_embeddings['fast']),\n",
    "np.dot(reduced_embeddings['bite'],reduced_embeddings['fly']))\n",
    "#vis_embeddings(word2vec_mat,unique_words)\n",
    "#print(np.dot(word2vec_dict['slender'],word2vec_dict['cats']))\n",
    "#print(np.dot(word2vec_dict['nap'],word2vec_dict['fly']))\n",
    "#print(np.dot(word2vec_dict['cats'],word2vec_dict['fly']))\n",
    "#print(np.dot(word2vec_dict['bite'],word2vec_dict['fly']))\n",
    "#print(np.dot(word2vec_dict['small'],word2vec_dict['big']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_empty_vec(n_dim):\n",
    "    sem = []#[['e0',1.0]] # change to [] and range(0,n_dim) to put empty vector. Here we need a specifier, and therefore not empty vec\n",
    "    for i in range(0,n_dim):\n",
    "        sem.append(['e' + str(i), 0.0])\n",
    "    sem.append(['e' + str(n_dim), 1.0])\n",
    "    return sem\n",
    "def get_sem(word2vec_dict,word):\n",
    "    vec = word2vec_dict[word]\n",
    "    sem = []\n",
    "    for i in range(len(vec)):\n",
    "        sem.append(['e' + str(i), vec[i]])\n",
    "    sem.append(['e' + str(len(vec)), 0.0])\n",
    "    return sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "fname must be a PathLike or file handle",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-bc0c799784d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;31m#np.savetxt(str(os.path.dirname(os.getcwd())) + '\\\\data\\\\embeddings\\\\cbow_embeddings.csv',vis_matrix)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;31m#np.savetxt(str(os.path.dirname(os.getcwd())) + '\\\\data\\\\embeddings\\\\words.csv',np.array(vis_array_words),fmt=\"%s\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m \u001b[0mvis_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvis_matrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvis_array_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#, save_path=str(os.path.dirname(os.getcwd())) + '\\\\data\\\\word_embeddings_vis_cbow.png')\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-8399f9cabad1>\u001b[0m in \u001b[0;36mvis_embeddings\u001b[1;34m(vecs, names, save_path)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0msns_heatmap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcos_sim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mxticklabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myticklabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msns_heatmap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_figure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\karkau\\AppData\\Local\\Continuum\\anaconda2\\envs\\Dora\\lib\\site-packages\\matplotlib\\figure.pyc\u001b[0m in \u001b[0;36msavefig\u001b[1;34m(self, fname, **kwargs)\u001b[0m\n\u001b[0;32m   2060\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_frameon\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframeon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2061\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2062\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2063\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2064\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mframeon\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\karkau\\AppData\\Local\\Continuum\\anaconda2\\envs\\Dora\\lib\\site-packages\\matplotlib\\backend_bases.pyc\u001b[0m in \u001b[0;36mprint_figure\u001b[1;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, **kwargs)\u001b[0m\n\u001b[0;32m   2261\u001b[0m                 \u001b[0morientation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2262\u001b[0m                 \u001b[0mbbox_inches_restore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_bbox_inches_restore\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2263\u001b[1;33m                 **kwargs)\n\u001b[0m\u001b[0;32m   2264\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2265\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mrestore_bbox\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\karkau\\AppData\\Local\\Continuum\\anaconda2\\envs\\Dora\\lib\\site-packages\\matplotlib\\backends\\backend_agg.pyc\u001b[0m in \u001b[0;36mprint_png\u001b[1;34m(self, filename_or_obj, *args, **kwargs)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_file_cm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m                 _png.write_png(renderer._renderer, fh,\n\u001b[0;32m    532\u001b[0m                                self.figure.dpi, metadata=metadata)\n",
      "\u001b[1;32mC:\\Users\\karkau\\AppData\\Local\\Continuum\\anaconda2\\envs\\Dora\\lib\\contextlib.pyc\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"generator didn't yield\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\karkau\\AppData\\Local\\Continuum\\anaconda2\\envs\\Dora\\lib\\site-packages\\matplotlib\\cbook\\__init__.pyc\u001b[0m in \u001b[0;36mopen_file_cm\u001b[1;34m(path_or_file, mode, encoding)\u001b[0m\n\u001b[0;32m    624\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mopen_file_cm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m     \u001b[1;34mr\"\"\"Pass through file objects and context-manage `.PathLike`\\s.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 626\u001b[1;33m     \u001b[0mfh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_filehandle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    627\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mopened\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mfh\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\karkau\\AppData\\Local\\Continuum\\anaconda2\\envs\\Dora\\lib\\site-packages\\matplotlib\\cbook\\__init__.pyc\u001b[0m in \u001b[0;36mto_filehandle\u001b[1;34m(fname, flag, return_opened, encoding)\u001b[0m\n\u001b[0;32m    615\u001b[0m         \u001b[0mopened\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 617\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'fname must be a PathLike or file handle'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    618\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_opened\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopened\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: fname must be a PathLike or file handle"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test on two conditions - \n",
    "# words arranged within a single analog\n",
    "# words arranged by lexical content across analogs\n",
    "for i in [0]:#range(NUM_ANALOGS):\n",
    "    analog_words = {}\n",
    "    nouns = {}\n",
    "    verbs = {}\n",
    "    i_verbs = {}\n",
    "    adj = {}\n",
    "    prep = {}\n",
    "    adverbs = {}\n",
    "    for j in range(PER_ANALOG):\n",
    "        sent = stimuli[i*PER_ANALOG + j]\n",
    "        sent_words = sent.split(' ')\n",
    "        for w in sent_words:\n",
    "            analog_words[w] = reduced_embeddings[w]        \n",
    "        # put words into different lexical categories  \n",
    "        if j == 0:\n",
    "            nouns[sent_words[0]],verbs[sent_words[1]],nouns[sent_words[2]] = reduced_embeddings[sent_words[0]], \\\n",
    "                reduced_embeddings[sent_words[1]],reduced_embeddings[sent_words[2]]\n",
    "        if j == 1:\n",
    "            adj[sent_words[0]],i_verbs[sent_words[2]] = reduced_embeddings[sent_words[0]],reduced_embeddings[sent_words[0]]\n",
    "        if j == 2:\n",
    "            i_verbs[sent_words[1]] = reduced_embeddings[sent_words[1]]\n",
    "        if j == 3:\n",
    "            verbs[sent_words[1]],adj[sent_words[2]],verbs[sent_words[4]] = reduced_embeddings[sent_words[1]], \\\n",
    "                reduced_embeddings[sent_words[2]],reduced_embeddings[sent_words[4]]\n",
    "        if j == 4:\n",
    "            adj[sent_words[0]],verbs[sent_words[2]],adj[sent_words[3]] = reduced_embeddings[sent_words[0]], \\\n",
    "                reduced_embeddings[sent_words[2]],reduced_embeddings[sent_words[3]]\n",
    "        if j == 5:\n",
    "            adj[sent_words[0]],adj[sent_words[1]],i_verbs[sent_words[3]],prep[sent_words[4]],nouns[sent_words[5]] = reduced_embeddings[sent_words[0]], \\\n",
    "                reduced_embeddings[sent_words[1]],reduced_embeddings[sent_words[3]],reduced_embeddings[sent_words[4]],reduced_embeddings[sent_words[5]]\n",
    "        if j == 6:\n",
    "            nouns[sent_words[0]],verbs[sent_words[1]],adverbs[sent_words[2]],prep[sent_words[3]],nouns[sent_words[4]] = reduced_embeddings[sent_words[0]], \\\n",
    "                reduced_embeddings[sent_words[1]],reduced_embeddings[sent_words[2]],reduced_embeddings[sent_words[3]],reduced_embeddings[sent_words[4]]\n",
    "        if j == 7:\n",
    "            adj[sent_words[0]],adj[sent_words[1]],verbs[sent_words[3]],adverbs[sent_words[4]] = reduced_embeddings[sent_words[0]], \\\n",
    "                reduced_embeddings[sent_words[1]],reduced_embeddings[sent_words[3]],reduced_embeddings[sent_words[4]]            \n",
    "# now arrange them into a matrix, and send to visualisation function\n",
    "all_dicts = [nouns, verbs, i_verbs, adj, prep, adverbs]\n",
    "vis_matrix = np.zeros((len(analog_words.keys()),N_COMPONENTS))\n",
    "vis_array_words = []\n",
    "ind = 0\n",
    "for d in all_dicts:\n",
    "    for _,k in enumerate(d.keys()):\n",
    "        vis_matrix[ind,:] = d[k]\n",
    "        vis_array_words.append(k)\n",
    "        ind = ind + 1\n",
    "#np.savetxt(str(os.path.dirname(os.getcwd())) + '\\\\data\\\\embeddings\\\\cbow_embeddings.csv',vis_matrix)\n",
    "#np.savetxt(str(os.path.dirname(os.getcwd())) + '\\\\data\\\\embeddings\\\\words.csv',np.array(vis_array_words),fmt=\"%s\")\n",
    "vis_embeddings(vis_matrix,vis_array_words)#, save_path=str(os.path.dirname(os.getcwd())) + '\\\\data\\\\word_embeddings_vis_cbow.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22L, 10L)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced_embeddings contains the word2vec\n",
    "# type-1 with separate det for empty predicate\n",
    "import copy\n",
    "random.seed(999)\n",
    "symProps = []\n",
    "# for type = [0,1,2,3], [9,10,11]\n",
    "prototype_1={'name': 'non_exist', \n",
    "             'RBs': [{'pred_name': 'non_exist', 'pred_sem': [], \n",
    "                      'higher_order': False, \n",
    "                      'object_name': 'obj', 'object_sem': [], 'P': 'non_exist'}, \n",
    "                     {'pred_name': 'non_exist', 'pred_sem': [], \n",
    "                      'higher_order': False, \n",
    "                      'object_name': 'obj', 'object_sem': [], 'P': 'non_exist'}], \n",
    "             'set': 'memory', 'analog': None}\n",
    "# for simple two word props like \"happy cats\" present in type = [4,5,6,7,8]\n",
    "# 'name' is simply the proposition number + '.' + '1'\n",
    "prototype_2 = {'name': 'non_exist', \n",
    "             'RBs': [{'pred_name': 'non_exist', 'pred_sem': [], \n",
    "                      'higher_order': False, \n",
    "                      'object_name': 'obj', 'object_sem': [], 'P': 'non_exist'}],\n",
    "              'set':'memory','analog':None}\n",
    "# for 3 word props in [4,5,6,7,8] containing prototype_2\n",
    "prototype_3 = {'name': 'non_exist', \n",
    "             'RBs': [{'pred_name': 'non_exist', 'pred_sem': [], \n",
    "                      'higher_order': True, \n",
    "                      'object_name': 'obj', 'object_sem': [], 'P': 'non_exist'}, \n",
    "                     {'pred_name': 'non_exist', 'pred_sem': [], \n",
    "                      'higher_order': False, \n",
    "                      'object_name': 'non_exist', 'object_sem': [], 'P': 'non_exist'}], \n",
    "             'set': 'memory', 'analog': None}\n",
    "# for 6 word props in [16,17,18]\n",
    "prototype_4 = {'name': 'non_exist', \n",
    "             'RBs': [{'pred_name': 'non_exist', 'pred_sem': [], \n",
    "                      'higher_order': False, \n",
    "                      'object_name': 'obj', 'object_sem': None, 'P': 'non_exist'}, \n",
    "                     {'pred_name': 'non_exist', 'pred_sem': [], \n",
    "                      'higher_order': True, \n",
    "                      'object_name': 'obj', 'object_sem': None, 'P': 'non_exist'}], \n",
    "             'set': 'memory', 'analog': None}\n",
    "#  for 5 word props in [19,20] \n",
    "prototype_5 = {'name': 'non_exist', \n",
    "             'RBs': [{'pred_name': 'non_exist', 'pred_sem': [], \n",
    "                      'higher_order': True, \n",
    "                      'object_name': 'obj', 'object_sem': None, 'P': 'non_exist'}, \n",
    "                     {'pred_name': 'non_exist', 'pred_sem': [], \n",
    "                      'higher_order': True, \n",
    "                      'object_name': 'obj', 'object_sem': None, 'P': 'non_exist'}], \n",
    "             'set': 'memory', 'analog': None}\n",
    "#word2vec_dict = reduced_embeddings\n",
    "for analog in range(0,NUM_ANALOGS): # \n",
    "    for proposition in range(0,PER_ANALOG):\n",
    "        current_sentence = sentences[analog*PER_ANALOG + proposition]\n",
    "        current_sentence = current_sentence.split(' ')\n",
    "        # depending on proposition number, decompose propositions into their RB's\n",
    "        if (proposition >=0 and proposition < 4) or (proposition >=9 and proposition < 12):\n",
    "            prop_dict = copy.deepcopy(prototype_1)\n",
    "            prop_dict['RBs'][0]['pred_name'] = 'det' # strange behaviour - predicate has to exist to form RB with P in object\n",
    "            prop_dict['RBs'][0]['pred_sem'] = get_empty_vec(n_dim=N_COMPONENTS)\n",
    "            prop_dict['RBs'][0]['object_name'] = current_sentence[0]\n",
    "            prop_dict['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[0])\n",
    "            prop_dict['RBs'][1]['pred_name'] = current_sentence[1]\n",
    "            prop_dict['RBs'][1]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[1])\n",
    "            prop_dict['RBs'][1]['object_name'] = current_sentence[2]\n",
    "            prop_dict['RBs'][1]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[2])\n",
    "            prop_dict['analog'] = analog\n",
    "            #prop_dict['set'] = random.choice(['recipient','driver'])\n",
    "            prop_dict['name'] = 'P' + str(proposition)\n",
    "            symProps.append(prop_dict)\n",
    "        elif proposition >=4 and proposition < 8:\n",
    "            # first encode lower order prop\n",
    "            prop_dict_1 = copy.deepcopy(prototype_2)\n",
    "            prop_dict_1['name'] = 'P' + str(proposition) + '.1'\n",
    "            prop_dict_1['RBs'][0]['pred_name'] = current_sentence[0]\n",
    "            prop_dict_1['RBs'][0]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[0])\n",
    "            prop_dict_1['RBs'][0]['object_name'] = current_sentence[1]\n",
    "            prop_dict_1['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[1])\n",
    "            prop_dict_1['analog'] = analog\n",
    "            #prop_dict_1['set'] = random.choice(['recipient','driver'])\n",
    "            symProps.append(prop_dict_1)\n",
    "            # now encode the higher order sem\n",
    "            prop_dict = copy.deepcopy(prototype_3)\n",
    "            prop_dict['name'] = 'P' + str(proposition)\n",
    "            prop_dict['RBs'][0]['pred_name'] = 'det' # strange behaviour - predicate has to exist to form RB with P in object\n",
    "            prop_dict['RBs'][0]['pred_sem'] = get_empty_vec(n_dim=N_COMPONENTS)\n",
    "            prop_dict['RBs'][0]['object_name'] = 'non_exist'#current_sentence[0] + current_sentence[1]\n",
    "            prop_dict['RBs'][0]['object_sem'] = []\n",
    "            prop_dict['RBs'][0]['P'] = 'P' + str(proposition) + '.1'\n",
    "            prop_dict['RBs'][1]['pred_name'] = current_sentence[2]\n",
    "            prop_dict['RBs'][1]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[2])\n",
    "            prop_dict['analog'] = analog\n",
    "            #prop_dict['set'] = random.choice(['recipient','driver'])\n",
    "            symProps.append(prop_dict)\n",
    "        elif proposition >= 12 and proposition < 16:\n",
    "            prop_dict = copy.deepcopy(prototype_1)\n",
    "            prop_dict['RBs'][0]['pred_name'] = 'det' # strange behaviour - predicate has to exist to form RB with P in object\n",
    "            prop_dict['RBs'][0]['pred_sem'] = get_empty_vec(n_dim=N_COMPONENTS)\n",
    "            prop_dict['RBs'][0]['object_name'] = current_sentence[0]\n",
    "            prop_dict['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[0])\n",
    "            prop_dict['RBs'][1]['pred_name'] = current_sentence[1]\n",
    "            prop_dict['RBs'][1]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[1])\n",
    "            prop_dict['analog'] = analog\n",
    "            prop_dict['name'] = 'P' + str(proposition)\n",
    "            symProps.append(prop_dict)\n",
    "        elif proposition >= 16 and proposition < 19:\n",
    "            prop_dict_1 = copy.deepcopy(prototype_2)\n",
    "            prop_dict_1['name'] = 'P' + str(proposition) + '.1'\n",
    "            prop_dict_1['RBs'][0]['pred_name'] = current_sentence[2]\n",
    "            prop_dict_1['RBs'][0]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[2])\n",
    "            prop_dict_1['RBs'][0]['object_name'] = current_sentence[3]\n",
    "            prop_dict_1['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[3])\n",
    "            prop_dict_1['analog'] = analog\n",
    "            symProps.append(prop_dict_1)\n",
    "            # add higher order prop\n",
    "            prop_dict_2 = copy.deepcopy(prototype_3)\n",
    "            prop_dict_2['name'] = 'P' + str(proposition) + '.2'\n",
    "            prop_dict_2['RBs'][0]['P'] = 'P' + str(proposition) + '.1'\n",
    "            prop_dict_2['RBs'][0]['pred_name'] = 'det'\n",
    "            prop_dict_2['RBs'][0]['pred_sem'] = get_empty_vec(n_dim=N_COMPONENTS)\n",
    "            prop_dict_2['RBs'][1]['pred_name'] = current_sentence[4]\n",
    "            prop_dict_2['RBs'][1]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[4])\n",
    "            prop_dict_2['RBs'][1]['object_name'] = current_sentence[5]\n",
    "            prop_dict_2['RBs'][1]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[5])\n",
    "            prop_dict_2['analog'] = analog\n",
    "            symProps.append(prop_dict_2)\n",
    "            # now add the highest order prop\n",
    "            prop_dict = copy.deepcopy(prototype_4)\n",
    "            prop_dict['name'] = 'P' + str(proposition)\n",
    "            prop_dict['RBs'][0]['pred_name'] = 'det'\n",
    "            prop_dict_2['RBs'][0]['pred_sem'] = get_empty_vec(n_dim=N_COMPONENTS)            \n",
    "            prop_dict['RBs'][0]['object_name'] = current_sentence[0]\n",
    "            prop_dict['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[0])\n",
    "            prop_dict['RBs'][1]['pred_name'] = current_sentence[1]\n",
    "            prop_dict['RBs'][1]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[1])\n",
    "            prop_dict['RBs'][1]['P'] = 'P' + str(proposition) + '.1'\n",
    "            prop_dict['analog'] = analog\n",
    "            prop_dict['name'] = 'P' + str(proposition)\n",
    "            symProps.append(prop_dict)\n",
    "        elif proposition >= 19 and proposition < 21:\n",
    "            prop_dict_1 = copy.deepcopy(prototype_2)\n",
    "            prop_dict_1['name'] = 'P' + str(proposition) + '.1'\n",
    "            prop_dict_1['RBs'][0]['pred_name'] = current_sentence[0]\n",
    "            prop_dict_1['RBs'][0]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[0])\n",
    "            prop_dict_1['RBs'][0]['object_name'] = current_sentence[1]\n",
    "            prop_dict_1['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[1])\n",
    "            prop_dict_1['analog'] = analog\n",
    "            symProps.append(prop_dict_1)\n",
    "            prop_dict_1 = copy.deepcopy(prototype_2)\n",
    "            prop_dict_1['name'] = 'P' + str(proposition) + '.2'\n",
    "            prop_dict_1['RBs'][0]['pred_name'] = current_sentence[3]\n",
    "            prop_dict_1['RBs'][0]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[3])\n",
    "            prop_dict_1['RBs'][0]['object_name'] = current_sentence[4]\n",
    "            prop_dict_1['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[4])\n",
    "            prop_dict_1['analog'] = analog\n",
    "            symProps.append(prop_dict_1)\n",
    "            prop_dict = copy.deepcopy(prototype_5)\n",
    "            prop_dict['name'] = 'P' + str(proposition)\n",
    "            prop_dict['RBs'][0]['P'] = 'P' + str(proposition) + '.1'\n",
    "            prop_dict['RBs'][0]['pred_name'] = 'det'\n",
    "            prop_dict['RBs'][0]['pred_sem'] = get_empty_vec(n_dim=N_COMPONENTS)\n",
    "            prop_dict['RBs'][1]['P'] = 'P' + str(proposition) + '.2'\n",
    "            prop_dict['RBs'][1]['pred_name'] = current_sentence[2]\n",
    "            prop_dict['RBs'][1]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[2])\n",
    "            prop_dict['analog'] = analog\n",
    "            symProps.append(prop_dict)\n",
    "        elif proposition >= 21 and proposition < 23:\n",
    "            prop_dict_1 = copy.deepcopy(prototype_2)\n",
    "            prop_dict_1['name'] = 'P' + str(proposition) + '.1'\n",
    "            prop_dict_1['RBs'][0]['pred_name'] = current_sentence[2]\n",
    "            prop_dict_1['RBs'][0]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[2])\n",
    "            prop_dict_1['RBs'][0]['object_name'] = current_sentence[3]\n",
    "            prop_dict_1['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[3])\n",
    "            prop_dict_1['analog'] = analog\n",
    "            symProps.append(prop_dict_1)\n",
    "            # add higher order prop\n",
    "            prop_dict = copy.deepcopy(prototype_4)\n",
    "            prop_dict['name'] = 'P' + str(proposition)\n",
    "            prop_dict['RBs'][0]['pred_name'] = 'det'\n",
    "            prop_dict['RBs'][0]['pred_sem'] = get_empty_vec(n_dim=N_COMPONENTS)\n",
    "            prop_dict['RBs'][0]['object_name'] = current_sentence[0]\n",
    "            prop_dict['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[0])\n",
    "            prop_dict['RBs'][1]['pred_name'] = current_sentence[1]\n",
    "            prop_dict['RBs'][1]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[1])\n",
    "            prop_dict['RBs'][1]['P'] = 'P' + str(proposition) + '.1'\n",
    "            prop_dict['analog'] = analog\n",
    "            symProps.append(prop_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type 2 with filled in slot like structure. ie., no empty predicate \n",
    "# reduced_embeddings contains the word2vec\n",
    "random.seed(999)\n",
    "symProps = []\n",
    "# for type = [0,1,2,3], [9,10,11]\n",
    "prototype_1={'name': 'non_exist', \n",
    "             'RBs': [{'pred_name': 'non_exist', 'pred_sem': [], \n",
    "                      'higher_order': False, \n",
    "                      'object_name': 'obj', 'object_sem': [], 'P': 'non_exist'}, \n",
    "                     {'pred_name': 'non_exist', 'pred_sem': [], \n",
    "                      'higher_order': False, \n",
    "                      'object_name': 'obj', 'object_sem': [], 'P': 'non_exist'}], \n",
    "             'set': 'memory', 'analog': None}\n",
    "# for 3 word props in [4,5,6,7,8] containing prototype_1\n",
    "prototype_2 = {'name': 'non_exist', \n",
    "             'RBs': [{'pred_name': 'non_exist', 'pred_sem': [], \n",
    "                      'higher_order': False, \n",
    "                      'object_name': 'obj', 'object_sem': [], 'P': 'non_exist'}, \n",
    "                     {'pred_name': 'non_exist', 'pred_sem': [], \n",
    "                      'higher_order': True, \n",
    "                      'object_name': 'non_exist', 'object_sem': [], 'P': 'non_exist'}], \n",
    "             'set': 'memory', 'analog': None}\n",
    "# for 2 word prop = single RB\n",
    "prototype_3 = {'name': 'non_exist', \n",
    "             'RBs': [{'pred_name': 'non_exist', 'pred_sem': [], \n",
    "                      'higher_order': False, \n",
    "                      'object_name': 'obj', 'object_sem': [], 'P': 'non_exist'}], \n",
    "             'set': 'memory', 'analog': None}\n",
    "\n",
    "#word2vec_dict = reduced_embeddings\n",
    "for analog in range(0,NUM_ANALOGS): # \n",
    "    for proposition in range(0,PER_ANALOG):\n",
    "        current_sentence = sentences[analog*PER_ANALOG + proposition]\n",
    "        current_sentence = current_sentence.split(' ')\n",
    "        # depending on proposition number, decompose propositions into their RB's\n",
    "        if (proposition >=0 and proposition < 4) or (proposition >=9 and proposition < 12): # == [- dogs][bite cats]\n",
    "            prop_dict = copy.deepcopy(prototype_1)\n",
    "            prop_dict['RBs'][0]['object_name'] = current_sentence[0]\n",
    "            prop_dict['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[0])\n",
    "            prop_dict['RBs'][1]['pred_name'] = current_sentence[1]\n",
    "            prop_dict['RBs'][1]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[1])\n",
    "            prop_dict['RBs'][1]['object_name'] = current_sentence[2]\n",
    "            prop_dict['RBs'][1]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[2])\n",
    "            prop_dict['analog'] = analog\n",
    "            prop_dict['name'] = 'P' + str(analog) + str(proposition)\n",
    "            symProps.append(prop_dict)\n",
    "        elif proposition >=4 and proposition < 9: # == [happy cats][sleep -]\n",
    "            prop_dict = copy.deepcopy(prototype_1)\n",
    "            prop_dict['RBs'][0]['pred_name'] = current_sentence[0] \n",
    "            prop_dict['RBs'][0]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[1])\n",
    "            prop_dict['RBs'][0]['object_name'] = current_sentence[1]\n",
    "            prop_dict['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[1])\n",
    "            prop_dict['RBs'][1]['pred_name'] = current_sentence[2]\n",
    "            prop_dict['RBs'][1]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[2])\n",
    "            prop_dict['analog'] = analog\n",
    "            prop_dict['name'] = 'P' + str(analog) + str(proposition)\n",
    "            symProps.append(prop_dict)\n",
    "        elif proposition >= 12 and proposition < 16: # == [- eagles][fly -]\n",
    "            prop_dict = copy.deepcopy(prototype_1)\n",
    "            prop_dict['RBs'][0]['object_name'] = current_sentence[0]\n",
    "            prop_dict['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[0])\n",
    "            prop_dict['RBs'][1]['pred_name'] = current_sentence[1]\n",
    "            prop_dict['RBs'][1]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[1])\n",
    "            prop_dict['analog'] = analog\n",
    "            prop_dict['name'] = 'P' + str(analog) + str(proposition)\n",
    "            symProps.append(prop_dict)\n",
    "        '''elif proposition >= 16 and proposition < 19: # == [- dogs][think [[big dogs][bite cats]]\n",
    "            prop_dict_1 = copy.deepcopy(prototype_1) # == [[big dogs][bite cats]]\n",
    "            prop_dict_1['RBs'][0]['pred_name'] = current_sentence[2]\n",
    "            prop_dict_1['RBs'][0]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[2])\n",
    "            prop_dict_1['RBs'][0]['object_name'] = current_sentence[3]\n",
    "            prop_dict_1['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[3])\n",
    "            prop_dict_1['RBs'][1]['pred_name'] = current_sentence[4]\n",
    "            prop_dict_1['RBs'][1]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[4])\n",
    "            prop_dict_1['RBs'][1]['object_name'] = current_sentence[5]\n",
    "            prop_dict_1['RBs'][1]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[5])\n",
    "            prop_dict_1['analog'] = analog\n",
    "            prop_dict_1['name'] = 'P' + str(analog) + str(proposition) + '.1'\n",
    "            symProps.append(prop_dict_1)\n",
    "            # add higher order prop \n",
    "            prop_dict = copy.deepcopy(prototype_2) # == [- dogs][think P-N.1]\n",
    "            prop_dict['RBs'][0]['object_name'] = current_sentence[0]\n",
    "            prop_dict['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[0])\n",
    "            prop_dict['RBs'][1]['pred_name'] = current_sentence[1]\n",
    "            prop_dict['RBs'][1]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[1])\n",
    "            prop_dict['RBs'][1]['P'] = 'P' + str(analog) + str(proposition) + '.1'\n",
    "            prop_dict['analog'] = analog\n",
    "            prop_dict['name'] = 'P' + str(analog) + str(proposition)\n",
    "            symProps.append(prop_dict)\n",
    "        elif proposition >= 19 and proposition < 21: # == [big dogs][bite [small cats]]\n",
    "            prop_dict_1 = copy.deepcopy(prototype_3) # [small cats]\n",
    "            prop_dict_1['RBs'][0]['pred_name'] = current_sentence[3]\n",
    "            prop_dict_1['RBs'][0]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[3])\n",
    "            prop_dict_1['RBs'][0]['object_name'] = current_sentence[4]\n",
    "            prop_dict_1['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[4])\n",
    "            prop_dict_1['analog'] = analog\n",
    "            prop_dict_1['name'] = 'P' + str(analog) + str(proposition) + '.1'            \n",
    "            symProps.append(prop_dict_1)\n",
    "            prop_dict = copy.deepcopy(prototype_2) # == [big dogs][bite P-N.1]\n",
    "            prop_dict['RBs'][0]['pred_name'] = current_sentence[0]\n",
    "            prop_dict['RBs'][0]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[0])\n",
    "            prop_dict['RBs'][0]['object_name'] = current_sentence[1]\n",
    "            prop_dict['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[1])\n",
    "            prop_dict['RBs'][1]['pred_name'] = current_sentence[2]\n",
    "            prop_dict['RBs'][1]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[2])\n",
    "            prop_dict['RBs'][1]['P'] = 'P' + str(analog) + str(proposition) + '.1'\n",
    "            prop_dict['analog'] = analog\n",
    "            prop_dict['name'] = 'P' + str(analog) + str(proposition)\n",
    "            symProps.append(prop_dict)\n",
    "        elif proposition >= 21 and proposition < 23: # == [- lions][hunt [fast deer]]\n",
    "            prop_dict_1 = copy.deepcopy(prototype_3)\n",
    "            prop_dict_1['RBs'][0]['pred_name'] = current_sentence[2]\n",
    "            prop_dict_1['RBs'][0]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[2])\n",
    "            prop_dict_1['RBs'][0]['object_name'] = current_sentence[3]\n",
    "            prop_dict_1['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[3])\n",
    "            prop_dict_1['analog'] = analog\n",
    "            prop_dict_1['name'] = 'P' + str(analog) + str(proposition) + '.1'        \n",
    "            symProps.append(prop_dict_1)\n",
    "            # add higher order prop\n",
    "            prop_dict = copy.deepcopy(prototype_2)\n",
    "            prop_dict['RBs'][0]['object_name'] = current_sentence[0]\n",
    "            prop_dict['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[0])\n",
    "            prop_dict['RBs'][1]['pred_name'] = current_sentence[1]\n",
    "            prop_dict['RBs'][1]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[1])\n",
    "            prop_dict['RBs'][1]['P'] = 'P' + str(analog) + str(proposition) + '.1'\n",
    "            prop_dict['analog'] = analog\n",
    "            prop_dict['name'] = 'P' + str(analog) + str(proposition)            \n",
    "            symProps.append(prop_dict)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type 3 sym setup - 4 analogs consisting of 5 sentence types\n",
    "random.seed(999)\n",
    "symProps = []\n",
    "# for type = [0,1,2,3], [9,10,11]\n",
    "prototype_1={'name': 'non_exist', \n",
    "             'RBs': [{'pred_name': 'non_exist', 'pred_sem': [], \n",
    "                      'higher_order': False, \n",
    "                      'object_name': 'obj', 'object_sem': [], 'P': 'non_exist'}, \n",
    "                     {'pred_name': 'non_exist', 'pred_sem': [], \n",
    "                      'higher_order': False, \n",
    "                      'object_name': 'obj', 'object_sem': [], 'P': 'non_exist'}], \n",
    "             'set': 'memory', 'analog': None}\n",
    "# for 3 word props in [4,5,6,7,8] containing prototype_1\n",
    "prototype_2 = {'name': 'non_exist', \n",
    "             'RBs': [{'pred_name': 'non_exist', 'pred_sem': [], \n",
    "                      'higher_order': False, \n",
    "                      'object_name': 'obj', 'object_sem': [], 'P': 'non_exist'}, \n",
    "                     {'pred_name': 'non_exist', 'pred_sem': [], \n",
    "                      'higher_order': True, \n",
    "                      'object_name': 'non_exist', 'object_sem': [], 'P': 'non_exist'}], \n",
    "             'set': 'memory', 'analog': None}\n",
    "# for 2 word prop = single RB\n",
    "prototype_3 = {'name': 'non_exist', \n",
    "             'RBs': [{'pred_name': 'non_exist', 'pred_sem': [], \n",
    "                      'higher_order': False, \n",
    "                      'object_name': 'obj', 'object_sem': [], 'P': 'non_exist'}], \n",
    "             'set': 'memory', 'analog': None}\n",
    "\n",
    "#word2vec_dict = reduced_embeddings\n",
    "for analog in range(0,NUM_ANALOGS): # \n",
    "    for proposition in range(0,PER_ANALOG):\n",
    "        current_sentence = sentences[analog*PER_ANALOG + proposition]\n",
    "        current_sentence = current_sentence.split(' ')\n",
    "        # depending on proposition number, decompose propositions into their RB's\n",
    "        if proposition == 0: # == [- dogs][bite cats]\n",
    "            prop_dict = copy.deepcopy(prototype_1)\n",
    "            prop_dict['RBs'][0]['pred_name'] = 'det'\n",
    "            prop_dict['RBs'][0]['pred_sem'] = get_empty_vec(n_dim=N_COMPONENTS)\n",
    "            prop_dict['RBs'][0]['object_name'] = current_sentence[0]\n",
    "            prop_dict['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[0])\n",
    "            #prop_dict['RBs'][0]['object_name'] = current_sentence[0]\n",
    "            prop_dict['RBs'][1]['pred_name'] = current_sentence[1]\n",
    "            prop_dict['RBs'][1]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[1])\n",
    "            prop_dict['RBs'][1]['object_name'] = current_sentence[2]\n",
    "            prop_dict['RBs'][1]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[2])\n",
    "            prop_dict['analog'] = analog\n",
    "            prop_dict['name'] = 'P' + str(analog) + str(proposition)\n",
    "            symProps.append(prop_dict)\n",
    "        elif proposition == 1: # == [happy cats][sleep -]\n",
    "            prop_dict = copy.deepcopy(prototype_1)\n",
    "            prop_dict['RBs'][0]['pred_name'] = current_sentence[0] \n",
    "            prop_dict['RBs'][0]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[1])\n",
    "            prop_dict['RBs'][0]['object_name'] = current_sentence[1]\n",
    "            prop_dict['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[1])\n",
    "            prop_dict['RBs'][1]['pred_name'] = current_sentence[2]\n",
    "            prop_dict['RBs'][1]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[2])\n",
    "            prop_dict['analog'] = analog\n",
    "            prop_dict['name'] = 'P' + str(analog) + str(proposition)\n",
    "            symProps.append(prop_dict)\n",
    "        elif proposition == 2: # == [- eagles][fly -]\n",
    "            prop_dict = copy.deepcopy(prototype_1)\n",
    "            prop_dict['RBs'][0]['pred_name'] = 'det'\n",
    "            prop_dict['RBs'][0]['pred_sem'] = get_empty_vec(n_dim=N_COMPONENTS)\n",
    "            prop_dict['RBs'][0]['object_name'] = current_sentence[0]\n",
    "            prop_dict['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[0])\n",
    "            #prop_dict['RBs'][0]['object_name'] = current_sentence[0]\n",
    "            prop_dict['RBs'][1]['pred_name'] = current_sentence[1]\n",
    "            prop_dict['RBs'][1]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[1])\n",
    "            prop_dict['analog'] = analog\n",
    "            prop_dict['name'] = 'P' + str(analog) + str(proposition)\n",
    "            symProps.append(prop_dict)\n",
    "        elif proposition == 3: # == [- dogs][think [[big dogs][bite cats]]\n",
    "            prop_dict_1 = copy.deepcopy(prototype_1) # == [[big dogs][bite cats]]\n",
    "            prop_dict_1['RBs'][0]['pred_name'] = current_sentence[2]\n",
    "            prop_dict_1['RBs'][0]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[2])\n",
    "            prop_dict_1['RBs'][0]['object_name'] = current_sentence[3]\n",
    "            prop_dict_1['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[3])\n",
    "            prop_dict_1['RBs'][1]['pred_name'] = current_sentence[4]\n",
    "            prop_dict_1['RBs'][1]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[4])\n",
    "            prop_dict_1['RBs'][1]['object_name'] = current_sentence[5]\n",
    "            prop_dict_1['RBs'][1]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[5])\n",
    "            prop_dict_1['analog'] = analog\n",
    "            prop_dict_1['name'] = 'P' + str(analog) + str(proposition) + '.1'\n",
    "            symProps.append(prop_dict_1)\n",
    "            # add higher order prop \n",
    "            prop_dict = copy.deepcopy(prototype_2) # == [- dogs][think P-N.1]\n",
    "            prop_dict['RBs'][0]['pred_name'] = 'det'\n",
    "            prop_dict['RBs'][0]['pred_sem'] = get_empty_vec(n_dim=N_COMPONENTS)\n",
    "            prop_dict['RBs'][0]['object_name'] = current_sentence[0]\n",
    "            prop_dict['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[0])\n",
    "            prop_dict['RBs'][1]['pred_name'] = current_sentence[1]\n",
    "            prop_dict['RBs'][1]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[1])\n",
    "            prop_dict['RBs'][1]['P'] = 'P' + str(analog) + str(proposition) + '.1'\n",
    "            prop_dict['analog'] = analog\n",
    "            prop_dict['name'] = 'P' + str(analog) + str(proposition)\n",
    "            symProps.append(prop_dict)\n",
    "        elif proposition == 4: # == [big dogs][bite [small cats]]\n",
    "            prop_dict_1 = copy.deepcopy(prototype_3) # [small cats]\n",
    "            prop_dict_1['RBs'][0]['pred_name'] = current_sentence[3]\n",
    "            prop_dict_1['RBs'][0]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[3])\n",
    "            prop_dict_1['RBs'][0]['object_name'] = current_sentence[4]\n",
    "            prop_dict_1['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[4])\n",
    "            prop_dict_1['analog'] = analog\n",
    "            prop_dict_1['name'] = 'P' + str(analog) + str(proposition) + '.1'            \n",
    "            symProps.append(prop_dict_1)\n",
    "            prop_dict = copy.deepcopy(prototype_2) # == [big dogs][bite P-N.1]\n",
    "            prop_dict['RBs'][0]['pred_name'] = current_sentence[0]\n",
    "            prop_dict['RBs'][0]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[0])\n",
    "            prop_dict['RBs'][0]['object_name'] = current_sentence[1]\n",
    "            prop_dict['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[1])\n",
    "            prop_dict['RBs'][1]['pred_name'] = current_sentence[2]\n",
    "            prop_dict['RBs'][1]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[2])\n",
    "            prop_dict['RBs'][1]['P'] = 'P' + str(analog) + str(proposition) + '.1'\n",
    "            prop_dict['analog'] = analog\n",
    "            prop_dict['name'] = 'P' + str(analog) + str(proposition)\n",
    "            symProps.append(prop_dict)\n",
    "        elif proposition == 5: # == [[angry [hungry dogs]] [growl [at strangers]]]\n",
    "            prop_dict_1 = copy.deepcopy(prototype_3) # [hungry dogs]\n",
    "            prop_dict_1['RBs'][0]['pred_name'] = current_sentence[1]\n",
    "            prop_dict_1['RBs'][0]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[1])\n",
    "            prop_dict_1['RBs'][0]['object_name'] = current_sentence[2]\n",
    "            prop_dict_1['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[2])\n",
    "            prop_dict_1['analog'] = analog\n",
    "            prop_dict_1['name'] = 'P' + str(analog) + str(proposition) + '.1'            \n",
    "            symProps.append(prop_dict_1)\n",
    "            prop_dict_2 = copy.deepcopy(prototype_3)\n",
    "            prop_dict_2['RBs'][0]['pred_name'] = current_sentence[4] # [at strangers]\n",
    "            prop_dict_2['RBs'][0]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[4])\n",
    "            prop_dict_2['RBs'][0]['object_name'] = current_sentence[5]\n",
    "            prop_dict_2['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[5])\n",
    "            prop_dict_2['analog'] = analog\n",
    "            prop_dict_2['name'] = 'P' + str(analog) + str(proposition) + '.2'                        \n",
    "            symProps.append(prop_dict_2)\n",
    "            prop_dict_3 = copy.deepcopy(prototype_2) # [angry [P N.1]] [growl [P N.2]]\n",
    "            prop_dict_3['RBs'][0]['pred_name'] = current_sentence[0]\n",
    "            prop_dict_3['RBs'][0]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[0])\n",
    "            prop_dict_3['RBs'][0]['higher_order'] = True\n",
    "            prop_dict_3['RBs'][0]['P'] = 'P' + str(analog) + str(proposition) + '.1'\n",
    "            prop_dict_3['RBs'][1]['pred_name'] = current_sentence[3]\n",
    "            prop_dict_3['RBs'][1]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[3])\n",
    "            prop_dict_3['RBs'][1]['higher_order'] = True\n",
    "            prop_dict_3['RBs'][1]['P'] = 'P' + str(analog) + str(proposition) + '.2'            \n",
    "            prop_dict_3['analog'] = analog\n",
    "            prop_dict_3['name'] = 'P' + str(analog) + str(proposition)            \n",
    "            symProps.append(prop_dict_3)                  \n",
    "        elif proposition == 6: # [kitten [stay [away [from water]]]]\n",
    "            prop_dict_1 = copy.deepcopy(prototype_3)\n",
    "            prop_dict_1['RBs'][0]['pred_name'] = current_sentence[3] # [from water]\n",
    "            prop_dict_1['RBs'][0]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[3])\n",
    "            prop_dict_1['RBs'][0]['object_name'] = current_sentence[4]\n",
    "            prop_dict_1['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[4])\n",
    "            prop_dict_1['analog'] = analog\n",
    "            prop_dict_1['name'] = 'P' + str(analog) + str(proposition) + '.1'\n",
    "            symProps.append(prop_dict_1)\n",
    "            prop_dict_2 = copy.deepcopy(prototype_3) # [away [P N.1]]\n",
    "            prop_dict_2['RBs'][0]['pred_name'] = current_sentence[2]\n",
    "            prop_dict_2['RBs'][0]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[2])\n",
    "            prop_dict_2['RBs'][0]['higher_order'] = True\n",
    "            prop_dict_2['RBs'][0]['P'] = 'P' + str(analog) + str(proposition) + '.1'\n",
    "            prop_dict_2['analog'] = analog\n",
    "            prop_dict_2['name'] = 'P' + str(analog) + str(proposition) + '.2'\n",
    "            symProps.append(prop_dict_2)\n",
    "            prop_dict_3 = copy.deepcopy(prototype_2) # [- kitten] [stay [P N.2]]  OR [kitten PN.3] ?! (kitten is not the predicate)\n",
    "            prop_dict_3['RBs'][0]['pred_name'] = 'det'\n",
    "            prop_dict_3['RBs'][0]['pred_sem'] = get_empty_vec(n_dim=N_COMPONENTS)\n",
    "            prop_dict_3['RBs'][0]['object_name'] = current_sentence[0]\n",
    "            prop_dict_3['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[0])\n",
    "            prop_dict_3['RBs'][1]['pred_name'] = current_sentence[1]\n",
    "            prop_dict_3['RBs'][1]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[1])\n",
    "            prop_dict_3['RBs'][1]['higher_order'] = True\n",
    "            prop_dict_3['RBs'][1]['P'] = 'P' + str(analog) + str(proposition) + '.2'            \n",
    "            prop_dict_3['analog'] = analog\n",
    "            prop_dict_3['name'] = 'P' + str(analog) + str(proposition)            \n",
    "            symProps.append(prop_dict_3)\n",
    "        elif proposition == 7: # [[cute [fluffy cats]] [purr softly]]\n",
    "            prop_dict_1 = copy.deepcopy(prototype_3)\n",
    "            prop_dict_1['RBs'][0]['pred_name'] = current_sentence[1] # [fluffy cats]\n",
    "            prop_dict_1['RBs'][0]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[1])\n",
    "            prop_dict_1['RBs'][0]['object_name'] = current_sentence[2]\n",
    "            prop_dict_1['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[2])\n",
    "            prop_dict_1['analog'] = analog\n",
    "            prop_dict_1['name'] = 'P' + str(analog) + str(proposition) + '.1'\n",
    "            symProps.append(prop_dict_1)\n",
    "            prop_dict_2 = copy.deepcopy(prototype_2) # [cute PN.1] [purr softly]\n",
    "            prop_dict_2['RBs'][0]['pred_name'] = current_sentence[0]\n",
    "            prop_dict_2['RBs'][0]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[0])\n",
    "            prop_dict_2['RBs'][0]['higher_order'] = True\n",
    "            prop_dict_2['RBs'][0]['P'] = 'P' + str(analog) + str(proposition) + '.1'            \n",
    "            prop_dict_2['RBs'][1]['higher_order'] = False\n",
    "            prop_dict_2['RBs'][1]['pred_name'] = current_sentence[3]\n",
    "            prop_dict_2['RBs'][1]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[3])\n",
    "            prop_dict_2['RBs'][1]['object_name'] = current_sentence[4]\n",
    "            prop_dict_2['RBs'][1]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[4])            \n",
    "            prop_dict_2['analog'] = analog\n",
    "            prop_dict_2['name'] = 'P' + str(analog) + str(proposition)            \n",
    "            symProps.append(prop_dict_2)\n",
    "''''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['e0', 0.06480299358623666],\n",
       " ['e1', 0.410369420308889],\n",
       " ['e2', 0.08415358252775379],\n",
       " ['e3', 0.20782311894926406],\n",
       " ['e4', 0.4601755301390137],\n",
       " ['e5', 0.23720713003764196],\n",
       " ['e6', 0.223714081131958],\n",
       " ['e7', 0.31554810590104265],\n",
       " ['e8', 0.35444081652806214],\n",
       " ['e9', 0.48357998918545486],\n",
       " ['e10', 0.0]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sem(word2vec_dict=reduced_embeddings,word='snakes')\n",
    "#get_empty_vec(n_dim=N_COMPONENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialising lexical one hot vecs :\n",
    "# categories - noun, transitive verb, intransitive verb, adjective, complementizer verb, and empty sem\n",
    "noun = [['e0',1],['e1',0],['e2',0],['e3',0],['e4',0],['e5',0]]\n",
    "t_verb = [['e0',0],['e1',1],['e2',0],['e3',0],['e4',0],['e5',0]]\n",
    "it_verb = [['e0',0],['e1',0],['e2',1],['e3',0],['e4',0],['e5',0]]\n",
    "adj = [['e0',0],['e1',0],['e2',0],['e3',1],['e4',0],['e5',0]]\n",
    "c_verb = [['e0',0],['e1',0],['e2',0],['e3',0],['e4',1],['e5',0]]\n",
    "empty_sem = [['e0',0],['e1',0],['e2',0],['e3',0],['e4',0],['e5',1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type 4 sym setup - 4 analogs consisting of 5 sentence types, with lexical one hot encoding only\n",
    "random.seed(999)\n",
    "symProps = []\n",
    "# for type = [0,1,2,3], [9,10,11]\n",
    "prototype_1={'name': 'non_exist', \n",
    "             'RBs': [{'pred_name': 'non_exist', 'pred_sem': [], \n",
    "                      'higher_order': False, \n",
    "                      'object_name': 'obj', 'object_sem': [], 'P': 'non_exist'}, \n",
    "                     {'pred_name': 'non_exist', 'pred_sem': [], \n",
    "                      'higher_order': False, \n",
    "                      'object_name': 'obj', 'object_sem': [], 'P': 'non_exist'}], \n",
    "             'set': 'memory', 'analog': None}\n",
    "# for 3 word props in [4,5,6,7,8] containing prototype_1\n",
    "prototype_2 = {'name': 'non_exist', \n",
    "             'RBs': [{'pred_name': 'non_exist', 'pred_sem': [], \n",
    "                      'higher_order': False, \n",
    "                      'object_name': 'obj', 'object_sem': [], 'P': 'non_exist'}, \n",
    "                     {'pred_name': 'non_exist', 'pred_sem': [], \n",
    "                      'higher_order': True, \n",
    "                      'object_name': 'non_exist', 'object_sem': [], 'P': 'non_exist'}], \n",
    "             'set': 'memory', 'analog': None}\n",
    "# for 2 word prop = single RB\n",
    "prototype_3 = {'name': 'non_exist', \n",
    "             'RBs': [{'pred_name': 'non_exist', 'pred_sem': [], \n",
    "                      'higher_order': False, \n",
    "                      'object_name': 'obj', 'object_sem': [], 'P': 'non_exist'}], \n",
    "             'set': 'memory', 'analog': None}\n",
    "\n",
    "#word2vec_dict = reduced_embeddings\n",
    "for analog in range(0,NUM_ANALOGS): # \n",
    "    for proposition in range(0,PER_ANALOG):\n",
    "        current_sentence = sentences[analog*PER_ANALOG + proposition]\n",
    "        current_sentence = current_sentence.split(' ')\n",
    "        # depending on proposition number, decompose propositions into their RB's\n",
    "        if proposition == 0: # == [- dogs][bite cats]\n",
    "            prop_dict = copy.deepcopy(prototype_1)\n",
    "            prop_dict['RBs'][0]['pred_name'] = 'noun'\n",
    "            prop_dict['RBs'][0]['pred_sem'] = noun\n",
    "            #prop_dict['RBs'][0]['object_name'] = 'noun'\n",
    "            #prop_dict['RBs'][0]['object_sem'] = noun\n",
    "            prop_dict['RBs'][1]['pred_name'] = 't_verb'\n",
    "            prop_dict['RBs'][1]['pred_sem'] = t_verb\n",
    "            prop_dict['RBs'][1]['object_name'] = 'noun'\n",
    "            prop_dict['RBs'][1]['object_sem'] = noun\n",
    "            prop_dict['analog'] = analog\n",
    "            prop_dict['name'] = 'P' + str(analog) + str(proposition)\n",
    "            symProps.append(prop_dict)\n",
    "        elif proposition == 1: # == [happy cats][sleep -]\n",
    "            prop_dict = copy.deepcopy(prototype_1)\n",
    "            prop_dict['RBs'][0]['pred_name'] = 'adj'\n",
    "            prop_dict['RBs'][0]['pred_sem'] = adj\n",
    "            prop_dict['RBs'][0]['object_name'] = 'noun'\n",
    "            prop_dict['RBs'][0]['object_sem'] = noun\n",
    "            prop_dict['RBs'][1]['pred_name'] = 'it_verb'\n",
    "            prop_dict['RBs'][1]['pred_sem'] = it_verb\n",
    "            prop_dict['analog'] = analog\n",
    "            prop_dict['name'] = 'P' + str(analog) + str(proposition)\n",
    "            symProps.append(prop_dict)\n",
    "        elif proposition == 2: # == [- eagles][fly -]\n",
    "            prop_dict = copy.deepcopy(prototype_1)\n",
    "            prop_dict['RBs'][0]['pred_name'] = 'noun'\n",
    "            prop_dict['RBs'][0]['pred_sem'] = noun\n",
    "            #prop_dict['RBs'][0]['object_name'] = 'noun'\n",
    "            #prop_dict['RBs'][0]['object_sem'] = noun\n",
    "            prop_dict['RBs'][1]['pred_name'] = 'it_verb'\n",
    "            prop_dict['RBs'][1]['pred_sem'] = it_verb\n",
    "            prop_dict['analog'] = analog\n",
    "            prop_dict['name'] = 'P' + str(analog) + str(proposition)\n",
    "            symProps.append(prop_dict)\n",
    "        elif proposition == 3: # == [- dogs][think [[big dogs][bite cats]]\n",
    "            prop_dict_1 = copy.deepcopy(prototype_1) # == [[big dogs][bite cats]]\n",
    "            prop_dict_1['RBs'][0]['pred_name'] = 'adj'\n",
    "            prop_dict_1['RBs'][0]['pred_sem'] = adj\n",
    "            prop_dict_1['RBs'][0]['object_name'] = 'noun'\n",
    "            prop_dict_1['RBs'][0]['object_sem'] = noun\n",
    "            prop_dict_1['RBs'][1]['pred_name'] = 't_verb'\n",
    "            prop_dict_1['RBs'][1]['pred_sem'] = t_verb\n",
    "            prop_dict_1['RBs'][1]['object_name'] = 'noun'\n",
    "            prop_dict_1['RBs'][1]['object_sem'] = noun\n",
    "            prop_dict_1['analog'] = analog\n",
    "            prop_dict_1['name'] = 'P' + str(analog) + str(proposition) + '.1'\n",
    "            symProps.append(prop_dict_1)\n",
    "            # add higher order prop \n",
    "            prop_dict = copy.deepcopy(prototype_2) # == [- dogs][think P-N.1]\n",
    "            prop_dict['RBs'][0]['pred_name'] = 'noun'\n",
    "            prop_dict['RBs'][0]['pred_sem'] = noun\n",
    "            #prop_dict['RBs'][0]['object_name'] = 'noun'\n",
    "            #prop_dict['RBs'][0]['object_sem'] = noun\n",
    "            prop_dict['RBs'][1]['pred_name'] = 'c_verb'\n",
    "            prop_dict['RBs'][1]['pred_sem'] = c_verb\n",
    "            prop_dict['RBs'][1]['P'] = 'P' + str(analog) + str(proposition) + '.1'\n",
    "            prop_dict['analog'] = analog\n",
    "            prop_dict['name'] = 'P' + str(analog) + str(proposition)\n",
    "            symProps.append(prop_dict)\n",
    "        elif proposition == 4: # == [big dogs][bite [small cats]]\n",
    "            prop_dict_1 = copy.deepcopy(prototype_3) # [small cats]\n",
    "            prop_dict_1['RBs'][0]['pred_name'] = 'adj'\n",
    "            prop_dict_1['RBs'][0]['pred_sem'] = adj\n",
    "            prop_dict_1['RBs'][0]['object_name'] = 'noun'\n",
    "            prop_dict_1['RBs'][0]['object_sem'] = noun\n",
    "            prop_dict_1['analog'] = analog\n",
    "            prop_dict_1['name'] = 'P' + str(analog) + str(proposition) + '.1'            \n",
    "            symProps.append(prop_dict_1)\n",
    "            prop_dict = copy.deepcopy(prototype_2) # == [big dogs][bite P-N.1]\n",
    "            prop_dict['RBs'][0]['pred_name'] = 'adj'\n",
    "            prop_dict['RBs'][0]['pred_sem'] = adj\n",
    "            prop_dict['RBs'][0]['object_name'] = 'noun'\n",
    "            prop_dict['RBs'][0]['object_sem'] = noun\n",
    "            prop_dict['RBs'][1]['pred_name'] = 't_verb'\n",
    "            prop_dict['RBs'][1]['pred_sem'] = t_verb\n",
    "            prop_dict['RBs'][1]['P'] = 'P' + str(analog) + str(proposition) + '.1'\n",
    "            prop_dict['analog'] = analog\n",
    "            prop_dict['name'] = 'P' + str(analog) + str(proposition)\n",
    "            symProps.append(prop_dict)\n",
    "            # add a proposition that is the reverse of proposition 4, to see how it behaves:\n",
    "            \n",
    "''''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['e0', 0.0], ['e1', 0.0], ['e2', 0.0], ['e3', 0.0], ['e4', 0.0], ['e5', 1.0]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_empty_vec(n_dim=N_COMPONENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert symProps dict to text and write to python file\n",
    "test_sim_path = str(os.path.dirname(os.getcwd())) + '\\\\test_sim_custom-WordGCN-7-empty_pred.py'\n",
    "write_file = open(test_sim_path, 'w')\n",
    "simType = 'sym_file'\n",
    "write_file.write(\"simType='sym_file'\\n\")\n",
    "write_file.write(\"symProps=\")\n",
    "write_file.write(str(symProps))\n",
    "write_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9642436703816699, 0.7648118897384594, 0.6713845447337492, 0.5863380140569783, 0.7189319440521685, 0.9305206857412015)\n"
     ]
    }
   ],
   "source": [
    "# tests : \n",
    "# hungry dogs bark -- deer are beautiful\n",
    "# hungry dogs is closer to are beautiful, than hungry dogs to happy cats\n",
    "# compare with word embedding distances\n",
    "print(np.dot(reduced_embeddings['think'],reduced_embeddings['say']),\n",
    "np.dot(reduced_embeddings['think'],reduced_embeddings['easy']),\n",
    "np.dot(reduced_embeddings['hungry'],reduced_embeddings['majestic']),\n",
    "np.dot(reduced_embeddings['hungry'],reduced_embeddings['say']),\n",
    "np.dot(reduced_embeddings['easy'],reduced_embeddings['say']),\n",
    "np.dot(reduced_embeddings['easy'],reduced_embeddings['small']))\n",
    "#np.dot(reduced_embeddings['dogs'],reduced_embeddings['cats']),\n",
    "#np.dot(reduced_embeddings['dogs'],reduced_embeddings['beautiful']),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('bulky', ' : cats', 0.8996268858494894)\n",
      "('deer', ' : cats', 0.820462193248583)\n",
      "('worms', ' : cats', 0.6502365789785044)\n",
      "('jump', ' : cats', 0.6323390693496724)\n",
      "('say', ' : cats', 0.9027534281245476)\n",
      "('majestic', ' : cats', 0.8550571610386262)\n",
      "('bark', ' : cats', 0.7878160179016085)\n",
      "('growl', ' : cats', 0.6755928010050203)\n",
      "('glide', ' : cats', 0.8397984004420732)\n",
      "('bite', ' : cats', 0.7403121271920742)\n",
      "('fast', ' : cats', 0.7254383669191854)\n",
      "('cats', ' : cats', 1.0)\n",
      "('lions', ' : cats', 0.764426044970141)\n",
      "('easy', ' : cats', 0.8910096864477822)\n",
      "('eagles', ' : cats', 0.8890342490947979)\n",
      "('angry', ' : cats', 0.7780345444718267)\n",
      "('dogs', ' : cats', 0.7571206678669506)\n",
      "('big', ' : cats', 0.9041580455034262)\n",
      "('spot', ' : cats', 0.7588218594637269)\n",
      "('early', ' : cats', 0.8394382787546042)\n",
      "('know', ' : cats', 0.8519041673856597)\n",
      "('catch', ' : cats', 0.9421742716964969)\n",
      "('eat', ' : cats', 0.7949577087911066)\n",
      "('fly', ' : cats', 0.7605063106469391)\n",
      "('wise', ' : cats', 0.6889790913834667)\n",
      "('roar', ' : cats', 0.9016176511328159)\n",
      "('hunt', ' : cats', 0.7271379774945056)\n",
      "('hungry', ' : cats', 0.8281504805015696)\n",
      "('stalk', ' : cats', 0.8985925671114)\n",
      "('small', ' : cats', 0.8564828517393979)\n",
      "('think', ' : cats', 0.760174898956839)\n"
     ]
    }
   ],
   "source": [
    "for word in unique_words:\n",
    "    print(word, ' : cats', np.dot(reduced_embeddings['cats'],reduced_embeddings[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\karkau\\\\Documents\\\\karthikeya\\\\Dora\\\\BrPong_1\\\\workspace\\\\karthikeya\\\\data\\\\results\\\\wordgcn\\\\WordGCN.pkl'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_file= open(str(os.path.dirname(os.getcwd())) + '\\\\data\\\\results\\\\wordgcn\\\\WordGCN.pkl', 'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data= pkl.load(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [[1.0,\n",
       "   0.4000000000000001,\n",
       "   0.4375,\n",
       "   0.4117647058823529,\n",
       "   0.3888888888888889,\n",
       "   0.43805718578657793,\n",
       "   0.43550332807480685,\n",
       "   0.43550332807480685,\n",
       "   0.39704178961326847,\n",
       "   0.4569161461205482,\n",
       "   0.4505581301130532,\n",
       "   0.4446081860188387,\n",
       "   0.4306723589191704,\n",
       "   0.4024753179687956,\n",
       "   0.4084839604827696,\n",
       "   0.4398048837010607,\n",
       "   0.4146731760610001,\n",
       "   0.43095264842343484,\n",
       "   0.43095264842343484,\n",
       "   0.4404850517744469,\n",
       "   0.4285800503751375,\n",
       "   0.45358225767405275,\n",
       "   0.45358225767405275,\n",
       "   0.48393879891438324,\n",
       "   0.4594958907259751,\n",
       "   0.472606242620821,\n",
       "   0.487256069214687,\n",
       "   0.48104921906600495,\n",
       "   0.47948321062978466,\n",
       "   0.47946544140079755,\n",
       "   0.49529280323533403,\n",
       "   0.5672767734167177,\n",
       "   0.5710509238700896,\n",
       "   0.580129571960611,\n",
       "   0.6116819720549431,\n",
       "   0.6116819720549431,\n",
       "   0.6258853495555361,\n",
       "   0.6395590602290292,\n",
       "   0.6538582402804481,\n",
       "   0.6399502141684592,\n",
       "   0.6156612089185731,\n",
       "   0.6156612089185731,\n",
       "   0.6375908465516077,\n",
       "   0.6380179462956389,\n",
       "   0.6262938853050157,\n",
       "   0.6361531536449678,\n",
       "   0.6361166724080298,\n",
       "   0.6316716884165804,\n",
       "   0.6431623149167582,\n",
       "   0.6617157629989788,\n",
       "   0.6629984467424361,\n",
       "   0.6732021540358756,\n",
       "   0.6732021540358756,\n",
       "   0.6658846139050268,\n",
       "   0.6658846139050268,\n",
       "   0.7015522584366836,\n",
       "   0.697733531947044,\n",
       "   0.6965479507464494,\n",
       "   0.6929886651323727,\n",
       "   0.6868304008544446,\n",
       "   0.6735357202703789,\n",
       "   0.673111068776523,\n",
       "   0.6624255912076004,\n",
       "   0.659522611906021,\n",
       "   0.6589189395367469,\n",
       "   0.658918939536747,\n",
       "   0.658918939536747,\n",
       "   0.658265550392093,\n",
       "   0.658265550392093,\n",
       "   0.6660200384931128,\n",
       "   0.669500002841284,\n",
       "   0.6694655015106104,\n",
       "   0.6550560297288055,\n",
       "   0.6550560297288056,\n",
       "   0.6604461462648651,\n",
       "   0.6640475257189781,\n",
       "   0.6721600963693962,\n",
       "   0.6721600963693962,\n",
       "   0.7167452983525557,\n",
       "   0.7164271174163984,\n",
       "   0.7164271174163984,\n",
       "   0.7114641324795006,\n",
       "   0.7083113889882832,\n",
       "   0.7034767385020928,\n",
       "   0.6995744936620709,\n",
       "   0.7006784701820074,\n",
       "   0.6998134739886986,\n",
       "   0.6996317204621431,\n",
       "   0.6996422968074396,\n",
       "   0.6987595819352278,\n",
       "   0.6974895934592203,\n",
       "   0.6981698417684038,\n",
       "   0.6981698417684037,\n",
       "   0.6981698417684037,\n",
       "   0.693904173883014,\n",
       "   0.6794298095383934,\n",
       "   0.6723541781876755,\n",
       "   0.671635076783383,\n",
       "   0.6718957653750884,\n",
       "   0.6706068095858949],\n",
       "  [0.07515416254704824,\n",
       "   0.07576767609436587,\n",
       "   0.07876359377087681,\n",
       "   0.07876359377087681,\n",
       "   0.07876359377087681,\n",
       "   0.07907862744007471,\n",
       "   0.08081588393121686,\n",
       "   0.08081588393121686,\n",
       "   0.08024097770460753,\n",
       "   0.08122910064672498,\n",
       "   0.0816515657099684,\n",
       "   0.0805470561416823,\n",
       "   0.07980590279809543,\n",
       "   0.07911616487999,\n",
       "   0.07888973236984775,\n",
       "   0.07915030113176603,\n",
       "   0.07848902667211693,\n",
       "   0.07907686227496903,\n",
       "   0.07907686227496903,\n",
       "   0.07834739670916054,\n",
       "   0.07834739670916054,\n",
       "   0.07772599486033056,\n",
       "   0.07772599486033056,\n",
       "   0.07959058123519458,\n",
       "   0.07912940854672976,\n",
       "   0.0791294355647291,\n",
       "   0.0797123589249823,\n",
       "   0.079506843877392,\n",
       "   0.07778325834343677,\n",
       "   0.07778341633771052,\n",
       "   0.07742467242935593,\n",
       "   0.0788635096108596,\n",
       "   0.0786644602392426,\n",
       "   0.07927886184953273,\n",
       "   0.07892558483091017,\n",
       "   0.07892558483091017,\n",
       "   0.07896201257290329,\n",
       "   0.0778133619905093,\n",
       "   0.07762342401519191,\n",
       "   0.07743400448258952,\n",
       "   0.07697844671425538,\n",
       "   0.07697844671425538,\n",
       "   0.07680444079248926,\n",
       "   0.07652657627257411,\n",
       "   0.07464193195840078,\n",
       "   0.07471178082799432,\n",
       "   0.0747124494344813,\n",
       "   0.07466633867439049,\n",
       "   0.07459225591413948,\n",
       "   0.07430233505538637,\n",
       "   0.07352151401710788,\n",
       "   0.07331160623291695,\n",
       "   0.07449175185351575,\n",
       "   0.07454121346410353,\n",
       "   0.07454121346410353,\n",
       "   0.07391387018116025,\n",
       "   0.07370098119142571,\n",
       "   0.07370881196283863,\n",
       "   0.0728322685964899,\n",
       "   0.07301810811083279,\n",
       "   0.07241480159499776,\n",
       "   0.07242062781233573,\n",
       "   0.07178047985096575,\n",
       "   0.07175493741876908,\n",
       "   0.07174538465191498,\n",
       "   0.07153923620372445,\n",
       "   0.07129478427093942,\n",
       "   0.0711233822025292,\n",
       "   0.06927085764339751,\n",
       "   0.0690644938211958,\n",
       "   0.06902511000049459,\n",
       "   0.0690252813526626,\n",
       "   0.06865765966916121,\n",
       "   0.06865765966916121,\n",
       "   0.06936965179770302,\n",
       "   0.06822402822698341,\n",
       "   0.06823276632604941,\n",
       "   0.06823276632604941,\n",
       "   0.06810832849183604,\n",
       "   0.06765707028328073,\n",
       "   0.06765707028328073,\n",
       "   0.06638228879921061,\n",
       "   0.06694250020529521,\n",
       "   0.06815809644265586,\n",
       "   0.06823786843431774,\n",
       "   0.06795271343644259,\n",
       "   0.06797748473431357,\n",
       "   0.06798003727289556,\n",
       "   0.06797991697618821,\n",
       "   0.0680026283379714,\n",
       "   0.06800219364049574,\n",
       "   0.06799203236568795,\n",
       "   0.06799200635316623,\n",
       "   0.06799200635316623,\n",
       "   0.06703896732670438,\n",
       "   0.06645854267132435,\n",
       "   0.06626180856812554,\n",
       "   0.06720694582580582,\n",
       "   0.06644361163416945,\n",
       "   0.06642415576361511],\n",
       "  array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]])],\n",
       " 1: [[0.6666666666666666,\n",
       "   0.5,\n",
       "   0.5,\n",
       "   0.5476959518587862,\n",
       "   0.5824885709465719,\n",
       "   0.6398982146890378,\n",
       "   0.6398982146890378,\n",
       "   0.6610806726485062,\n",
       "   0.6610806726485062,\n",
       "   0.6619185717512303,\n",
       "   0.6303986397630764,\n",
       "   0.6681720057419862,\n",
       "   0.624693744872421,\n",
       "   0.624693744872421,\n",
       "   0.6202841475737744,\n",
       "   0.6247727784537317,\n",
       "   0.6533938257396648,\n",
       "   0.6905649078165267,\n",
       "   0.7032519739826607,\n",
       "   0.7102149179406863,\n",
       "   0.6915165021316709,\n",
       "   0.706098040263657,\n",
       "   0.7017924998497685,\n",
       "   0.6857453502313922,\n",
       "   0.6872715370518,\n",
       "   0.6770580234837466,\n",
       "   0.6669660867176137,\n",
       "   0.6669660867176137,\n",
       "   0.6770580234837466,\n",
       "   0.6766156743621842,\n",
       "   0.631049448380648,\n",
       "   0.6257503965210706,\n",
       "   0.6232491836919726,\n",
       "   0.6338821258524785,\n",
       "   0.6312020062719328,\n",
       "   0.6311272975053277,\n",
       "   0.6311272975053277,\n",
       "   0.6329155963108448,\n",
       "   0.668400039523498,\n",
       "   0.6510589763076897,\n",
       "   0.6505769109954254,\n",
       "   0.6469086324871944,\n",
       "   0.6466951379050583,\n",
       "   0.6466951379050583,\n",
       "   0.6466951379050583,\n",
       "   0.6652282972232606,\n",
       "   0.6652282972232606,\n",
       "   0.6645995300109094,\n",
       "   0.6535713401222815,\n",
       "   0.6535713401222815,\n",
       "   0.6535713401222815,\n",
       "   0.6535713401222815,\n",
       "   0.6597879231300572,\n",
       "   0.6496697129545483,\n",
       "   0.6580454680888788,\n",
       "   0.6871611982722038,\n",
       "   0.6719982618716106,\n",
       "   0.6800227255013161,\n",
       "   0.6812869185828342,\n",
       "   0.6812419979190555,\n",
       "   0.6803760274034867,\n",
       "   0.6787374355986747,\n",
       "   0.686129041251962,\n",
       "   0.704173994537616,\n",
       "   0.704173994537616,\n",
       "   0.704173994537616,\n",
       "   0.7041739945376159,\n",
       "   0.7195916010240019,\n",
       "   0.719389580349153,\n",
       "   0.7268298259618062,\n",
       "   0.7268298259618062,\n",
       "   0.7254768709918203,\n",
       "   0.7254768709918203,\n",
       "   0.7240306154780197,\n",
       "   0.7266241480343524,\n",
       "   0.7266351442572282,\n",
       "   0.726602192541891,\n",
       "   0.7266096816365795,\n",
       "   0.7304861394116767,\n",
       "   0.7219275404769846,\n",
       "   0.7183096302408183,\n",
       "   0.7221666145031974,\n",
       "   0.7221666145031974,\n",
       "   0.7176511384914334,\n",
       "   0.7176511384914334,\n",
       "   0.7126207486703225,\n",
       "   0.7178731475223605,\n",
       "   0.717857888489199,\n",
       "   0.7154439557229281,\n",
       "   0.7154450225995799,\n",
       "   0.7149643085220777,\n",
       "   0.7217334420090666,\n",
       "   0.7217334420090666,\n",
       "   0.7221060302238698,\n",
       "   0.7199621579311155,\n",
       "   0.7202179100061523,\n",
       "   0.7240296925267136,\n",
       "   0.7240296925267136,\n",
       "   0.7240296925267133,\n",
       "   0.7357206291526084],\n",
       "  [0.07576767609436587,\n",
       "   0.0775791113542719,\n",
       "   0.07817359599705716,\n",
       "   0.07778911257410362,\n",
       "   0.07838200448865262,\n",
       "   0.07955378167170465,\n",
       "   0.07955378167170465,\n",
       "   0.08013361823143794,\n",
       "   0.08013361823143794,\n",
       "   0.08128088309096443,\n",
       "   0.08128088309096443,\n",
       "   0.08205193950963227,\n",
       "   0.08148575448935698,\n",
       "   0.08148575448935698,\n",
       "   0.08317274771508718,\n",
       "   0.08427865179072767,\n",
       "   0.0852208997044384,\n",
       "   0.08467450436283298,\n",
       "   0.08506718627597079,\n",
       "   0.08495107735653366,\n",
       "   0.08479352486581004,\n",
       "   0.08479076896115172,\n",
       "   0.08482376045275619,\n",
       "   0.08444696839496206,\n",
       "   0.08370331082862494,\n",
       "   0.08316636397496738,\n",
       "   0.08260781745224939,\n",
       "   0.08260781745224939,\n",
       "   0.08316636397496738,\n",
       "   0.08317979697817349,\n",
       "   0.08108023808912085,\n",
       "   0.0810349658224556,\n",
       "   0.0806786973759156,\n",
       "   0.08052502390136682,\n",
       "   0.08059117101336724,\n",
       "   0.07972822359572394,\n",
       "   0.07885396556398748,\n",
       "   0.07877453107253209,\n",
       "   0.07935928599963991,\n",
       "   0.07826501284088642,\n",
       "   0.07826555745919404,\n",
       "   0.0771772188962107,\n",
       "   0.07717779086001686,\n",
       "   0.07717779086001686,\n",
       "   0.07681294624861332,\n",
       "   0.07682696241712526,\n",
       "   0.07682696241712526,\n",
       "   0.0768460501958725,\n",
       "   0.07674614490643508,\n",
       "   0.07674614490643508,\n",
       "   0.07674614490643508,\n",
       "   0.07674614490643508,\n",
       "   0.07476495832357939,\n",
       "   0.07413746848481162,\n",
       "   0.07399348427184299,\n",
       "   0.07379133233384184,\n",
       "   0.07374032440807643,\n",
       "   0.07385909468947544,\n",
       "   0.07373855557617923,\n",
       "   0.07373984797906041,\n",
       "   0.07358153611742253,\n",
       "   0.07362156783971853,\n",
       "   0.07328134691529369,\n",
       "   0.07431338371026937,\n",
       "   0.07431338371026937,\n",
       "   0.07431338371026937,\n",
       "   0.0742376837709217,\n",
       "   0.07433226111294645,\n",
       "   0.07433849112933644,\n",
       "   0.07378273882234336,\n",
       "   0.07378273882234336,\n",
       "   0.07183823581381815,\n",
       "   0.07183823581381815,\n",
       "   0.07185224618262177,\n",
       "   0.0717660616644346,\n",
       "   0.071764476635519,\n",
       "   0.0717643440432431,\n",
       "   0.07176424406806119,\n",
       "   0.07207975576870251,\n",
       "   0.07141622734522034,\n",
       "   0.07179027135193501,\n",
       "   0.07225485528962168,\n",
       "   0.07225485528962168,\n",
       "   0.07118422748324958,\n",
       "   0.07118422748324958,\n",
       "   0.07211761910898017,\n",
       "   0.0719991051980191,\n",
       "   0.07211305062882817,\n",
       "   0.07216877281102062,\n",
       "   0.07216913635370607,\n",
       "   0.07325280833458676,\n",
       "   0.0731821436512941,\n",
       "   0.0731821436512941,\n",
       "   0.07239738744422217,\n",
       "   0.07243360950116604,\n",
       "   0.07242585415006228,\n",
       "   0.0718729657414359,\n",
       "   0.0718729657414359,\n",
       "   0.07091282516622151,\n",
       "   0.07073492267643844],\n",
       "  array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]])],\n",
       " 2: [[0.3333333333333333,\n",
       "   0.3333333333333333,\n",
       "   0.4000000000000001,\n",
       "   0.5887351193125457,\n",
       "   0.5998681975059915,\n",
       "   0.6346622672880793,\n",
       "   0.6346622672880793,\n",
       "   0.6410186880294023,\n",
       "   0.6571640675806019,\n",
       "   0.6315373022270223,\n",
       "   0.6315373022270223,\n",
       "   0.6315373022270223,\n",
       "   0.644696684290343,\n",
       "   0.6350502386709868,\n",
       "   0.5889507788487023,\n",
       "   0.5926642224986668,\n",
       "   0.5999459579320326,\n",
       "   0.5999459579320326,\n",
       "   0.6009781515379221,\n",
       "   0.6009781515379221,\n",
       "   0.5766468044284141,\n",
       "   0.5766468044284141,\n",
       "   0.6020745447201711,\n",
       "   0.6020745447201711,\n",
       "   0.614132891849863,\n",
       "   0.614132891849863,\n",
       "   0.614132891849863,\n",
       "   0.625481924442514,\n",
       "   0.625481924442514,\n",
       "   0.625481924442514,\n",
       "   0.6584630265058935,\n",
       "   0.6639932096246192,\n",
       "   0.6639932096246192,\n",
       "   0.6418601026371319,\n",
       "   0.6418601026371319,\n",
       "   0.6211549380359341,\n",
       "   0.6329938462223111,\n",
       "   0.6393655877011521,\n",
       "   0.6392821089617778,\n",
       "   0.6595957031709445,\n",
       "   0.6532602191132058,\n",
       "   0.6412967961006348,\n",
       "   0.5980300318297999,\n",
       "   0.5845774003791593,\n",
       "   0.5912693734087172,\n",
       "   0.5889118820455989,\n",
       "   0.5889118820455989,\n",
       "   0.5534629036891652,\n",
       "   0.5534629036891652,\n",
       "   0.5534629036891652,\n",
       "   0.5535104439112108,\n",
       "   0.5651064845451423,\n",
       "   0.5609017821783371,\n",
       "   0.5609017821783372,\n",
       "   0.5602111686387533,\n",
       "   0.5608141148009261,\n",
       "   0.5565763909013056,\n",
       "   0.5637110435419216,\n",
       "   0.5632459381495433,\n",
       "   0.5632459381495433,\n",
       "   0.5632807607583876,\n",
       "   0.5632807607583876,\n",
       "   0.5798464124977132,\n",
       "   0.5777700747480585,\n",
       "   0.5785669254540963,\n",
       "   0.5785669254540963,\n",
       "   0.5791010734128835,\n",
       "   0.5816453130364769,\n",
       "   0.5803818591743262,\n",
       "   0.5803818591743262,\n",
       "   0.5786136355593512,\n",
       "   0.5782375059887297,\n",
       "   0.5795180903732853,\n",
       "   0.5802660139369806,\n",
       "   0.5800527400219845,\n",
       "   0.5800527400219845,\n",
       "   0.6027309767159104,\n",
       "   0.6189548482142821,\n",
       "   0.6130742213682824,\n",
       "   0.6130742213682825,\n",
       "   0.6257806166641868,\n",
       "   0.6312580788998395,\n",
       "   0.6299435995535327,\n",
       "   0.6200201112086392,\n",
       "   0.6200201112086393,\n",
       "   0.6227649508790922,\n",
       "   0.6227649508790922,\n",
       "   0.62257034232283,\n",
       "   0.62257034232283,\n",
       "   0.6216782822020644,\n",
       "   0.6215975945594869,\n",
       "   0.6247325633395147,\n",
       "   0.6194617415507834,\n",
       "   0.605045301701782,\n",
       "   0.605137166262138,\n",
       "   0.6017508634636829,\n",
       "   0.6039174776305536,\n",
       "   0.6039332884733123,\n",
       "   0.6308720221147851,\n",
       "   0.6308720221147851],\n",
       "  [0.07637626158259733,\n",
       "   0.0769800358919501,\n",
       "   0.07817359599705716,\n",
       "   0.07916215423518129,\n",
       "   0.08032329580103033,\n",
       "   0.08146788958556003,\n",
       "   0.08146788958556003,\n",
       "   0.08028855854204264,\n",
       "   0.08200018542989271,\n",
       "   0.08124500480186272,\n",
       "   0.08124500480186272,\n",
       "   0.08124500480186272,\n",
       "   0.08181285594481644,\n",
       "   0.08237679279044481,\n",
       "   0.08139937889868842,\n",
       "   0.08141282541276969,\n",
       "   0.08122089436082693,\n",
       "   0.08122089436082693,\n",
       "   0.08123894465895115,\n",
       "   0.08120214023674958,\n",
       "   0.08177028905183922,\n",
       "   0.08177028905183922,\n",
       "   0.08234689355533598,\n",
       "   0.08234689355533598,\n",
       "   0.08290719794328129,\n",
       "   0.08290719794328129,\n",
       "   0.08290719794328129,\n",
       "   0.08179620760393239,\n",
       "   0.08179620760393239,\n",
       "   0.08179620760393239,\n",
       "   0.08163588782647975,\n",
       "   0.07953074500497531,\n",
       "   0.07953074500497531,\n",
       "   0.07933177477482001,\n",
       "   0.07933177477482001,\n",
       "   0.07933177477482001,\n",
       "   0.079913222194549,\n",
       "   0.07967604657648217,\n",
       "   0.07963679205107414,\n",
       "   0.08076732109563702,\n",
       "   0.08095652861120263,\n",
       "   0.07981268122527338,\n",
       "   0.07907361758564148,\n",
       "   0.07887776833899693,\n",
       "   0.07948560869919133,\n",
       "   0.0791996840498438,\n",
       "   0.0791996840498438,\n",
       "   0.07609974744180772,\n",
       "   0.07609974744180772,\n",
       "   0.07609974744180772,\n",
       "   0.07520287527875207,\n",
       "   0.07487087500088385,\n",
       "   0.07461685128708194,\n",
       "   0.0746076587269858,\n",
       "   0.0746307311817078,\n",
       "   0.07532274440196128,\n",
       "   0.0748017195227302,\n",
       "   0.07493990998375728,\n",
       "   0.07494565423147767,\n",
       "   0.07494565423147767,\n",
       "   0.07275989414177768,\n",
       "   0.07217562989692833,\n",
       "   0.07217569118825057,\n",
       "   0.07211520510308668,\n",
       "   0.07210686724468567,\n",
       "   0.07181054766015686,\n",
       "   0.07180310673936875,\n",
       "   0.07140196105358397,\n",
       "   0.07148234125065582,\n",
       "   0.07148234125065582,\n",
       "   0.07192094175747309,\n",
       "   0.06997546830572796,\n",
       "   0.06993157561302833,\n",
       "   0.0698668487979922,\n",
       "   0.06951738953955006,\n",
       "   0.06951738953955006,\n",
       "   0.06935500175224157,\n",
       "   0.06944661107709756,\n",
       "   0.0675769157538812,\n",
       "   0.06758054340259229,\n",
       "   0.0681071758741464,\n",
       "   0.06783054978981146,\n",
       "   0.06835535401632477,\n",
       "   0.06823186306318019,\n",
       "   0.06823186306318019,\n",
       "   0.06759316708161989,\n",
       "   0.06759316708161989,\n",
       "   0.06750613655786933,\n",
       "   0.06750613655786933,\n",
       "   0.06789975596099347,\n",
       "   0.06789990615289214,\n",
       "   0.06792227155997468,\n",
       "   0.0679500107950157,\n",
       "   0.06724346572253678,\n",
       "   0.06723978957586123,\n",
       "   0.0672850114247913,\n",
       "   0.06720423774807519,\n",
       "   0.0672033735236233,\n",
       "   0.06732381009518591,\n",
       "   0.06732381009518591],\n",
       "  array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]])],\n",
       " 3: [[0.3333333333333333,\n",
       "   0.42857142857142855,\n",
       "   0.45454545454545453,\n",
       "   0.4704013546128549,\n",
       "   0.5111397119503276,\n",
       "   0.5111397119503276,\n",
       "   0.3966907704430078,\n",
       "   0.3966907704430078,\n",
       "   0.5206119526626005,\n",
       "   0.46906681882210516,\n",
       "   0.4932001452392822,\n",
       "   0.4932001452392822,\n",
       "   0.5351723750211544,\n",
       "   0.5158566774837093,\n",
       "   0.5158566774837093,\n",
       "   0.5079426367976099,\n",
       "   0.5232855306646749,\n",
       "   0.5275948751427536,\n",
       "   0.5275948751427536,\n",
       "   0.5272714524914103,\n",
       "   0.5934294461666798,\n",
       "   0.6286613424888755,\n",
       "   0.6421876819765415,\n",
       "   0.6420159125593948,\n",
       "   0.5938468008630228,\n",
       "   0.58074508476183,\n",
       "   0.5667699209205577,\n",
       "   0.5671318472218656,\n",
       "   0.6156783722468514,\n",
       "   0.6187232183664941,\n",
       "   0.6340953061258104,\n",
       "   0.6537338236296852,\n",
       "   0.6537338236296852,\n",
       "   0.6537338236296852,\n",
       "   0.6633150196929611,\n",
       "   0.6880169865792437,\n",
       "   0.6880169865792437,\n",
       "   0.6879880077305374,\n",
       "   0.6681903640086645,\n",
       "   0.6924965453832149,\n",
       "   0.6661666458516743,\n",
       "   0.6732935158676743,\n",
       "   0.6732935158676743,\n",
       "   0.6626765691075259,\n",
       "   0.6560179132559266,\n",
       "   0.6560390139379138,\n",
       "   0.6364801304063735,\n",
       "   0.6356969738777102,\n",
       "   0.6462009205920382,\n",
       "   0.6578792590383741,\n",
       "   0.6619862197839508,\n",
       "   0.6601142180089403,\n",
       "   0.6907695418483004,\n",
       "   0.6921324650746951,\n",
       "   0.6916324696481856,\n",
       "   0.6916324696481856,\n",
       "   0.691993310085307,\n",
       "   0.6976938597075331,\n",
       "   0.6977176402650502,\n",
       "   0.6977176402650502,\n",
       "   0.7091398374183239,\n",
       "   0.7079371503053669,\n",
       "   0.7166883639266156,\n",
       "   0.7187868967679243,\n",
       "   0.7177664303092839,\n",
       "   0.7318510163816148,\n",
       "   0.7205248667572485,\n",
       "   0.7226964754915021,\n",
       "   0.7226469434611594,\n",
       "   0.7139672527946803,\n",
       "   0.7139672527946803,\n",
       "   0.7151837417524763,\n",
       "   0.7143978128001263,\n",
       "   0.7143978128001263,\n",
       "   0.7145602333673967,\n",
       "   0.7140083369125874,\n",
       "   0.7140083369125874,\n",
       "   0.7166684425148611,\n",
       "   0.7166684425148611,\n",
       "   0.7167517650496575,\n",
       "   0.7167517650496575,\n",
       "   0.7108645963656842,\n",
       "   0.7093294829413277,\n",
       "   0.7086097445447854,\n",
       "   0.7086724543441254,\n",
       "   0.7086724543441254,\n",
       "   0.7076921208918743,\n",
       "   0.7207011506541062,\n",
       "   0.7207011506541063,\n",
       "   0.7168846294898622,\n",
       "   0.7110278218915728,\n",
       "   0.7087933228668589,\n",
       "   0.7087933228668589,\n",
       "   0.705313742982218,\n",
       "   0.7055359305710619,\n",
       "   0.7055359305710619,\n",
       "   0.6929454323120343,\n",
       "   0.6929454323120342,\n",
       "   0.7039750786067732,\n",
       "   0.7165055188514273],\n",
       "  [0.07576767609436587,\n",
       "   0.07637626158259733,\n",
       "   0.0775791113542719,\n",
       "   0.07755388300019085,\n",
       "   0.07814855955806149,\n",
       "   0.07814855955806149,\n",
       "   0.0768116618812758,\n",
       "   0.0768116618812758,\n",
       "   0.07774364323723282,\n",
       "   0.0783368792867748,\n",
       "   0.07892565646849774,\n",
       "   0.07892575457471235,\n",
       "   0.07928947743800695,\n",
       "   0.07786319359599299,\n",
       "   0.07786319359599299,\n",
       "   0.07771301817902204,\n",
       "   0.07948012941780347,\n",
       "   0.07931730996671453,\n",
       "   0.07843689769519797,\n",
       "   0.07843033294916031,\n",
       "   0.07944320500705083,\n",
       "   0.07878960658318798,\n",
       "   0.07919244061419642,\n",
       "   0.07919018848517667,\n",
       "   0.07701615109114368,\n",
       "   0.07641266214640921,\n",
       "   0.07488270100742613,\n",
       "   0.07457473120182499,\n",
       "   0.07730511909242721,\n",
       "   0.07767894370876738,\n",
       "   0.07740779209856744,\n",
       "   0.07740553179503953,\n",
       "   0.07711605585196361,\n",
       "   0.07711605585196361,\n",
       "   0.07771989532520587,\n",
       "   0.07793591780040397,\n",
       "   0.07771494510698791,\n",
       "   0.07682023785811753,\n",
       "   0.07584177530200317,\n",
       "   0.07420058850726388,\n",
       "   0.07448758838665263,\n",
       "   0.07438547381367544,\n",
       "   0.07438547381367544,\n",
       "   0.07346214980176867,\n",
       "   0.07316792932779868,\n",
       "   0.07316833877656724,\n",
       "   0.07323775461914397,\n",
       "   0.07325981469315034,\n",
       "   0.07268259847849563,\n",
       "   0.07445431043876383,\n",
       "   0.0742427446701972,\n",
       "   0.07426370932604846,\n",
       "   0.07410869633742108,\n",
       "   0.07408485575205115,\n",
       "   0.07410377494451363,\n",
       "   0.07335805958387702,\n",
       "   0.07328119977797193,\n",
       "   0.07312291424939384,\n",
       "   0.07307161904981502,\n",
       "   0.07432058907824936,\n",
       "   0.07409435171656428,\n",
       "   0.07330172875574657,\n",
       "   0.07310013623398945,\n",
       "   0.07292091059460117,\n",
       "   0.07230152227779144,\n",
       "   0.07251987479843268,\n",
       "   0.07188655529220297,\n",
       "   0.07121731191001572,\n",
       "   0.07121819247202954,\n",
       "   0.07114923062745904,\n",
       "   0.07114923062745904,\n",
       "   0.07162217975102882,\n",
       "   0.0716156538245191,\n",
       "   0.07142726842086275,\n",
       "   0.0714230531651583,\n",
       "   0.07133454755990248,\n",
       "   0.07133454755990248,\n",
       "   0.07124658818125353,\n",
       "   0.07124658818125353,\n",
       "   0.07124749449416644,\n",
       "   0.07124749449416644,\n",
       "   0.07127813356159368,\n",
       "   0.07128901742180105,\n",
       "   0.07161343365022696,\n",
       "   0.07161202300271323,\n",
       "   0.07161202300271323,\n",
       "   0.07165975659442754,\n",
       "   0.07343787988190183,\n",
       "   0.07251351910521861,\n",
       "   0.07255306331513511,\n",
       "   0.07250858195387361,\n",
       "   0.07247154659128549,\n",
       "   0.07247154659128549,\n",
       "   0.07250707532995275,\n",
       "   0.07249825840886887,\n",
       "   0.07249825840886887,\n",
       "   0.07019171874708273,\n",
       "   0.06823374958670905,\n",
       "   0.06793476873661375,\n",
       "   0.06767261960542684],\n",
       "  array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]])],\n",
       " 4: [[0.4000000000000001,\n",
       "   0.45454545454545453,\n",
       "   0.35714285714285715,\n",
       "   0.35714285714285715,\n",
       "   0.329642144375716,\n",
       "   0.37502230966012534,\n",
       "   0.3333531641423336,\n",
       "   0.31789663482214237,\n",
       "   0.43478661668057594,\n",
       "   0.39286043513047314,\n",
       "   0.4137962821949395,\n",
       "   0.40000307278844155,\n",
       "   0.40731463153475217,\n",
       "   0.412931926593252,\n",
       "   0.44215154418939523,\n",
       "   0.45131607785897304,\n",
       "   0.4406561660856933,\n",
       "   0.44483400625075603,\n",
       "   0.44483400625075603,\n",
       "   0.4313541878795209,\n",
       "   0.4648697376957561,\n",
       "   0.4648697376957561,\n",
       "   0.47854237703974895,\n",
       "   0.47854237703974895,\n",
       "   0.47854237703974895,\n",
       "   0.5122392383613766,\n",
       "   0.5122392383613766,\n",
       "   0.5121820928695896,\n",
       "   0.5121820928695896,\n",
       "   0.5190689682933392,\n",
       "   0.519068968293339,\n",
       "   0.5011276687569078,\n",
       "   0.5207147013551656,\n",
       "   0.5644599395212019,\n",
       "   0.5427313135623136,\n",
       "   0.5430138599392885,\n",
       "   0.5551561768265203,\n",
       "   0.5604109792941256,\n",
       "   0.5867163418637646,\n",
       "   0.5828991389865802,\n",
       "   0.582715156766454,\n",
       "   0.5933824317902094,\n",
       "   0.5931705426502428,\n",
       "   0.5896692352611633,\n",
       "   0.5980327724610749,\n",
       "   0.6050653186197911,\n",
       "   0.6047425964591315,\n",
       "   0.5773191341370636,\n",
       "   0.5775312468068323,\n",
       "   0.5778245094581805,\n",
       "   0.5778245094581804,\n",
       "   0.5979245247532988,\n",
       "   0.5979245247532988,\n",
       "   0.5924133702922366,\n",
       "   0.595959455905566,\n",
       "   0.5959594559055658,\n",
       "   0.6224015397216685,\n",
       "   0.6224015397216685,\n",
       "   0.6288214065733126,\n",
       "   0.6974589079379935,\n",
       "   0.7162110478537523,\n",
       "   0.7260274285879459,\n",
       "   0.7260274285879459,\n",
       "   0.7258317366430375,\n",
       "   0.7257848326553458,\n",
       "   0.7257848326553458,\n",
       "   0.7291981547036298,\n",
       "   0.6988077591812927,\n",
       "   0.6988077591812927,\n",
       "   0.6988077591812927,\n",
       "   0.6920469673633313,\n",
       "   0.6920469673633313,\n",
       "   0.6920469673633313,\n",
       "   0.6961862888030426,\n",
       "   0.6847028484758224,\n",
       "   0.6812017224906514,\n",
       "   0.6862452669455535,\n",
       "   0.6839200414004835,\n",
       "   0.6834578017240104,\n",
       "   0.669507787748596,\n",
       "   0.6693693293293334,\n",
       "   0.6732639393897273,\n",
       "   0.6703392849362719,\n",
       "   0.6722179317754929,\n",
       "   0.6656183196954307,\n",
       "   0.6656183196954307,\n",
       "   0.6645071294558247,\n",
       "   0.6643186287792493,\n",
       "   0.66465419734414,\n",
       "   0.6866829315840305,\n",
       "   0.6926075568476947,\n",
       "   0.7036469702527693,\n",
       "   0.6999976682580724,\n",
       "   0.6999976682580724,\n",
       "   0.698494569863846,\n",
       "   0.7002991388306391,\n",
       "   0.7067151828956251,\n",
       "   0.7062782880631855,\n",
       "   0.7072773769668813,\n",
       "   0.7066652025907247],\n",
       "  [0.07576767609436587,\n",
       "   0.0775791113542719,\n",
       "   0.0775791113542719,\n",
       "   0.0775791113542719,\n",
       "   0.07691515415848778,\n",
       "   0.07728114677726705,\n",
       "   0.07728114677726705,\n",
       "   0.07723642279724283,\n",
       "   0.07952471529032414,\n",
       "   0.08010476224669608,\n",
       "   0.08068063910996429,\n",
       "   0.08068063910996429,\n",
       "   0.08063747084932228,\n",
       "   0.08120957023385787,\n",
       "   0.08169883872298493,\n",
       "   0.08091423337295763,\n",
       "   0.0807980878294131,\n",
       "   0.08057654452835723,\n",
       "   0.08057654452835723,\n",
       "   0.08057654452835723,\n",
       "   0.08094295809953339,\n",
       "   0.08094295809953339,\n",
       "   0.08094295809953339,\n",
       "   0.08094295809953339,\n",
       "   0.08094295809953339,\n",
       "   0.08115560756229159,\n",
       "   0.08115560756229159,\n",
       "   0.0811577345495912,\n",
       "   0.07954940186882019,\n",
       "   0.07854033400760732,\n",
       "   0.07854033400760732,\n",
       "   0.07796560542586117,\n",
       "   0.07797361651002957,\n",
       "   0.07701415594660557,\n",
       "   0.07615520886636673,\n",
       "   0.0761566645058163,\n",
       "   0.07511597720048116,\n",
       "   0.07491848073075633,\n",
       "   0.0744372034121557,\n",
       "   0.0738424698588333,\n",
       "   0.07344938988444516,\n",
       "   0.07264307777171679,\n",
       "   0.07265073115618585,\n",
       "   0.07258835924133149,\n",
       "   0.07229964485729863,\n",
       "   0.0728650154276541,\n",
       "   0.07265569414210378,\n",
       "   0.07137583560052302,\n",
       "   0.070382326293293,\n",
       "   0.07037826016059552,\n",
       "   0.07037826016059552,\n",
       "   0.06998929800800657,\n",
       "   0.06998929800800657,\n",
       "   0.0700997090308088,\n",
       "   0.0699694201560081,\n",
       "   0.06953195558522487,\n",
       "   0.06982032336451001,\n",
       "   0.06982032336451001,\n",
       "   0.06962523142634562,\n",
       "   0.06987269145710918,\n",
       "   0.07016456357379099,\n",
       "   0.07040420890724389,\n",
       "   0.07040420890724389,\n",
       "   0.07040598343748869,\n",
       "   0.07040564483781411,\n",
       "   0.07040564483781411,\n",
       "   0.07015068436163965,\n",
       "   0.06933642758168959,\n",
       "   0.06952672293837882,\n",
       "   0.06952672293837882,\n",
       "   0.06946085503456856,\n",
       "   0.06836316437704446,\n",
       "   0.06836316437704446,\n",
       "   0.06827516245646446,\n",
       "   0.06828118762852808,\n",
       "   0.06832339315204623,\n",
       "   0.06790904075143923,\n",
       "   0.06797386722273316,\n",
       "   0.06797788024986505,\n",
       "   0.06803369762316537,\n",
       "   0.06700478933172477,\n",
       "   0.06724974945861047,\n",
       "   0.06728680285718557,\n",
       "   0.06725183524345173,\n",
       "   0.06733337837453893,\n",
       "   0.06733337837453893,\n",
       "   0.06747917429152403,\n",
       "   0.06711872208071071,\n",
       "   0.06684595832814773,\n",
       "   0.06749961932068631,\n",
       "   0.06725687019868079,\n",
       "   0.06705246463097586,\n",
       "   0.06806240415959028,\n",
       "   0.06806277276528451,\n",
       "   0.06809727529087273,\n",
       "   0.06806483324640662,\n",
       "   0.06788270122885494,\n",
       "   0.06788203646120411,\n",
       "   0.06716805785456008,\n",
       "   0.0671804400932937],\n",
       "  array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]])],\n",
       " 5: [[0.75,\n",
       "   0.5,\n",
       "   0.5384615384615384,\n",
       "   0.5625,\n",
       "   0.47619047619047616,\n",
       "   0.5,\n",
       "   0.4683856286528467,\n",
       "   0.44795988893175853,\n",
       "   0.41212309781721784,\n",
       "   0.43372622381385856,\n",
       "   0.43372622381385856,\n",
       "   0.41766228959853047,\n",
       "   0.3574558418867441,\n",
       "   0.3704686043726721,\n",
       "   0.3847173968485441,\n",
       "   0.44454267844674616,\n",
       "   0.48954307276675846,\n",
       "   0.5645722065657913,\n",
       "   0.5645722065657913,\n",
       "   0.5645722065657913,\n",
       "   0.5795869580635227,\n",
       "   0.5795869580635227,\n",
       "   0.559228487809872,\n",
       "   0.559228487809872,\n",
       "   0.559228487809872,\n",
       "   0.573920871549543,\n",
       "   0.5937112464305617,\n",
       "   0.594943318426094,\n",
       "   0.5633064042080859,\n",
       "   0.5633064042080859,\n",
       "   0.5712847662671898,\n",
       "   0.5976651043109978,\n",
       "   0.5976651043109978,\n",
       "   0.6215417261248491,\n",
       "   0.6214362444030697,\n",
       "   0.6214362444030697,\n",
       "   0.585721958688784,\n",
       "   0.585721958688784,\n",
       "   0.585721958688784,\n",
       "   0.6157468841149668,\n",
       "   0.6157468841149668,\n",
       "   0.6157468841149668,\n",
       "   0.6095531406295958,\n",
       "   0.6095531406295958,\n",
       "   0.6099163460876712,\n",
       "   0.6233675065674066,\n",
       "   0.6233675065674068,\n",
       "   0.6379875670822709,\n",
       "   0.6360628188931066,\n",
       "   0.6098798161133884,\n",
       "   0.6281760434150558,\n",
       "   0.619949042232647,\n",
       "   0.619949042232647,\n",
       "   0.6479524798714845,\n",
       "   0.6476380678185159,\n",
       "   0.6885095508354295,\n",
       "   0.683436997758706,\n",
       "   0.661381221236523,\n",
       "   0.661381221236523,\n",
       "   0.6677138613305382,\n",
       "   0.6677128157876816,\n",
       "   0.6533330342544083,\n",
       "   0.6606775515620696,\n",
       "   0.6833756191982806,\n",
       "   0.6865096746324435,\n",
       "   0.6865096746324435,\n",
       "   0.7080756718012606,\n",
       "   0.7076580532902109,\n",
       "   0.7065826173915715,\n",
       "   0.7060832728834207,\n",
       "   0.7060832728834207,\n",
       "   0.6951686549837869,\n",
       "   0.6951686549837869,\n",
       "   0.7126276994244227,\n",
       "   0.7139059767131434,\n",
       "   0.7139059767131434,\n",
       "   0.7078921479870711,\n",
       "   0.7079004403810435,\n",
       "   0.7022348208841358,\n",
       "   0.7022348208841358,\n",
       "   0.7026317371667432,\n",
       "   0.698978933345236,\n",
       "   0.6989786020719215,\n",
       "   0.7007052351955285,\n",
       "   0.7007052351955285,\n",
       "   0.7066501509398168,\n",
       "   0.7053611907714662,\n",
       "   0.7050978242884934,\n",
       "   0.6904986432992143,\n",
       "   0.6823473897513512,\n",
       "   0.6824962212942305,\n",
       "   0.6823278450585819,\n",
       "   0.6801157549625766,\n",
       "   0.6735392610495535,\n",
       "   0.6796825442736814,\n",
       "   0.679489605749656,\n",
       "   0.6795677869643623,\n",
       "   0.6757730414441047,\n",
       "   0.6889684758097102,\n",
       "   0.6889684758097102],\n",
       "  [0.07637626158259733,\n",
       "   0.07637626158259733,\n",
       "   0.07876359377087681,\n",
       "   0.07993052538854532,\n",
       "   0.08050764858994132,\n",
       "   0.0810806639962579,\n",
       "   0.08029035399394381,\n",
       "   0.08029063425832364,\n",
       "   0.08029063425832364,\n",
       "   0.08080635539679884,\n",
       "   0.08080635539679884,\n",
       "   0.08080635539679884,\n",
       "   0.0801420456589752,\n",
       "   0.08022627891197988,\n",
       "   0.07935711178194704,\n",
       "   0.08051544184538531,\n",
       "   0.0809447662772852,\n",
       "   0.08049204452851542,\n",
       "   0.08049204452851542,\n",
       "   0.08075576559233752,\n",
       "   0.08132703283040121,\n",
       "   0.08132703283040121,\n",
       "   0.08069749591019247,\n",
       "   0.08069749591019247,\n",
       "   0.08069749591019247,\n",
       "   0.0797957410260215,\n",
       "   0.0798316541823091,\n",
       "   0.07816938792193943,\n",
       "   0.07806986217125539,\n",
       "   0.07806986217125539,\n",
       "   0.07789329478660323,\n",
       "   0.07661561180669962,\n",
       "   0.07661561180669962,\n",
       "   0.07690908618434589,\n",
       "   0.0769096955319511,\n",
       "   0.0769096955319511,\n",
       "   0.0763053646490522,\n",
       "   0.07563197406556695,\n",
       "   0.07563197406556695,\n",
       "   0.07629861352427678,\n",
       "   0.07629861352427678,\n",
       "   0.07629861352427678,\n",
       "   0.07620702768627215,\n",
       "   0.07620702768627215,\n",
       "   0.07620659004071569,\n",
       "   0.07681169805587101,\n",
       "   0.07681169805587101,\n",
       "   0.07671045287052679,\n",
       "   0.0765772639291985,\n",
       "   0.07578261356051989,\n",
       "   0.07565017364288186,\n",
       "   0.07545015206271487,\n",
       "   0.07545015206271487,\n",
       "   0.0760706442282183,\n",
       "   0.0760834575107384,\n",
       "   0.077004124585946,\n",
       "   0.07640764918889345,\n",
       "   0.07583392841320327,\n",
       "   0.07583392841320327,\n",
       "   0.07575289913177795,\n",
       "   0.07575284084651182,\n",
       "   0.0759205443466825,\n",
       "   0.07496668881817477,\n",
       "   0.0751569729788076,\n",
       "   0.0752371448762543,\n",
       "   0.0752371448762543,\n",
       "   0.07584292292593378,\n",
       "   0.07515689919640421,\n",
       "   0.07479148065914053,\n",
       "   0.07431924823421714,\n",
       "   0.07418173476066482,\n",
       "   0.07430286997319598,\n",
       "   0.07432498755829363,\n",
       "   0.07438470682224115,\n",
       "   0.07436249411051546,\n",
       "   0.07436249411051546,\n",
       "   0.07461820841800466,\n",
       "   0.07461830973733398,\n",
       "   0.07464276585060665,\n",
       "   0.07372135995223224,\n",
       "   0.07376911329389615,\n",
       "   0.07451616485452194,\n",
       "   0.07451615982145642,\n",
       "   0.07445567197566769,\n",
       "   0.07445567197566769,\n",
       "   0.07509387342106219,\n",
       "   0.07548808939164692,\n",
       "   0.07546491805607271,\n",
       "   0.0752358464230221,\n",
       "   0.07588847622532248,\n",
       "   0.07591515650693764,\n",
       "   0.07591961272404626,\n",
       "   0.07576362587021405,\n",
       "   0.07571620590653778,\n",
       "   0.0749583113035426,\n",
       "   0.07496250210571984,\n",
       "   0.07506687947058355,\n",
       "   0.07505135326014269,\n",
       "   0.07456277917671705,\n",
       "   0.07456034399271212],\n",
       "  array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]])],\n",
       " 6: [[0.5555555555555556,\n",
       "   0.5454545454545454,\n",
       "   0.3,\n",
       "   0.3,\n",
       "   0.2525306215860977,\n",
       "   0.24050535389152158,\n",
       "   0.37710885132174804,\n",
       "   0.37710885132174804,\n",
       "   0.4052866681054978,\n",
       "   0.40572544726701876,\n",
       "   0.39012062237213346,\n",
       "   0.39012062237213346,\n",
       "   0.3722063212826731,\n",
       "   0.3578906935410318,\n",
       "   0.38453618850208837,\n",
       "   0.3728835767292978,\n",
       "   0.3801473723447665,\n",
       "   0.38662589275856285,\n",
       "   0.406437550855266,\n",
       "   0.406437550855266,\n",
       "   0.4081829652859035,\n",
       "   0.3695935952600686,\n",
       "   0.3695935952600686,\n",
       "   0.394066995417897,\n",
       "   0.394066995417897,\n",
       "   0.3978233452053888,\n",
       "   0.3771364219491995,\n",
       "   0.3771364219491995,\n",
       "   0.3661385967788704,\n",
       "   0.41682486633607946,\n",
       "   0.41682486633607946,\n",
       "   0.41682486633607946,\n",
       "   0.41682486633607946,\n",
       "   0.41682486633607946,\n",
       "   0.4278767838256647,\n",
       "   0.4278767838256647,\n",
       "   0.4272959987335124,\n",
       "   0.42472779963812807,\n",
       "   0.4130000366893666,\n",
       "   0.46273814101168254,\n",
       "   0.48293753124994576,\n",
       "   0.49253604624208197,\n",
       "   0.4870375994356409,\n",
       "   0.48813373283554035,\n",
       "   0.48813373283554035,\n",
       "   0.48813373283554035,\n",
       "   0.4895282231968598,\n",
       "   0.4883060768266764,\n",
       "   0.5034912142491196,\n",
       "   0.5034912142491196,\n",
       "   0.5031443602990016,\n",
       "   0.5031443602990016,\n",
       "   0.5031443602990017,\n",
       "   0.5181577323765736,\n",
       "   0.5525858823341953,\n",
       "   0.6186552158982722,\n",
       "   0.6349957192585033,\n",
       "   0.6408896498913996,\n",
       "   0.6408896498913996,\n",
       "   0.6419099543592506,\n",
       "   0.6419099543592506,\n",
       "   0.6337646593747108,\n",
       "   0.6337646593747108,\n",
       "   0.6337646593747108,\n",
       "   0.6339516839886268,\n",
       "   0.6313995145135262,\n",
       "   0.624633112909118,\n",
       "   0.6246342287691778,\n",
       "   0.6479612774124309,\n",
       "   0.6410304340923734,\n",
       "   0.6405829081293473,\n",
       "   0.6409750886875045,\n",
       "   0.6429262195658819,\n",
       "   0.6429262195658819,\n",
       "   0.6359437933000115,\n",
       "   0.6454667682575247,\n",
       "   0.650814841922902,\n",
       "   0.6545597436594273,\n",
       "   0.6545597436594273,\n",
       "   0.6543881882469662,\n",
       "   0.6759361573511711,\n",
       "   0.689799454012103,\n",
       "   0.7097349993570095,\n",
       "   0.7376843659054982,\n",
       "   0.7376952396513009,\n",
       "   0.7270563476501855,\n",
       "   0.7270563476501856,\n",
       "   0.7260282070199138,\n",
       "   0.7241315595712159,\n",
       "   0.7297165938178011,\n",
       "   0.7295150600859835,\n",
       "   0.7293526355524179,\n",
       "   0.7334301460924321,\n",
       "   0.7386018907679471,\n",
       "   0.7408702354272052,\n",
       "   0.7408990904790663,\n",
       "   0.7408990904790663,\n",
       "   0.7494801957187198,\n",
       "   0.731707983256155,\n",
       "   0.7308730557710956],\n",
       "  [0.0775791113542719,\n",
       "   0.07817359599705716,\n",
       "   0.07817359599705716,\n",
       "   0.07817359599705716,\n",
       "   0.07752327009744218,\n",
       "   0.07752327009744218,\n",
       "   0.07900304135851788,\n",
       "   0.07900304135851788,\n",
       "   0.07949787877877759,\n",
       "   0.07948665379270478,\n",
       "   0.07948665379270478,\n",
       "   0.07948665379270478,\n",
       "   0.07877104490418992,\n",
       "   0.07877104490418992,\n",
       "   0.08051493832249816,\n",
       "   0.08051493832249816,\n",
       "   0.0810879022152397,\n",
       "   0.08165684587504508,\n",
       "   0.07974527025094418,\n",
       "   0.07974527025094418,\n",
       "   0.07909575778468023,\n",
       "   0.0771712345846069,\n",
       "   0.07717152156472988,\n",
       "   0.0769951160076444,\n",
       "   0.0769951160076444,\n",
       "   0.0768765347561929,\n",
       "   0.07627566762235696,\n",
       "   0.07627566762235696,\n",
       "   0.07484106648417123,\n",
       "   0.07581544341942556,\n",
       "   0.07581544341942556,\n",
       "   0.07581544341942556,\n",
       "   0.07581544341942556,\n",
       "   0.07581544341942556,\n",
       "   0.075620829561142,\n",
       "   0.07558148042967996,\n",
       "   0.07550384811492762,\n",
       "   0.07475234771563102,\n",
       "   0.07464160241857501,\n",
       "   0.0764635206651899,\n",
       "   0.07652464706653372,\n",
       "   0.07647334807335221,\n",
       "   0.07651709334083553,\n",
       "   0.0754677680579894,\n",
       "   0.0754677680579894,\n",
       "   0.0754677680579894,\n",
       "   0.07548770834768145,\n",
       "   0.07534775162880247,\n",
       "   0.07596504977575486,\n",
       "   0.07596504977575486,\n",
       "   0.07597576043044305,\n",
       "   0.07597576043044305,\n",
       "   0.07597576043044305,\n",
       "   0.07658406320839593,\n",
       "   0.07763749751755308,\n",
       "   0.07729093458610332,\n",
       "   0.07716735762414265,\n",
       "   0.07700200560770021,\n",
       "   0.07700200560770021,\n",
       "   0.07698659336238248,\n",
       "   0.07701285761113165,\n",
       "   0.07699790082946535,\n",
       "   0.07609201317549481,\n",
       "   0.07609201317549481,\n",
       "   0.07524869191375094,\n",
       "   0.07526678577741655,\n",
       "   0.07544925152453223,\n",
       "   0.07544921509751612,\n",
       "   0.07405893174103355,\n",
       "   0.07408379446628777,\n",
       "   0.07409365402596686,\n",
       "   0.0740882672124705,\n",
       "   0.07401207541043471,\n",
       "   0.07401207541043471,\n",
       "   0.07393305079182587,\n",
       "   0.07376608932023739,\n",
       "   0.07358666626552016,\n",
       "   0.07349956512222008,\n",
       "   0.07349956512222008,\n",
       "   0.07350131622133631,\n",
       "   0.07348597669893997,\n",
       "   0.07290955631244161,\n",
       "   0.07243919689509656,\n",
       "   0.07210114427490509,\n",
       "   0.07210082228236317,\n",
       "   0.07198588678076177,\n",
       "   0.07201826496981542,\n",
       "   0.07202322035719307,\n",
       "   0.07204543541243812,\n",
       "   0.07187004210053216,\n",
       "   0.07174961091824408,\n",
       "   0.07174960432194374,\n",
       "   0.07168049100649289,\n",
       "   0.07153042774454312,\n",
       "   0.0717687186026519,\n",
       "   0.0717356632602841,\n",
       "   0.0717356632602841,\n",
       "   0.07259724068327254,\n",
       "   0.07289367662792727,\n",
       "   0.07289713427184082],\n",
       "  array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]])],\n",
       " 7: [[0.36363636363636365,\n",
       "   0.47619047619047616,\n",
       "   0.4583333333333333,\n",
       "   0.48148148148148145,\n",
       "   0.5185185185185185,\n",
       "   0.5384615384615384,\n",
       "   0.5945945945945946,\n",
       "   0.5713204062654189,\n",
       "   0.5835861326545916,\n",
       "   0.5744353523299025,\n",
       "   0.5744353523299025,\n",
       "   0.6138518041584327,\n",
       "   0.6138518041584327,\n",
       "   0.6342570133302949,\n",
       "   0.6342570133302949,\n",
       "   0.6342570133302949,\n",
       "   0.6342570133302948,\n",
       "   0.6420827715004969,\n",
       "   0.6845820284456855,\n",
       "   0.701802729722835,\n",
       "   0.6931467813781633,\n",
       "   0.6623694554077272,\n",
       "   0.6623694554077272,\n",
       "   0.6623694554077272,\n",
       "   0.6623694554077272,\n",
       "   0.6506405775137779,\n",
       "   0.6514719115428319,\n",
       "   0.6498296440619636,\n",
       "   0.6503424494058567,\n",
       "   0.6503424494058567,\n",
       "   0.6804711560502745,\n",
       "   0.6637213755669424,\n",
       "   0.6636950181889425,\n",
       "   0.6655979201106265,\n",
       "   0.6617024323988075,\n",
       "   0.6712797765571731,\n",
       "   0.6916215879679964,\n",
       "   0.7180831771523646,\n",
       "   0.6969630837067068,\n",
       "   0.6877801468493343,\n",
       "   0.6862118831670848,\n",
       "   0.6661488776026907,\n",
       "   0.6660676401505908,\n",
       "   0.6537929397115371,\n",
       "   0.6532026333657751,\n",
       "   0.6441907137560237,\n",
       "   0.6441907137560237,\n",
       "   0.6330819368016715,\n",
       "   0.6277059488554905,\n",
       "   0.6467273412450508,\n",
       "   0.65482701350064,\n",
       "   0.6506068930452045,\n",
       "   0.6849307882245667,\n",
       "   0.6849307882245667,\n",
       "   0.6895932225766604,\n",
       "   0.6862298487578243,\n",
       "   0.6826618080752742,\n",
       "   0.6826618080752742,\n",
       "   0.6708399003276843,\n",
       "   0.6658548181042511,\n",
       "   0.6654805473650512,\n",
       "   0.6579324913100164,\n",
       "   0.6575150326322835,\n",
       "   0.6565752707558826,\n",
       "   0.6542149988940572,\n",
       "   0.6502921627803189,\n",
       "   0.6423881521117618,\n",
       "   0.6423808737946282,\n",
       "   0.6479562917483539,\n",
       "   0.6487722372579671,\n",
       "   0.647974598252642,\n",
       "   0.6515021087312672,\n",
       "   0.6513175401178722,\n",
       "   0.6419924493216717,\n",
       "   0.6418796477281739,\n",
       "   0.6334796300076898,\n",
       "   0.6382159764416598,\n",
       "   0.6382159764416598,\n",
       "   0.6462821919282545,\n",
       "   0.6606246877496309,\n",
       "   0.6629847831572706,\n",
       "   0.6629847831572706,\n",
       "   0.6900709967151955,\n",
       "   0.6900597422368219,\n",
       "   0.6860269286014639,\n",
       "   0.6860269286014639,\n",
       "   0.7059078971112549,\n",
       "   0.7007055536970833,\n",
       "   0.7200826686625851,\n",
       "   0.7200826686625851,\n",
       "   0.7200826686625851,\n",
       "   0.7200826686625851,\n",
       "   0.7207424298690885,\n",
       "   0.7199888257016087,\n",
       "   0.7449119263085237,\n",
       "   0.743514511278544,\n",
       "   0.7427117110282062,\n",
       "   0.7336328342620696,\n",
       "   0.7217867195239833,\n",
       "   0.7101173879549608],\n",
       "  [0.0769800358919501,\n",
       "   0.08050764858994133,\n",
       "   0.0810806639962579,\n",
       "   0.08221471437193746,\n",
       "   0.08243861271147761,\n",
       "   0.08243861271147761,\n",
       "   0.0868151231441489,\n",
       "   0.08367749842441673,\n",
       "   0.08378913748048204,\n",
       "   0.08361771609036728,\n",
       "   0.08301472621049526,\n",
       "   0.08269022189042634,\n",
       "   0.08269022189042634,\n",
       "   0.08342039621360228,\n",
       "   0.08342039621360228,\n",
       "   0.08243851391946647,\n",
       "   0.08243851391946647,\n",
       "   0.08067886155568602,\n",
       "   0.08075387140598413,\n",
       "   0.08145988089176394,\n",
       "   0.08050378438083466,\n",
       "   0.07990998499071345,\n",
       "   0.07990998499071345,\n",
       "   0.07990998499071345,\n",
       "   0.07990998499071345,\n",
       "   0.07928257580640381,\n",
       "   0.07721045891162652,\n",
       "   0.07718128739760857,\n",
       "   0.07640687374033889,\n",
       "   0.07640687374033889,\n",
       "   0.07732905880178521,\n",
       "   0.07782148078209766,\n",
       "   0.07601912185136452,\n",
       "   0.07608394904022268,\n",
       "   0.07536559830458736,\n",
       "   0.0752231157281095,\n",
       "   0.0752231157281095,\n",
       "   0.0759180206611955,\n",
       "   0.0759180206611955,\n",
       "   0.07530573197653094,\n",
       "   0.07593046571052761,\n",
       "   0.0750054293941928,\n",
       "   0.07502502207024285,\n",
       "   0.07417022584546647,\n",
       "   0.0741323351910289,\n",
       "   0.07479286290506942,\n",
       "   0.07479286290506942,\n",
       "   0.07485618170955743,\n",
       "   0.07478773455530822,\n",
       "   0.07478773455530822,\n",
       "   0.07462584699634922,\n",
       "   0.07395562082000433,\n",
       "   0.07155493277962773,\n",
       "   0.07155493277962773,\n",
       "   0.07138860989877092,\n",
       "   0.0714235879566322,\n",
       "   0.07146573650355333,\n",
       "   0.07198044261950604,\n",
       "   0.07142260855295626,\n",
       "   0.07053635854232096,\n",
       "   0.07006553292695594,\n",
       "   0.06969942550119107,\n",
       "   0.06970476485222962,\n",
       "   0.06940893072904092,\n",
       "   0.0694974274629832,\n",
       "   0.06957701528070794,\n",
       "   0.06939188034846601,\n",
       "   0.06939218880165185,\n",
       "   0.06980617648207008,\n",
       "   0.07003860713202675,\n",
       "   0.07005123153035836,\n",
       "   0.06994427667209176,\n",
       "   0.06994812603353603,\n",
       "   0.06882356486047207,\n",
       "   0.06882842512138049,\n",
       "   0.06881536859298232,\n",
       "   0.06874340591415182,\n",
       "   0.06874340591415182,\n",
       "   0.06871663860541347,\n",
       "   0.06898033742993945,\n",
       "   0.06877482091204543,\n",
       "   0.06877482091204543,\n",
       "   0.06873453822118217,\n",
       "   0.06873383351253386,\n",
       "   0.06911004722495612,\n",
       "   0.06911659572806847,\n",
       "   0.06909043430287819,\n",
       "   0.06919837829527058,\n",
       "   0.0690209733365302,\n",
       "   0.0690209733365302,\n",
       "   0.0690209733365302,\n",
       "   0.0690209733365302,\n",
       "   0.06797574382055403,\n",
       "   0.06825724309508518,\n",
       "   0.0685306744758439,\n",
       "   0.06815407989246215,\n",
       "   0.06809806183438497,\n",
       "   0.06822559315344347,\n",
       "   0.06750087973647682,\n",
       "   0.06775506490783993],\n",
       "  array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]])],\n",
       " 8: [[0.3333333333333333,\n",
       "   0.2858965794465502,\n",
       "   0.2933865932181476,\n",
       "   0.2933865932181476,\n",
       "   0.3287172635572402,\n",
       "   0.3287172635572402,\n",
       "   0.2866941256873133,\n",
       "   0.27366257451970816,\n",
       "   0.27366257451970816,\n",
       "   0.26176420171450343,\n",
       "   0.2584725879987983,\n",
       "   0.28457386010018076,\n",
       "   0.4011104438414312,\n",
       "   0.3926776158053586,\n",
       "   0.4360343250462398,\n",
       "   0.48115157904254063,\n",
       "   0.517865568309779,\n",
       "   0.5364092002978643,\n",
       "   0.5364092002978643,\n",
       "   0.566398590400286,\n",
       "   0.6011211830809655,\n",
       "   0.6011211830809655,\n",
       "   0.6011211830809655,\n",
       "   0.5836206111301818,\n",
       "   0.5823133836696633,\n",
       "   0.5823133836696633,\n",
       "   0.5823133836696633,\n",
       "   0.5949705538614918,\n",
       "   0.6217887785203234,\n",
       "   0.6359296312426768,\n",
       "   0.6359296312426768,\n",
       "   0.6359296312426769,\n",
       "   0.6304123900001759,\n",
       "   0.6312055768885896,\n",
       "   0.6312055768885896,\n",
       "   0.6368971370895821,\n",
       "   0.6368971370895821,\n",
       "   0.6368971370895821,\n",
       "   0.6355372564335678,\n",
       "   0.6355372564335678,\n",
       "   0.6355372564335678,\n",
       "   0.6254836809037219,\n",
       "   0.6032867857400017,\n",
       "   0.6210305147323547,\n",
       "   0.5995117193224703,\n",
       "   0.5995140580683591,\n",
       "   0.5995140580683591,\n",
       "   0.5974285407515892,\n",
       "   0.5702457274749544,\n",
       "   0.5702457274749544,\n",
       "   0.5724071325025272,\n",
       "   0.5712825992295553,\n",
       "   0.5712577601580896,\n",
       "   0.5712577601580896,\n",
       "   0.568057981850812,\n",
       "   0.5672100556376997,\n",
       "   0.5687895526233363,\n",
       "   0.5682186847634044,\n",
       "   0.5670506170125247,\n",
       "   0.5670506170125247,\n",
       "   0.564022377094905,\n",
       "   0.5640143088806411,\n",
       "   0.562899819363678,\n",
       "   0.562899819363678,\n",
       "   0.5690343916471893,\n",
       "   0.5541610536504263,\n",
       "   0.550858491499266,\n",
       "   0.549155514082475,\n",
       "   0.5706478619348069,\n",
       "   0.5706478619348069,\n",
       "   0.5706478619348069,\n",
       "   0.5706478619348069,\n",
       "   0.5706478619348069,\n",
       "   0.5732135095077117,\n",
       "   0.5776405987837415,\n",
       "   0.5776405987837415,\n",
       "   0.5862255215925155,\n",
       "   0.5909390361418888,\n",
       "   0.591788094107095,\n",
       "   0.591788094107095,\n",
       "   0.5941996133296187,\n",
       "   0.5907844260357469,\n",
       "   0.600119261101544,\n",
       "   0.5976498063150237,\n",
       "   0.5978554394622245,\n",
       "   0.5978554394622245,\n",
       "   0.6045133002990101,\n",
       "   0.6036240204682605,\n",
       "   0.6166314752238927,\n",
       "   0.6112488059622121,\n",
       "   0.6131757560193464,\n",
       "   0.6146197538833801,\n",
       "   0.6146197538833801,\n",
       "   0.6146197538833801,\n",
       "   0.635649265092759,\n",
       "   0.6393958118105164,\n",
       "   0.6393958118105164,\n",
       "   0.6393958118105163,\n",
       "   0.6441567008355062,\n",
       "   0.6477447043864953],\n",
       "  [0.07637626158259733,\n",
       "   0.07688430878850425,\n",
       "   0.07740926044941308,\n",
       "   0.07748412437705329,\n",
       "   0.0780793322401726,\n",
       "   0.0780793322401726,\n",
       "   0.07814997269934121,\n",
       "   0.07814997269934121,\n",
       "   0.07814997269934121,\n",
       "   0.07814997269934121,\n",
       "   0.07755429777962364,\n",
       "   0.07745917902964765,\n",
       "   0.07820085994418931,\n",
       "   0.07680948494027844,\n",
       "   0.07740320601020413,\n",
       "   0.0785903396470793,\n",
       "   0.08006427196898737,\n",
       "   0.08027915613976358,\n",
       "   0.08027915613976358,\n",
       "   0.0794173586139623,\n",
       "   0.08158435507371496,\n",
       "   0.08158435507371496,\n",
       "   0.08158435507371496,\n",
       "   0.08084373576228773,\n",
       "   0.08083141232113793,\n",
       "   0.08071210916665691,\n",
       "   0.08023586819403003,\n",
       "   0.0808108107708514,\n",
       "   0.0810190842169401,\n",
       "   0.08087324366636425,\n",
       "   0.08087324366636425,\n",
       "   0.0800514566452456,\n",
       "   0.07983619808124816,\n",
       "   0.07986140062354724,\n",
       "   0.07986140062354724,\n",
       "   0.0796673452734203,\n",
       "   0.0796673452734203,\n",
       "   0.0796673452734203,\n",
       "   0.077885882743811,\n",
       "   0.07852332315127963,\n",
       "   0.07852332315127963,\n",
       "   0.07783469116889284,\n",
       "   0.07711969638349399,\n",
       "   0.07711969638349399,\n",
       "   0.07621492450155988,\n",
       "   0.07621499354768846,\n",
       "   0.07621499354768846,\n",
       "   0.07595971417900144,\n",
       "   0.0752666817648841,\n",
       "   0.07526888292744886,\n",
       "   0.07428047944303263,\n",
       "   0.07503002357089561,\n",
       "   0.07502189717993045,\n",
       "   0.07502189717993045,\n",
       "   0.0745312532613937,\n",
       "   0.07383289160092245,\n",
       "   0.07360076149381,\n",
       "   0.07361522489346684,\n",
       "   0.07366171065788134,\n",
       "   0.07366171065788134,\n",
       "   0.07289500734735986,\n",
       "   0.07289533602491753,\n",
       "   0.07318686939016668,\n",
       "   0.07318686939016668,\n",
       "   0.073123063905729,\n",
       "   0.0732863034709273,\n",
       "   0.07306868309176645,\n",
       "   0.0729017741911029,\n",
       "   0.07350603244093044,\n",
       "   0.07350603244093044,\n",
       "   0.07350603244093044,\n",
       "   0.07350603244093044,\n",
       "   0.07350603244093044,\n",
       "   0.07258168721282725,\n",
       "   0.07240021836745203,\n",
       "   0.07123358628001757,\n",
       "   0.07118871824548162,\n",
       "   0.0709918248738867,\n",
       "   0.07119987110503857,\n",
       "   0.07119987110503857,\n",
       "   0.07083256498480486,\n",
       "   0.0713644893005247,\n",
       "   0.07118209713798594,\n",
       "   0.07119234491435394,\n",
       "   0.07123971084156051,\n",
       "   0.07123971084156051,\n",
       "   0.07104512977046178,\n",
       "   0.07101290450272552,\n",
       "   0.07088580344042954,\n",
       "   0.07085861679384,\n",
       "   0.07086038445438833,\n",
       "   0.0706972260216619,\n",
       "   0.0706972260216619,\n",
       "   0.0706972260216619,\n",
       "   0.07063479783303046,\n",
       "   0.07068868338209586,\n",
       "   0.07068868338209586,\n",
       "   0.07068745246485612,\n",
       "   0.07053084767334722,\n",
       "   0.07038927611137281],\n",
       "  array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]])],\n",
       " 9: [[0.5555555555555556,\n",
       "   0.5,\n",
       "   0.45454545454545453,\n",
       "   0.4074074074074074,\n",
       "   0.45049839867168134,\n",
       "   0.5100469611120464,\n",
       "   0.5094190260425434,\n",
       "   0.5094190260425434,\n",
       "   0.4553002324568787,\n",
       "   0.4590062883637462,\n",
       "   0.47164712924872293,\n",
       "   0.47164712924872293,\n",
       "   0.47753646698981667,\n",
       "   0.49128550733218995,\n",
       "   0.4916381618020374,\n",
       "   0.5137807080772997,\n",
       "   0.519181957706247,\n",
       "   0.5204571826119763,\n",
       "   0.5204571826119763,\n",
       "   0.5071531070400067,\n",
       "   0.4948319347160068,\n",
       "   0.4966711289017012,\n",
       "   0.5124768917276523,\n",
       "   0.5124768917276523,\n",
       "   0.5259110467903148,\n",
       "   0.5259110467903146,\n",
       "   0.5259110467903148,\n",
       "   0.5450976809684471,\n",
       "   0.5696923216272907,\n",
       "   0.5696923216272907,\n",
       "   0.5696923216272907,\n",
       "   0.5493911657590042,\n",
       "   0.5584460179270353,\n",
       "   0.5759691315947097,\n",
       "   0.5697616171133228,\n",
       "   0.6190435060590819,\n",
       "   0.6186272199383349,\n",
       "   0.6186272199383349,\n",
       "   0.6411825862588334,\n",
       "   0.6312154358771344,\n",
       "   0.6743469336361785,\n",
       "   0.6743469336361784,\n",
       "   0.6859564500922849,\n",
       "   0.6866332487894581,\n",
       "   0.686633248789458,\n",
       "   0.6754031318369011,\n",
       "   0.642087041169353,\n",
       "   0.6420870411693529,\n",
       "   0.642087041169353,\n",
       "   0.6434049524469018,\n",
       "   0.6434049524469019,\n",
       "   0.6724089270176068,\n",
       "   0.6735271993519208,\n",
       "   0.6735271993519208,\n",
       "   0.6735279647149139,\n",
       "   0.6735311675157639,\n",
       "   0.6899577182475499,\n",
       "   0.6812762371027515,\n",
       "   0.6754702151204636,\n",
       "   0.675394180124387,\n",
       "   0.6766296144168018,\n",
       "   0.6766296144168018,\n",
       "   0.6984563761721826,\n",
       "   0.6985804872591811,\n",
       "   0.6907520775548484,\n",
       "   0.6905553368426295,\n",
       "   0.6905553368426295,\n",
       "   0.6885068092382928,\n",
       "   0.6885068092382928,\n",
       "   0.6940251719324039,\n",
       "   0.6933930437662614,\n",
       "   0.7060823024044395,\n",
       "   0.7066847822115748,\n",
       "   0.7051065561359153,\n",
       "   0.7005321883779806,\n",
       "   0.7020509639844769,\n",
       "   0.7040952998971347,\n",
       "   0.7057319833412203,\n",
       "   0.7057319833412203,\n",
       "   0.7095475456752994,\n",
       "   0.7100846627770631,\n",
       "   0.7383922178616334,\n",
       "   0.7379816765786489,\n",
       "   0.7386892519880326,\n",
       "   0.738684088170228,\n",
       "   0.7447302377189365,\n",
       "   0.7438887060557366,\n",
       "   0.7333253172592312,\n",
       "   0.7245796849886927,\n",
       "   0.7248748307802485,\n",
       "   0.7223639976432733,\n",
       "   0.7214488179685649,\n",
       "   0.7214402042362916,\n",
       "   0.724584930949046,\n",
       "   0.7225609002960941,\n",
       "   0.7047031355839252,\n",
       "   0.7047031355839252,\n",
       "   0.7061494535318134,\n",
       "   0.7046435084042627,\n",
       "   0.7046435084042628],\n",
       "  [0.0775791113542719,\n",
       "   0.07934920476158722,\n",
       "   0.08050764858994133,\n",
       "   0.0810806639962579,\n",
       "   0.08159458979216508,\n",
       "   0.08274558442206237,\n",
       "   0.08320549380121839,\n",
       "   0.08320549380121839,\n",
       "   0.08376005486684679,\n",
       "   0.08363717080173164,\n",
       "   0.08364540919267834,\n",
       "   0.08364540919267834,\n",
       "   0.08397983962032171,\n",
       "   0.08452932068369856,\n",
       "   0.08439509190346027,\n",
       "   0.08343223803663237,\n",
       "   0.0832700131271969,\n",
       "   0.0826186891602662,\n",
       "   0.0826186891602662,\n",
       "   0.08256366401527342,\n",
       "   0.08171109424223229,\n",
       "   0.0810538386906071,\n",
       "   0.08156682701006257,\n",
       "   0.08071112999103582,\n",
       "   0.0799093999433535,\n",
       "   0.0799093999433535,\n",
       "   0.07991362101270803,\n",
       "   0.0800196870527209,\n",
       "   0.08084497294623377,\n",
       "   0.08017392331374712,\n",
       "   0.08017392331374712,\n",
       "   0.07967421156235881,\n",
       "   0.079104783835559,\n",
       "   0.07792840414699809,\n",
       "   0.07790187646090424,\n",
       "   0.07795521270525418,\n",
       "   0.07796912229239399,\n",
       "   0.07796912229239399,\n",
       "   0.07731773098156323,\n",
       "   0.0765927921299186,\n",
       "   0.07640191008213924,\n",
       "   0.07640191008213924,\n",
       "   0.07596861127050154,\n",
       "   0.07596611607824581,\n",
       "   0.07550061935442534,\n",
       "   0.07534874958408067,\n",
       "   0.07475396894499266,\n",
       "   0.07444411862855006,\n",
       "   0.07444411862855006,\n",
       "   0.07343613508710468,\n",
       "   0.07343613508710468,\n",
       "   0.07280324456054428,\n",
       "   0.07238454441827517,\n",
       "   0.07238454441827517,\n",
       "   0.07238479164609891,\n",
       "   0.07238502731476545,\n",
       "   0.07281792897576352,\n",
       "   0.0721440832786887,\n",
       "   0.07193626005342353,\n",
       "   0.07193860577975592,\n",
       "   0.07180767373088702,\n",
       "   0.07180767373088702,\n",
       "   0.07181177495861484,\n",
       "   0.07180641687300594,\n",
       "   0.0718229553513934,\n",
       "   0.07231647045727027,\n",
       "   0.07231647045727027,\n",
       "   0.07174358727582306,\n",
       "   0.07174358727582306,\n",
       "   0.07157879399295117,\n",
       "   0.07158693359139363,\n",
       "   0.07137804975353362,\n",
       "   0.07138972817357508,\n",
       "   0.07138295625771121,\n",
       "   0.07147068650733718,\n",
       "   0.07142724546290276,\n",
       "   0.07139492491773955,\n",
       "   0.07129192125019468,\n",
       "   0.07129192125019468,\n",
       "   0.07122943337116719,\n",
       "   0.07115211702732216,\n",
       "   0.07147542870580173,\n",
       "   0.07109903901680006,\n",
       "   0.07022853750510971,\n",
       "   0.07022853074584334,\n",
       "   0.06993312793496725,\n",
       "   0.06984096361542431,\n",
       "   0.06920873827238672,\n",
       "   0.06937873182514079,\n",
       "   0.06937756586198836,\n",
       "   0.06941882717163751,\n",
       "   0.06976461324354714,\n",
       "   0.06976095004752171,\n",
       "   0.06946948030134106,\n",
       "   0.06951542690066183,\n",
       "   0.06990921976690012,\n",
       "   0.06986113997289486,\n",
       "   0.06975149024151682,\n",
       "   0.06969938361873905,\n",
       "   0.06969818029389085],\n",
       "  array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]])]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp1 = [[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
