{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import CFG, ChartParser\n",
    "from random import choice\n",
    "from nltk.parse.generate import generate\n",
    "import gensim\n",
    "import os\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "import random\n",
    "from matplotlib import pyplot\n",
    "NUM_ANALOGS = 2\n",
    "N_COMPONENTS = 10\n",
    "PER_ANALOG = 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_word_embeddings(vec_dict,n_components=10):\n",
    "    X_train = []\n",
    "    X_train_names = []\n",
    "    for x in vec_dict:\n",
    "            X_train.append(vec_dict[x])\n",
    "            X_train_names.append(x)\n",
    "\n",
    "    X_train = np.asarray(X_train)\n",
    "    pca_embeddings = {}\n",
    "\n",
    "    # PCA to get Top Components\n",
    "    pca =  PCA(n_components = len(X_train_names))\n",
    "    X_train = X_train - np.mean(X_train)\n",
    "    X_fit = pca.fit_transform(X_train)\n",
    "    U1 = pca.components_\n",
    "\n",
    "    z = []\n",
    "\n",
    "    # Removing Projections on Top Components\n",
    "    for i, x in enumerate(X_train):\n",
    "        for u in U1[0:7]:        \n",
    "                x = x - np.dot(u.transpose(),x) * u \n",
    "        z.append(x)\n",
    "\n",
    "    z = np.asarray(z)\n",
    "\n",
    "    # PCA Dim Reduction\n",
    "    pca =  PCA(n_components = n_components)\n",
    "    X_train = z - np.mean(z)\n",
    "    X_new_final = pca.fit_transform(X_train)\n",
    "\n",
    "\n",
    "    # PCA to do Post-Processing Again\n",
    "    pca =  PCA(n_components = n_components)\n",
    "    X_new = X_new_final - np.mean(X_new_final)\n",
    "    X_new = pca.fit_transform(X_new)\n",
    "    Ufit = pca.components_\n",
    "\n",
    "    X_new_final = X_new_final - np.mean(X_new_final)\n",
    "\n",
    "    final_pca_embeddings = {}\n",
    "\n",
    "    for i, x in enumerate(X_train_names):\n",
    "            final_pca_embeddings[x] = X_new_final[i]\n",
    "            #for u in Ufit[0:7]:\n",
    "            #    final_pca_embeddings[x] = final_pca_embeddings[x] - np.dot(u.transpose(),final_pca_embeddings[x]) * u \n",
    "\n",
    "    return final_pca_embeddings#X_new_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is present in variable - stimuli\n",
    "stimuli_path = str(os.path.dirname(os.getcwd())) + '\\\\data\\\\analog_1_stimuli.py'\n",
    "f = open(stimuli_path, 'r')\n",
    "f.seek(0)  # to get to the beginning of the file.\n",
    "symstring = ''\n",
    "for line in f:\n",
    "    symstring += line\n",
    "exec symstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dogs bite cats',\n",
       " 'lions hunt deer',\n",
       " 'snakes eat rats',\n",
       " 'eagles eat worms',\n",
       " 'happy cats sleep',\n",
       " 'hungry dogs bark',\n",
       " 'angry lions roar',\n",
       " 'angry snakes hiss',\n",
       " 'small worms crawl',\n",
       " 'deer are beautiful',\n",
       " 'snakes are slender',\n",
       " 'eagles are majestic',\n",
       " 'eagles fly',\n",
       " 'snakes slither',\n",
       " 'dogs growl',\n",
       " 'cats nap',\n",
       " 'dogs think big dogs bite cats',\n",
       " 'eagles know big eagles eat worms',\n",
       " 'snakes think big snakes eat rats',\n",
       " 'big dogs bite small cats',\n",
       " 'big lions hunt fast deer',\n",
       " 'lions stalk fast deer',\n",
       " 'dogs chase small cats',\n",
       " 'kids like jeans',\n",
       " 'chefs wear hats',\n",
       " 'friends drink beer',\n",
       " 'kings give speeches',\n",
       " 'old jeans fade',\n",
       " 'young kids play',\n",
       " 'tired chefs cook',\n",
       " 'old friends bond',\n",
       " 'fresh beer bubbles',\n",
       " 'hats are protective',\n",
       " 'friends are necessary',\n",
       " 'kings are respected',\n",
       " 'kings rule',\n",
       " 'friends enjoy',\n",
       " 'kings run',\n",
       " 'jeans rip',\n",
       " 'kids think old kids like jeans',\n",
       " 'kings know old kings give speeches',\n",
       " 'friends guess godd friends drink together',\n",
       " 'old kids wear faded jeans',\n",
       " 'smart chefs cook good food',\n",
       " 'chefs make good food',\n",
       " 'kids wear short jeans']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stimuli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_embeddings(vecs, names):\n",
    "    X = vecs\n",
    "    #pca = PCA(n_components=2)\n",
    "    #result = pca.fit_transform(X)\n",
    "    ## create a scatter plot of the projection\n",
    "    #pyplot.scatter(result[:, 0], result[:, 1])\n",
    "    \n",
    "    #for i, word in enumerate(names):\n",
    "    #    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "    #pyplot.show()\n",
    "    \n",
    "    #cos_sim = np.dot(X, np.transpose(X))/(np.linalg.norm(X)*np.linalg.norm(X))\n",
    "    cos_sim = np.corrcoef(X)\n",
    "    sns_heatmap = sns.heatmap(cos_sim, linewidth=0.5,xticklabels=names, yticklabels=names)\n",
    "    fig = sns_heatmap.get_figure()\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec(sentences,model_type='custom', reduced_dim=10, normalised=True):\n",
    "    # first split all sentences and get unique words\n",
    "    unique_words = [sentence.split(' ') for sentence in sentences]\n",
    "    unique_words = [word.lower() for sentence in unique_words for word in sentence]\n",
    "    unique_words = list(set(unique_words))\n",
    "    # put these words into model and get embeddings\n",
    "    if model_type == 'custom':\n",
    "        # for custom model, give list of sentences\n",
    "        all_sentences = [sentence.split(' ') for sentence in sentences]\n",
    "        model_custom = gensim.models.Word2Vec(all_sentences, min_count=1,size=reduced_dim,sg=0)\n",
    "        vec_dict = {}\n",
    "        for word in unique_words:\n",
    "            vec_dict[word] = model_custom[word]\n",
    "    elif model_type == 'google':\n",
    "        # for google word2vec give all unique words\n",
    "        google_word2vec_path = str(os.path.dirname(os.getcwd())) + '\\\\data\\\\GoogleNews-vectors-negative300.bin'\n",
    "        model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "            google_word2vec_path, binary=True)\n",
    "        vec_dict = {}\n",
    "        for word in unique_words:\n",
    "            vec_dict[word] = model[word]\n",
    "        # reduce dimension size for the google 300b model\n",
    "        vec_dict = reduce_word_embeddings(vec_dict,n_components=reduced_dim)\n",
    "    # power normalise vectors, then put them into 0,1 range\n",
    "    word2vec_mat = np.zeros((len(unique_words),reduced_dim))\n",
    "    for ind,word in enumerate(unique_words):\n",
    "        word2vec_mat[ind,:] = vec_dict[word]#/np.linalg.norm(vec_dict[word])\n",
    "    # now 0-1 normalise by dimension\n",
    "    for ind in range(word2vec_mat.shape[1]):\n",
    "        word2vec_mat[:,ind] = (word2vec_mat[:,ind] - min(word2vec_mat[:,ind]))/(max(word2vec_mat[:,ind]) - min(word2vec_mat[:,ind]))\n",
    "    # power normalise again\n",
    "    for ind,word in enumerate(unique_words):\n",
    "        word2vec_mat[ind,:] = word2vec_mat[ind,:]/np.linalg.norm(word2vec_mat[ind,:])\n",
    "    \n",
    "    # convert back to dict\n",
    "    vec_dict = {}\n",
    "    for ind,word in enumerate(unique_words):\n",
    "        vec_dict[word] = word2vec_mat[ind,:]\n",
    "    return [vec_dict, word2vec_mat, unique_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7948621475062169, 0.7429283721561818, 0.7703827631443477, 0.729503893085569, 0.9002832523810654, 0.7390895237818941, 0.4511076465532744)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karkau\\AppData\\Local\\Continuum\\anaconda2\\envs\\Dora\\lib\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "[word2vec_dict, word2vec_mat, unique_words] = get_word2vec(sentences=stimuli,model_type='custom',reduced_dim=10)\n",
    "reduced_embeddings = word2vec_dict\n",
    "sentences = stimuli\n",
    "print(np.dot(reduced_embeddings['hungry'],reduced_embeddings['happy']),\n",
    "np.dot(reduced_embeddings['hungry'],reduced_embeddings['are']),\n",
    "np.dot(reduced_embeddings['dogs'],reduced_embeddings['cats']),\n",
    "np.dot(reduced_embeddings['dogs'],reduced_embeddings['beautiful']),\n",
    "np.dot(reduced_embeddings['dogs'],reduced_embeddings['lions']),\n",
    "np.dot(reduced_embeddings['bite'],reduced_embeddings['are']),\n",
    "np.dot(reduced_embeddings['bite'],reduced_embeddings['drink']))\n",
    "#vis_embeddings(word2vec_mat,unique_words)\n",
    "#print(np.dot(word2vec_dict['slender'],word2vec_dict['cats']))\n",
    "#print(np.dot(word2vec_dict['nap'],word2vec_dict['fly']))\n",
    "#print(np.dot(word2vec_dict['cats'],word2vec_dict['fly']))\n",
    "#print(np.dot(word2vec_dict['bite'],word2vec_dict['fly']))\n",
    "#print(np.dot(word2vec_dict['small'],word2vec_dict['big']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_empty_vec(n_dim):\n",
    "    sem = [['e0',1.0]] # change to [] and range(0,n_dim) to put empty vector. Here we need a specifier, and therefore not empty vec\n",
    "    for i in range(1,n_dim):\n",
    "        sem.append(['e' + str(i), 0.0])\n",
    "    return sem\n",
    "def get_sem(word2vec_dict,word):\n",
    "    vec = word2vec_dict[word]\n",
    "    sem = []\n",
    "    for i in range(len(vec)):\n",
    "        sem.append(['e' + str(i), vec[i]])\n",
    "    return sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dogs bite cats',\n",
       " 'lions hunt deer',\n",
       " 'snakes eat rats',\n",
       " 'eagles eat worms',\n",
       " 'happy cats sleep',\n",
       " 'hungry dogs bark',\n",
       " 'angry lions growl',\n",
       " 'angry snakes hiss',\n",
       " 'small worms crawl',\n",
       " 'deer are beautiful',\n",
       " 'snakes are slender',\n",
       " 'eagles are majestic',\n",
       " 'eagles fly',\n",
       " 'snakes crawl',\n",
       " 'dogs bark',\n",
       " 'cats nap',\n",
       " 'dogs think big dogs bite cats',\n",
       " 'worms know big eagles eat worms',\n",
       " 'rats think big snakes eat rats',\n",
       " 'big dogs bite small cats']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stimuli[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced_embeddings contains the word2vec\n",
    "# type-1 with separate det for empty predicate\n",
    "import copy\n",
    "random.seed(999)\n",
    "symProps = []\n",
    "# for type = [0,1,2,3], [9,10,11]\n",
    "prototype_1={'name': 'non_exist', \n",
    "             'RBs': [{'pred_name': 'non_exist', 'pred_sem': [], \n",
    "                      'higher_order': False, \n",
    "                      'object_name': 'obj', 'object_sem': [], 'P': 'non_exist'}, \n",
    "                     {'pred_name': 'non_exist', 'pred_sem': [], \n",
    "                      'higher_order': False, \n",
    "                      'object_name': 'obj', 'object_sem': [], 'P': 'non_exist'}], \n",
    "             'set': 'memory', 'analog': None}\n",
    "# for simple two word props like \"happy cats\" present in type = [4,5,6,7,8]\n",
    "# 'name' is simply the proposition number + '.' + '1'\n",
    "prototype_2 = {'name': 'non_exist', \n",
    "             'RBs': [{'pred_name': 'non_exist', 'pred_sem': [], \n",
    "                      'higher_order': False, \n",
    "                      'object_name': 'obj', 'object_sem': [], 'P': 'non_exist'}],\n",
    "              'set':'memory','analog':None}\n",
    "# for 3 word props in [4,5,6,7,8] containing prototype_2\n",
    "prototype_3 = {'name': 'non_exist', \n",
    "             'RBs': [{'pred_name': 'non_exist', 'pred_sem': [], \n",
    "                      'higher_order': True, \n",
    "                      'object_name': 'obj', 'object_sem': [], 'P': 'non_exist'}, \n",
    "                     {'pred_name': 'non_exist', 'pred_sem': [], \n",
    "                      'higher_order': False, \n",
    "                      'object_name': 'non_exist', 'object_sem': [], 'P': 'non_exist'}], \n",
    "             'set': 'memory', 'analog': None}\n",
    "# for 6 word props in [16,17,18]\n",
    "prototype_4 = {'name': 'non_exist', \n",
    "             'RBs': [{'pred_name': 'non_exist', 'pred_sem': [], \n",
    "                      'higher_order': False, \n",
    "                      'object_name': 'obj', 'object_sem': None, 'P': 'non_exist'}, \n",
    "                     {'pred_name': 'non_exist', 'pred_sem': [], \n",
    "                      'higher_order': True, \n",
    "                      'object_name': 'obj', 'object_sem': None, 'P': 'non_exist'}], \n",
    "             'set': 'memory', 'analog': None}\n",
    "#  for 5 word props in [19,20] \n",
    "prototype_5 = {'name': 'non_exist', \n",
    "             'RBs': [{'pred_name': 'non_exist', 'pred_sem': [], \n",
    "                      'higher_order': True, \n",
    "                      'object_name': 'obj', 'object_sem': None, 'P': 'non_exist'}, \n",
    "                     {'pred_name': 'non_exist', 'pred_sem': [], \n",
    "                      'higher_order': True, \n",
    "                      'object_name': 'obj', 'object_sem': None, 'P': 'non_exist'}], \n",
    "             'set': 'memory', 'analog': None}\n",
    "#word2vec_dict = reduced_embeddings\n",
    "for analog in range(0,NUM_ANALOGS): # \n",
    "    for proposition in range(0,PER_ANALOG):\n",
    "        current_sentence = sentences[analog*PER_ANALOG + proposition]\n",
    "        current_sentence = current_sentence.split(' ')\n",
    "        # depending on proposition number, decompose propositions into their RB's\n",
    "        if (proposition >=0 and proposition < 4) or (proposition >=9 and proposition < 12):\n",
    "            prop_dict = copy.deepcopy(prototype_1)\n",
    "            prop_dict['RBs'][0]['pred_name'] = 'det' # strange behaviour - predicate has to exist to form RB with P in object\n",
    "            prop_dict['RBs'][0]['pred_sem'] = get_empty_vec(n_dim=N_COMPONENTS)\n",
    "            prop_dict['RBs'][0]['object_name'] = current_sentence[0]\n",
    "            prop_dict['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[0])\n",
    "            prop_dict['RBs'][1]['pred_name'] = current_sentence[1]\n",
    "            prop_dict['RBs'][1]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[1])\n",
    "            prop_dict['RBs'][1]['object_name'] = current_sentence[2]\n",
    "            prop_dict['RBs'][1]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[2])\n",
    "            prop_dict['analog'] = analog\n",
    "            #prop_dict['set'] = random.choice(['recipient','driver'])\n",
    "            prop_dict['name'] = 'P' + str(proposition)\n",
    "            symProps.append(prop_dict)\n",
    "        elif proposition >=4 and proposition < 8:\n",
    "            # first encode lower order prop\n",
    "            prop_dict_1 = copy.deepcopy(prototype_2)\n",
    "            prop_dict_1['name'] = 'P' + str(proposition) + '.1'\n",
    "            prop_dict_1['RBs'][0]['pred_name'] = current_sentence[0]\n",
    "            prop_dict_1['RBs'][0]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[0])\n",
    "            prop_dict_1['RBs'][0]['object_name'] = current_sentence[1]\n",
    "            prop_dict_1['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[1])\n",
    "            prop_dict_1['analog'] = analog\n",
    "            #prop_dict_1['set'] = random.choice(['recipient','driver'])\n",
    "            symProps.append(prop_dict_1)\n",
    "            # now encode the higher order sem\n",
    "            prop_dict = copy.deepcopy(prototype_3)\n",
    "            prop_dict['name'] = 'P' + str(proposition)\n",
    "            prop_dict['RBs'][0]['pred_name'] = 'det' # strange behaviour - predicate has to exist to form RB with P in object\n",
    "            prop_dict['RBs'][0]['pred_sem'] = get_empty_vec(n_dim=N_COMPONENTS)\n",
    "            prop_dict['RBs'][0]['object_name'] = 'non_exist'#current_sentence[0] + current_sentence[1]\n",
    "            prop_dict['RBs'][0]['object_sem'] = []\n",
    "            prop_dict['RBs'][0]['P'] = 'P' + str(proposition) + '.1'\n",
    "            prop_dict['RBs'][1]['pred_name'] = current_sentence[2]\n",
    "            prop_dict['RBs'][1]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[2])\n",
    "            prop_dict['analog'] = analog\n",
    "            #prop_dict['set'] = random.choice(['recipient','driver'])\n",
    "            symProps.append(prop_dict)\n",
    "        elif proposition >= 12 and proposition < 16:\n",
    "            prop_dict = copy.deepcopy(prototype_1)\n",
    "            prop_dict['RBs'][0]['pred_name'] = 'det' # strange behaviour - predicate has to exist to form RB with P in object\n",
    "            prop_dict['RBs'][0]['pred_sem'] = get_empty_vec(n_dim=N_COMPONENTS)\n",
    "            prop_dict['RBs'][0]['object_name'] = current_sentence[0]\n",
    "            prop_dict['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[0])\n",
    "            prop_dict['RBs'][1]['pred_name'] = current_sentence[1]\n",
    "            prop_dict['RBs'][1]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[1])\n",
    "            prop_dict['analog'] = analog\n",
    "            prop_dict['name'] = 'P' + str(proposition)\n",
    "            symProps.append(prop_dict)\n",
    "        elif proposition >= 16 and proposition < 19:\n",
    "            prop_dict_1 = copy.deepcopy(prototype_2)\n",
    "            prop_dict_1['name'] = 'P' + str(proposition) + '.1'\n",
    "            prop_dict_1['RBs'][0]['pred_name'] = current_sentence[2]\n",
    "            prop_dict_1['RBs'][0]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[2])\n",
    "            prop_dict_1['RBs'][0]['object_name'] = current_sentence[3]\n",
    "            prop_dict_1['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[3])\n",
    "            prop_dict_1['analog'] = analog\n",
    "            symProps.append(prop_dict_1)\n",
    "            # add higher order prop\n",
    "            prop_dict_2 = copy.deepcopy(prototype_3)\n",
    "            prop_dict_2['name'] = 'P' + str(proposition) + '.2'\n",
    "            prop_dict_2['RBs'][0]['P'] = 'P' + str(proposition) + '.1'\n",
    "            prop_dict_2['RBs'][0]['pred_name'] = 'det'\n",
    "            prop_dict_2['RBs'][0]['pred_sem'] = get_empty_vec(n_dim=N_COMPONENTS)\n",
    "            prop_dict_2['RBs'][1]['pred_name'] = current_sentence[4]\n",
    "            prop_dict_2['RBs'][1]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[4])\n",
    "            prop_dict_2['RBs'][1]['object_name'] = current_sentence[5]\n",
    "            prop_dict_2['RBs'][1]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[5])\n",
    "            prop_dict_2['analog'] = analog\n",
    "            symProps.append(prop_dict_2)\n",
    "            # now add the highest order prop\n",
    "            prop_dict = copy.deepcopy(prototype_4)\n",
    "            prop_dict['name'] = 'P' + str(proposition)\n",
    "            prop_dict['RBs'][0]['pred_name'] = 'det'\n",
    "            prop_dict_2['RBs'][0]['pred_sem'] = get_empty_vec(n_dim=N_COMPONENTS)            \n",
    "            prop_dict['RBs'][0]['object_name'] = current_sentence[0]\n",
    "            prop_dict['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[0])\n",
    "            prop_dict['RBs'][1]['pred_name'] = current_sentence[1]\n",
    "            prop_dict['RBs'][1]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[1])\n",
    "            prop_dict['RBs'][1]['P'] = 'P' + str(proposition) + '.1'\n",
    "            prop_dict['analog'] = analog\n",
    "            prop_dict['name'] = 'P' + str(proposition)\n",
    "            symProps.append(prop_dict)\n",
    "        elif proposition >= 19 and proposition < 21:\n",
    "            prop_dict_1 = copy.deepcopy(prototype_2)\n",
    "            prop_dict_1['name'] = 'P' + str(proposition) + '.1'\n",
    "            prop_dict_1['RBs'][0]['pred_name'] = current_sentence[0]\n",
    "            prop_dict_1['RBs'][0]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[0])\n",
    "            prop_dict_1['RBs'][0]['object_name'] = current_sentence[1]\n",
    "            prop_dict_1['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[1])\n",
    "            prop_dict_1['analog'] = analog\n",
    "            symProps.append(prop_dict_1)\n",
    "            prop_dict_1 = copy.deepcopy(prototype_2)\n",
    "            prop_dict_1['name'] = 'P' + str(proposition) + '.2'\n",
    "            prop_dict_1['RBs'][0]['pred_name'] = current_sentence[3]\n",
    "            prop_dict_1['RBs'][0]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[3])\n",
    "            prop_dict_1['RBs'][0]['object_name'] = current_sentence[4]\n",
    "            prop_dict_1['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[4])\n",
    "            prop_dict_1['analog'] = analog\n",
    "            symProps.append(prop_dict_1)\n",
    "            prop_dict = copy.deepcopy(prototype_5)\n",
    "            prop_dict['name'] = 'P' + str(proposition)\n",
    "            prop_dict['RBs'][0]['P'] = 'P' + str(proposition) + '.1'\n",
    "            prop_dict['RBs'][0]['pred_name'] = 'det'\n",
    "            prop_dict['RBs'][0]['pred_sem'] = get_empty_vec(n_dim=N_COMPONENTS)\n",
    "            prop_dict['RBs'][1]['P'] = 'P' + str(proposition) + '.2'\n",
    "            prop_dict['RBs'][1]['pred_name'] = current_sentence[2]\n",
    "            prop_dict['RBs'][1]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[2])\n",
    "            prop_dict['analog'] = analog\n",
    "            symProps.append(prop_dict)\n",
    "        elif proposition >= 21 and proposition < 23:\n",
    "            prop_dict_1 = copy.deepcopy(prototype_2)\n",
    "            prop_dict_1['name'] = 'P' + str(proposition) + '.1'\n",
    "            prop_dict_1['RBs'][0]['pred_name'] = current_sentence[2]\n",
    "            prop_dict_1['RBs'][0]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[2])\n",
    "            prop_dict_1['RBs'][0]['object_name'] = current_sentence[3]\n",
    "            prop_dict_1['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[3])\n",
    "            prop_dict_1['analog'] = analog\n",
    "            symProps.append(prop_dict_1)\n",
    "            # add higher order prop\n",
    "            prop_dict = copy.deepcopy(prototype_4)\n",
    "            prop_dict['name'] = 'P' + str(proposition)\n",
    "            prop_dict['RBs'][0]['pred_name'] = 'det'\n",
    "            prop_dict['RBs'][0]['pred_sem'] = get_empty_vec(n_dim=N_COMPONENTS)\n",
    "            prop_dict['RBs'][0]['object_name'] = current_sentence[0]\n",
    "            prop_dict['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[0])\n",
    "            prop_dict['RBs'][1]['pred_name'] = current_sentence[1]\n",
    "            prop_dict['RBs'][1]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[1])\n",
    "            prop_dict['RBs'][1]['P'] = 'P' + str(proposition) + '.1'\n",
    "            prop_dict['analog'] = analog\n",
    "            symProps.append(prop_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type 2 with filled in slot like structure. ie., no empty predicate \n",
    "# reduced_embeddings contains the word2vec\n",
    "import copy\n",
    "random.seed(999)\n",
    "symProps = []\n",
    "# for type = [0,1,2,3], [9,10,11]\n",
    "prototype_1={'name': 'non_exist', \n",
    "             'RBs': [{'pred_name': 'non_exist', 'pred_sem': [], \n",
    "                      'higher_order': False, \n",
    "                      'object_name': 'obj', 'object_sem': [], 'P': 'non_exist'}, \n",
    "                     {'pred_name': 'non_exist', 'pred_sem': [], \n",
    "                      'higher_order': False, \n",
    "                      'object_name': 'obj', 'object_sem': [], 'P': 'non_exist'}], \n",
    "             'set': 'memory', 'analog': None}\n",
    "# for 3 word props in [4,5,6,7,8] containing prototype_1\n",
    "prototype_2 = {'name': 'non_exist', \n",
    "             'RBs': [{'pred_name': 'non_exist', 'pred_sem': [], \n",
    "                      'higher_order': False, \n",
    "                      'object_name': 'obj', 'object_sem': [], 'P': 'non_exist'}, \n",
    "                     {'pred_name': 'non_exist', 'pred_sem': [], \n",
    "                      'higher_order': True, \n",
    "                      'object_name': 'non_exist', 'object_sem': [], 'P': 'non_exist'}], \n",
    "             'set': 'memory', 'analog': None}\n",
    "# for 2 word prop = single RB\n",
    "prototype_3 = {'name': 'non_exist', \n",
    "             'RBs': [{'pred_name': 'non_exist', 'pred_sem': [], \n",
    "                      'higher_order': False, \n",
    "                      'object_name': 'obj', 'object_sem': [], 'P': 'non_exist'}], \n",
    "             'set': 'memory', 'analog': None}\n",
    "\n",
    "#word2vec_dict = reduced_embeddings\n",
    "for analog in range(0,NUM_ANALOGS): # \n",
    "    for proposition in range(0,PER_ANALOG):\n",
    "        current_sentence = sentences[analog*PER_ANALOG + proposition]\n",
    "        current_sentence = current_sentence.split(' ')\n",
    "        # depending on proposition number, decompose propositions into their RB's\n",
    "        if (proposition >=0 and proposition < 4) or (proposition >=9 and proposition < 12): # == [- dogs][bite cats]\n",
    "            prop_dict = copy.deepcopy(prototype_1)\n",
    "            prop_dict['RBs'][0]['object_name'] = current_sentence[0]\n",
    "            prop_dict['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[0])\n",
    "            prop_dict['RBs'][1]['pred_name'] = current_sentence[1]\n",
    "            prop_dict['RBs'][1]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[1])\n",
    "            prop_dict['RBs'][1]['object_name'] = current_sentence[2]\n",
    "            prop_dict['RBs'][1]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[2])\n",
    "            prop_dict['analog'] = analog\n",
    "            prop_dict['name'] = 'P' + str(analog) + str(proposition)\n",
    "            symProps.append(prop_dict)\n",
    "        elif proposition >=4 and proposition < 9: # == [happy cats][sleep -]\n",
    "            prop_dict = copy.deepcopy(prototype_1)\n",
    "            prop_dict['RBs'][0]['pred_name'] = current_sentence[0] \n",
    "            prop_dict['RBs'][0]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[1])\n",
    "            prop_dict['RBs'][0]['object_name'] = current_sentence[1]\n",
    "            prop_dict['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[1])\n",
    "            prop_dict['RBs'][1]['pred_name'] = current_sentence[2]\n",
    "            prop_dict['RBs'][1]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[2])\n",
    "            prop_dict['analog'] = analog\n",
    "            prop_dict['name'] = 'P' + str(analog) + str(proposition)\n",
    "            symProps.append(prop_dict)\n",
    "        elif proposition >= 12 and proposition < 16: # == [- eagles][fly -]\n",
    "            prop_dict = copy.deepcopy(prototype_1)\n",
    "            prop_dict['RBs'][0]['object_name'] = current_sentence[0]\n",
    "            prop_dict['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[0])\n",
    "            prop_dict['RBs'][1]['pred_name'] = current_sentence[1]\n",
    "            prop_dict['RBs'][1]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[1])\n",
    "            prop_dict['analog'] = analog\n",
    "            prop_dict['name'] = 'P' + str(analog) + str(proposition)\n",
    "            symProps.append(prop_dict)\n",
    "        '''elif proposition >= 16 and proposition < 19: # == [- dogs][think [[big dogs][bite cats]]\n",
    "            prop_dict_1 = copy.deepcopy(prototype_1) # == [[big dogs][bite cats]]\n",
    "            prop_dict_1['RBs'][0]['pred_name'] = current_sentence[2]\n",
    "            prop_dict_1['RBs'][0]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[2])\n",
    "            prop_dict_1['RBs'][0]['object_name'] = current_sentence[3]\n",
    "            prop_dict_1['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[3])\n",
    "            prop_dict_1['RBs'][1]['pred_name'] = current_sentence[4]\n",
    "            prop_dict_1['RBs'][1]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[4])\n",
    "            prop_dict_1['RBs'][1]['object_name'] = current_sentence[5]\n",
    "            prop_dict_1['RBs'][1]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[5])\n",
    "            prop_dict_1['analog'] = analog\n",
    "            prop_dict_1['name'] = 'P' + str(analog) + str(proposition) + '.1'\n",
    "            symProps.append(prop_dict_1)\n",
    "            # add higher order prop \n",
    "            prop_dict = copy.deepcopy(prototype_2) # == [- dogs][think P-N.1]\n",
    "            prop_dict['RBs'][0]['object_name'] = current_sentence[0]\n",
    "            prop_dict['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[0])\n",
    "            prop_dict['RBs'][1]['pred_name'] = current_sentence[1]\n",
    "            prop_dict['RBs'][1]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[1])\n",
    "            prop_dict['RBs'][1]['P'] = 'P' + str(analog) + str(proposition) + '.1'\n",
    "            prop_dict['analog'] = analog\n",
    "            prop_dict['name'] = 'P' + str(analog) + str(proposition)\n",
    "            symProps.append(prop_dict)\n",
    "        elif proposition >= 19 and proposition < 21: # == [big dogs][bite [small cats]]\n",
    "            prop_dict_1 = copy.deepcopy(prototype_3) # [small cats]\n",
    "            prop_dict_1['RBs'][0]['pred_name'] = current_sentence[3]\n",
    "            prop_dict_1['RBs'][0]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[3])\n",
    "            prop_dict_1['RBs'][0]['object_name'] = current_sentence[4]\n",
    "            prop_dict_1['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[4])\n",
    "            prop_dict_1['analog'] = analog\n",
    "            prop_dict_1['name'] = 'P' + str(analog) + str(proposition) + '.1'            \n",
    "            symProps.append(prop_dict_1)\n",
    "            prop_dict = copy.deepcopy(prototype_2) # == [big dogs][bite P-N.1]\n",
    "            prop_dict['RBs'][0]['pred_name'] = current_sentence[0]\n",
    "            prop_dict['RBs'][0]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[0])\n",
    "            prop_dict['RBs'][0]['object_name'] = current_sentence[1]\n",
    "            prop_dict['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[1])\n",
    "            prop_dict['RBs'][1]['pred_name'] = current_sentence[2]\n",
    "            prop_dict['RBs'][1]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[2])\n",
    "            prop_dict['RBs'][1]['P'] = 'P' + str(analog) + str(proposition) + '.1'\n",
    "            prop_dict['analog'] = analog\n",
    "            prop_dict['name'] = 'P' + str(analog) + str(proposition)\n",
    "            symProps.append(prop_dict)\n",
    "        elif proposition >= 21 and proposition < 23: # == [- lions][hunt [fast deer]]\n",
    "            prop_dict_1 = copy.deepcopy(prototype_3)\n",
    "            prop_dict_1['RBs'][0]['pred_name'] = current_sentence[2]\n",
    "            prop_dict_1['RBs'][0]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[2])\n",
    "            prop_dict_1['RBs'][0]['object_name'] = current_sentence[3]\n",
    "            prop_dict_1['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[3])\n",
    "            prop_dict_1['analog'] = analog\n",
    "            prop_dict_1['name'] = 'P' + str(analog) + str(proposition) + '.1'        \n",
    "            symProps.append(prop_dict_1)\n",
    "            # add higher order prop\n",
    "            prop_dict = copy.deepcopy(prototype_2)\n",
    "            prop_dict['RBs'][0]['object_name'] = current_sentence[0]\n",
    "            prop_dict['RBs'][0]['object_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[0])\n",
    "            prop_dict['RBs'][1]['pred_name'] = current_sentence[1]\n",
    "            prop_dict['RBs'][1]['pred_sem'] = get_sem(word2vec_dict=reduced_embeddings,word=current_sentence[1])\n",
    "            prop_dict['RBs'][1]['P'] = 'P' + str(analog) + str(proposition)\n",
    "            prop_dict['analog'] = analog\n",
    "            prop_dict['name'] = 'P' + str(analog) + str(proposition)            \n",
    "            symProps.append(prop_dict)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'set': 'memory', 'RBs': [{'higher_order': False, 'pred_name': 'non_exist', 'P': 'non_exist', 'object_name': 'dogs', 'pred_sem': [], 'object_sem': [['e0', 0.006822470257043899], ['e1', 0.25614490831417924], ['e2', 0.5506898870955731], ['e3', 0.38148445114323964], ['e4', 0.37193818720329086], ['e5', 0.33596314084943635], ['e6', 0.21129443248625446], ['e7', 0.2926511872882535], ['e8', 0.2832333120969177], ['e9', 0.1543796969135759]]}, {'higher_order': False, 'pred_name': 'bite', 'P': 'non_exist', 'object_name': 'cats', 'pred_sem': [['e0', 0.44560466912712193], ['e1', 0.3773404932576564], ['e2', 0.06326750180471738], ['e3', 0.3616058765499093], ['e4', 0.29804980304088746], ['e5', 0.0157388258638247], ['e6', 0.4691073589380276], ['e7', 0.2893577026360207], ['e8', 0.36186573612070183], ['e9', 0.02170813822210958]], 'object_sem': [['e0', 0.42338047772329807], ['e1', 0.2912484498772743], ['e2', 0.2883927045364519], ['e3', 0.046974782727654846], ['e4', 0.1912517350672809], ['e5', 0.45469632923363534], ['e6', 0.3105452079034496], ['e7', 0.3072156361405165], ['e8', 0.2805031284301316], ['e9', 0.371104531095781]]}], 'name': 'P00', 'analog': 0}\n",
      "{'set': 'memory', 'RBs': [{'higher_order': False, 'pred_name': 'non_exist', 'P': 'non_exist', 'object_name': 'lions', 'pred_sem': [], 'object_sem': [['e0', 0.19218652956484994], ['e1', 0.34196674807656574], ['e2', 0.4959338481005764], ['e3', 0.29629760310834435], ['e4', 0.3641010486206996], ['e5', 0.08188803904905072], ['e6', 0.2841198235338181], ['e7', 0.12025416723111781], ['e8', 0.5017739029817114], ['e9', 0.16168802257227524]]}, {'higher_order': False, 'pred_name': 'hunt', 'P': 'non_exist', 'object_name': 'deer', 'pred_sem': [['e0', 0.45070719883333976], ['e1', 0.4795241377681369], ['e2', 0.06045663194947886], ['e3', 0.4905031472419048], ['e4', 0.27989269459645566], ['e5', 0.03996863065409569], ['e6', 0.29734622014769657], ['e7', 0.2310633703910588], ['e8', 0.2986804749709967], ['e9', 0.10825334447634381]], 'object_sem': [['e0', 0.34354446060193616], ['e1', 0.021189300298392118], ['e2', 0.4145233617487928], ['e3', 0.08446632099081995], ['e4', 0.3500276620658219], ['e5', 0.065748604412161], ['e6', 0.5252944112629603], ['e7', 0.13127252661058425], ['e8', 0.31269639540088845], ['e9', 0.42985586734206294]]}], 'name': 'P01', 'analog': 0}\n",
      "{'set': 'memory', 'RBs': [{'higher_order': False, 'pred_name': 'non_exist', 'P': 'non_exist', 'object_name': 'snakes', 'pred_sem': [], 'object_sem': [['e0', 0.062478680412611086], ['e1', 0.40827700036462977], ['e2', 0.09266280690410277], ['e3', 0.21451880316470742], ['e4', 0.4524263948680734], ['e5', 0.21832836148614468], ['e6', 0.21997987657710175], ['e7', 0.3158841691652269], ['e8', 0.3661433864030771], ['e9', 0.4901119695898595]]}, {'higher_order': False, 'pred_name': 'eat', 'P': 'non_exist', 'object_name': 'rats', 'pred_sem': [['e0', 0.19335232661896892], ['e1', 0.22724153167321526], ['e2', 0.43415174487635577], ['e3', 0.07849133662940504], ['e4', 0.35972491528581885], ['e5', 0.09424062675321504], ['e6', 0.44577414827265766], ['e7', 0.47767805117904616], ['e8', 0.38578976426945544], ['e9', 0.048161936205395384]], 'object_sem': [['e0', 0.07536238943121853], ['e1', 0.2763755589139757], ['e2', 0.24203376448151198], ['e3', 0.15493413819748988], ['e4', 0.3912150521195724], ['e5', 0.3749004533951296], ['e6', 0.31530060214623257], ['e7', 0.4783871753503448], ['e8', 0.41848741049575716], ['e9', 0.19583690001738097]]}], 'name': 'P02', 'analog': 0}\n",
      "{'set': 'memory', 'RBs': [{'higher_order': False, 'pred_name': 'non_exist', 'P': 'non_exist', 'object_name': 'eagles', 'pred_sem': [], 'object_sem': [['e0', 0.37863832935513814], ['e1', 0.30967993457406107], ['e2', 0.3905399830397763], ['e3', 0.23481996297454166], ['e4', 0.19767246865741406], ['e5', 0.1823301799104201], ['e6', 0.2120849302132264], ['e7', 0.5643461669399337], ['e8', 0.15655832685502033], ['e9', 0.3045876515208896]]}, {'higher_order': False, 'pred_name': 'eat', 'P': 'non_exist', 'object_name': 'worms', 'pred_sem': [['e0', 0.19335232661896892], ['e1', 0.22724153167321526], ['e2', 0.43415174487635577], ['e3', 0.07849133662940504], ['e4', 0.35972491528581885], ['e5', 0.09424062675321504], ['e6', 0.44577414827265766], ['e7', 0.47767805117904616], ['e8', 0.38578976426945544], ['e9', 0.048161936205395384]], 'object_sem': [['e0', 0.39372856549107366], ['e1', 0.10283011191181628], ['e2', 0.239150329677007], ['e3', 0.4588931338433854], ['e4', 0.10578514917575035], ['e5', 0.012347561266281226], ['e6', 0.4990635330277955], ['e7', 0.5520784437802572], ['e8', 0.03781558127205993], ['e9', 0.0]]}], 'name': 'P03', 'analog': 0}\n",
      "{'set': 'memory', 'RBs': [{'higher_order': False, 'pred_name': 'happy', 'P': 'non_exist', 'object_name': 'cats', 'pred_sem': [['e0', 0.42338047772329807], ['e1', 0.2912484498772743], ['e2', 0.2883927045364519], ['e3', 0.046974782727654846], ['e4', 0.1912517350672809], ['e5', 0.45469632923363534], ['e6', 0.3105452079034496], ['e7', 0.3072156361405165], ['e8', 0.2805031284301316], ['e9', 0.371104531095781]], 'object_sem': [['e0', 0.42338047772329807], ['e1', 0.2912484498772743], ['e2', 0.2883927045364519], ['e3', 0.046974782727654846], ['e4', 0.1912517350672809], ['e5', 0.45469632923363534], ['e6', 0.3105452079034496], ['e7', 0.3072156361405165], ['e8', 0.2805031284301316], ['e9', 0.371104531095781]]}, {'higher_order': False, 'pred_name': 'sleep', 'P': 'non_exist', 'object_name': 'obj', 'pred_sem': [['e0', 0.2820576925391763], ['e1', 0.05208075185582743], ['e2', 0.11190123328312923], ['e3', 0.09571774667242977], ['e4', 0.45027954634561385], ['e5', 0.5745250434056215], ['e6', 0.5403079643415377], ['e7', 0.14960766123826497], ['e8', 0.028892184220900053], ['e9', 0.21924114127758274]], 'object_sem': []}], 'name': 'P04', 'analog': 0}\n",
      "{'set': 'memory', 'RBs': [{'higher_order': False, 'pred_name': 'hungry', 'P': 'non_exist', 'object_name': 'dogs', 'pred_sem': [['e0', 0.006822470257043899], ['e1', 0.25614490831417924], ['e2', 0.5506898870955731], ['e3', 0.38148445114323964], ['e4', 0.37193818720329086], ['e5', 0.33596314084943635], ['e6', 0.21129443248625446], ['e7', 0.2926511872882535], ['e8', 0.2832333120969177], ['e9', 0.1543796969135759]], 'object_sem': [['e0', 0.006822470257043899], ['e1', 0.25614490831417924], ['e2', 0.5506898870955731], ['e3', 0.38148445114323964], ['e4', 0.37193818720329086], ['e5', 0.33596314084943635], ['e6', 0.21129443248625446], ['e7', 0.2926511872882535], ['e8', 0.2832333120969177], ['e9', 0.1543796969135759]]}, {'higher_order': False, 'pred_name': 'bark', 'P': 'non_exist', 'object_name': 'obj', 'pred_sem': [['e0', 0.041277212947918716], ['e1', 0.5487789298339892], ['e2', 0.3581455093535661], ['e3', 0.11120083155417188], ['e4', 0.035208086661969264], ['e5', 0.11287380761060602], ['e6', 0.40233832365612315], ['e7', 0.42232344272886474], ['e8', 0.1713213759163624], ['e9', 0.41586020540018104]], 'object_sem': []}], 'name': 'P05', 'analog': 0}\n",
      "{'set': 'memory', 'RBs': [{'higher_order': False, 'pred_name': 'angry', 'P': 'non_exist', 'object_name': 'lions', 'pred_sem': [['e0', 0.19218652956484994], ['e1', 0.34196674807656574], ['e2', 0.4959338481005764], ['e3', 0.29629760310834435], ['e4', 0.3641010486206996], ['e5', 0.08188803904905072], ['e6', 0.2841198235338181], ['e7', 0.12025416723111781], ['e8', 0.5017739029817114], ['e9', 0.16168802257227524]], 'object_sem': [['e0', 0.19218652956484994], ['e1', 0.34196674807656574], ['e2', 0.4959338481005764], ['e3', 0.29629760310834435], ['e4', 0.3641010486206996], ['e5', 0.08188803904905072], ['e6', 0.2841198235338181], ['e7', 0.12025416723111781], ['e8', 0.5017739029817114], ['e9', 0.16168802257227524]]}, {'higher_order': False, 'pred_name': 'roar', 'P': 'non_exist', 'object_name': 'obj', 'pred_sem': [['e0', 0.1746791843365203], ['e1', 0.16129987338870402], ['e2', 0.13409054880431734], ['e3', 0.12900118178397793], ['e4', 0.06536380657162766], ['e5', 0.49770399382768915], ['e6', 0.22128146288229986], ['e7', 0.4405445098614558], ['e8', 0.42902097099099035], ['e9', 0.4793352855861866]], 'object_sem': []}], 'name': 'P06', 'analog': 0}\n",
      "{'set': 'memory', 'RBs': [{'higher_order': False, 'pred_name': 'angry', 'P': 'non_exist', 'object_name': 'snakes', 'pred_sem': [['e0', 0.062478680412611086], ['e1', 0.40827700036462977], ['e2', 0.09266280690410277], ['e3', 0.21451880316470742], ['e4', 0.4524263948680734], ['e5', 0.21832836148614468], ['e6', 0.21997987657710175], ['e7', 0.3158841691652269], ['e8', 0.3661433864030771], ['e9', 0.4901119695898595]], 'object_sem': [['e0', 0.062478680412611086], ['e1', 0.40827700036462977], ['e2', 0.09266280690410277], ['e3', 0.21451880316470742], ['e4', 0.4524263948680734], ['e5', 0.21832836148614468], ['e6', 0.21997987657710175], ['e7', 0.3158841691652269], ['e8', 0.3661433864030771], ['e9', 0.4901119695898595]]}, {'higher_order': False, 'pred_name': 'hiss', 'P': 'non_exist', 'object_name': 'obj', 'pred_sem': [['e0', 0.39333388525750973], ['e1', 0.07816834520935312], ['e2', 0.13098932632929042], ['e3', 0.44585060987185393], ['e4', 0.28754757043424883], ['e5', 0.3771786285949282], ['e6', 0.0], ['e7', 0.024106989528450556], ['e8', 0.34824159138712735], ['e9', 0.5257723067381966]], 'object_sem': []}], 'name': 'P07', 'analog': 0}\n",
      "{'set': 'memory', 'RBs': [{'higher_order': False, 'pred_name': 'non_exist', 'P': 'non_exist', 'object_name': 'deer', 'pred_sem': [], 'object_sem': [['e0', 0.34354446060193616], ['e1', 0.021189300298392118], ['e2', 0.4145233617487928], ['e3', 0.08446632099081995], ['e4', 0.3500276620658219], ['e5', 0.065748604412161], ['e6', 0.5252944112629603], ['e7', 0.13127252661058425], ['e8', 0.31269639540088845], ['e9', 0.42985586734206294]]}, {'higher_order': False, 'pred_name': 'are', 'P': 'non_exist', 'object_name': 'beautiful', 'pred_sem': [['e0', 0.22701875637813881], ['e1', 0.48091478011076766], ['e2', 0.10177337746003438], ['e3', 0.15244806097304206], ['e4', 0.18833276273355185], ['e5', 0.14366924288612984], ['e6', 0.40444794945459767], ['e7', 0.46085036543628477], ['e8', 0.00700691203205826], ['e9', 0.5014627065317371]], 'object_sem': [['e0', 0.053832901974471825], ['e1', 0.38198470148249275], ['e2', 0.2562748977983582], ['e3', 0.032484891238540446], ['e4', 0.19984470844496968], ['e5', 0.4046948434660573], ['e6', 0.5580660603327611], ['e7', 0.005098914367816253], ['e8', 0.5188702363180729], ['e9', 0.007193227140523793]]}], 'name': 'P09', 'analog': 0}\n",
      "{'set': 'memory', 'RBs': [{'higher_order': False, 'pred_name': 'non_exist', 'P': 'non_exist', 'object_name': 'snakes', 'pred_sem': [], 'object_sem': [['e0', 0.062478680412611086], ['e1', 0.40827700036462977], ['e2', 0.09266280690410277], ['e3', 0.21451880316470742], ['e4', 0.4524263948680734], ['e5', 0.21832836148614468], ['e6', 0.21997987657710175], ['e7', 0.3158841691652269], ['e8', 0.3661433864030771], ['e9', 0.4901119695898595]]}, {'higher_order': False, 'pred_name': 'are', 'P': 'non_exist', 'object_name': 'slender', 'pred_sem': [['e0', 0.22701875637813881], ['e1', 0.48091478011076766], ['e2', 0.10177337746003438], ['e3', 0.15244806097304206], ['e4', 0.18833276273355185], ['e5', 0.14366924288612984], ['e6', 0.40444794945459767], ['e7', 0.46085036543628477], ['e8', 0.00700691203205826], ['e9', 0.5014627065317371]], 'object_sem': [['e0', 0.4098948470503008], ['e1', 0.16156901202032645], ['e2', 0.555337419406145], ['e3', 0.16991720226636642], ['e4', 0.4281260389247033], ['e5', 0.21379430082260847], ['e6', 0.19370790944476937], ['e7', 0.16008450863874904], ['e8', 0.33850338500351995], ['e9', 0.24874868779891415]]}], 'name': 'P010', 'analog': 0}\n",
      "{'set': 'memory', 'RBs': [{'higher_order': False, 'pred_name': 'non_exist', 'P': 'non_exist', 'object_name': 'eagles', 'pred_sem': [], 'object_sem': [['e0', 0.37863832935513814], ['e1', 0.30967993457406107], ['e2', 0.3905399830397763], ['e3', 0.23481996297454166], ['e4', 0.19767246865741406], ['e5', 0.1823301799104201], ['e6', 0.2120849302132264], ['e7', 0.5643461669399337], ['e8', 0.15655832685502033], ['e9', 0.3045876515208896]]}, {'higher_order': False, 'pred_name': 'are', 'P': 'non_exist', 'object_name': 'majestic', 'pred_sem': [['e0', 0.22701875637813881], ['e1', 0.48091478011076766], ['e2', 0.10177337746003438], ['e3', 0.15244806097304206], ['e4', 0.18833276273355185], ['e5', 0.14366924288612984], ['e6', 0.40444794945459767], ['e7', 0.46085036543628477], ['e8', 0.00700691203205826], ['e9', 0.5014627065317371]], 'object_sem': [['e0', 0.34713777113601124], ['e1', 0.3656112477713258], ['e2', 0.023405579286257597], ['e3', 0.4228851380461117], ['e4', 0.26553084728960596], ['e5', 0.4114460114531741], ['e6', 0.2204525626549422], ['e7', 0.31053435923631106], ['e8', 0.07204710291317651], ['e9', 0.42003329075578244]]}], 'name': 'P011', 'analog': 0}\n",
      "{'set': 'memory', 'RBs': [{'higher_order': False, 'pred_name': 'non_exist', 'P': 'non_exist', 'object_name': 'kids', 'pred_sem': [], 'object_sem': [['e0', 0.4552546236347022], ['e1', 0.2905279842630286], ['e2', 0.009043270663040599], ['e3', 0.13813908443770295], ['e4', 0.4835286682158543], ['e5', 0.2509966255823838], ['e6', 0.14802370850606153], ['e7', 0.5754133849163571], ['e8', 0.13319255224591253], ['e9', 0.14704221771588913]]}, {'higher_order': False, 'pred_name': 'like', 'P': 'non_exist', 'object_name': 'jeans', 'pred_sem': [['e0', 0.1444923662786079], ['e1', 0.4526472147767426], ['e2', 0.28121868285677665], ['e3', 0.3707123077308907], ['e4', 0.1740491743139847], ['e5', 0.013034323510384534], ['e6', 0.6364532530127176], ['e7', 0.3386248200286893], ['e8', 0.0571898041111767], ['e9', 0.06517436595965345]], 'object_sem': [['e0', 0.4798878324215645], ['e1', 0.07380340711139756], ['e2', 0.025215131648029857], ['e3', 0.43786784791305583], ['e4', 0.4017832186547394], ['e5', 0.2123375000356374], ['e6', 0.0005679765115312907], ['e7', 0.0], ['e8', 0.3268391874008882], ['e9', 0.5084835538739737]]}], 'name': 'P10', 'analog': 1}\n",
      "{'set': 'memory', 'RBs': [{'higher_order': False, 'pred_name': 'non_exist', 'P': 'non_exist', 'object_name': 'chefs', 'pred_sem': [], 'object_sem': [['e0', 0.30985104159164484], ['e1', 0.3672723004956025], ['e2', 0.4577688678340244], ['e3', 0.12385393841257483], ['e4', 0.4033354870093857], ['e5', 0.025593104888044747], ['e6', 0.20194902730721892], ['e7', 0.34214228482912473], ['e8', 0.13104003383731036], ['e9', 0.453718513650779]]}, {'higher_order': False, 'pred_name': 'wear', 'P': 'non_exist', 'object_name': 'hats', 'pred_sem': [['e0', 0.34038208012017707], ['e1', 0.37855193991792996], ['e2', 0.08374585363802778], ['e3', 0.4555069470848107], ['e4', 0.40876379935573215], ['e5', 0.21622950737384805], ['e6', 0.18686171862306777], ['e7', 0.4950362260494275], ['e8', 0.162768043208137], ['e9', 0.07761362684892356]], 'object_sem': [['e0', 0.6592590180934566], ['e1', 0.3225394266197766], ['e2', 0.03885290828550406], ['e3', 0.2515837842891815], ['e4', 0.08550220604291325], ['e5', 0.18658868779728646], ['e6', 0.09533635094625656], ['e7', 0.48407445341710725], ['e8', 0.06659830258733167], ['e9', 0.3264406845297924]]}], 'name': 'P11', 'analog': 1}\n",
      "{'set': 'memory', 'RBs': [{'higher_order': False, 'pred_name': 'non_exist', 'P': 'non_exist', 'object_name': 'friends', 'pred_sem': [], 'object_sem': [['e0', 0.42023914917906163], ['e1', 0.25627575215347254], ['e2', 0.10295485404430894], ['e3', 0.33552796376106336], ['e4', 0.2272244469452891], ['e5', 0.48626546263135634], ['e6', 0.3177538307284633], ['e7', 0.33396764169099463], ['e8', 0.35665149033040927], ['e9', 0.08219405262341646]]}, {'higher_order': False, 'pred_name': 'drink', 'P': 'non_exist', 'object_name': 'beer', 'pred_sem': [['e0', 0.0], ['e1', 0.0], ['e2', 0.2534052087007959], ['e3', 0.011162879585995737], ['e4', 0.6115333098231172], ['e5', 0.5575155299520163], ['e6', 0.08804328018409675], ['e7', 0.445883512238415], ['e8', 0.1867163363751009], ['e9', 0.0971489537811928]], 'object_sem': [['e0', 0.30637515059180936], ['e1', 0.5026935199458092], ['e2', 0.3512478450094192], ['e3', 0.38809576929302586], ['e4', 0.2443095997017018], ['e5', 0.12103628769485483], ['e6', 0.28954965021751394], ['e7', 0.009327695203248313], ['e8', 0.4330304608810057], ['e9', 0.1834714364754603]]}], 'name': 'P12', 'analog': 1}\n",
      "{'set': 'memory', 'RBs': [{'higher_order': False, 'pred_name': 'non_exist', 'P': 'non_exist', 'object_name': 'kings', 'pred_sem': [], 'object_sem': [['e0', 0.1464494323908714], ['e1', 0.10458834543087714], ['e2', 0.5110721749011595], ['e3', 0.41956792798093295], ['e4', 0.005554064225384257], ['e5', 0.5125366662332265], ['e6', 0.24819494918721638], ['e7', 0.07220411358475023], ['e8', 0.2991765212804259], ['e9', 0.3336710776124556]]}, {'higher_order': False, 'pred_name': 'give', 'P': 'non_exist', 'object_name': 'speeches', 'pred_sem': [['e0', 0.12230256454732408], ['e1', 0.10306773582479907], ['e2', 0.36642357391266], ['e3', 0.40393321078587596], ['e4', 0.3179502481048498], ['e5', 0.20449089560857428], ['e6', 0.34812761289208866], ['e7', 0.46649981657740835], ['e8', 0.2012180097317316], ['e9', 0.39341881299774206]], 'object_sem': [['e0', 0.6133591773853311], ['e1', 0.4172400198664153], ['e2', 0.1904388968215052], ['e3', 0.2681127157981841], ['e4', 0.3501364629095889], ['e5', 0.05000138105283601], ['e6', 0.3358276154242907], ['e7', 0.11293703301494101], ['e8', 0.2443097359639406], ['e9', 0.17672576208128532]]}], 'name': 'P13', 'analog': 1}\n",
      "{'set': 'memory', 'RBs': [{'higher_order': False, 'pred_name': 'old', 'P': 'non_exist', 'object_name': 'jeans', 'pred_sem': [['e0', 0.4798878324215645], ['e1', 0.07380340711139756], ['e2', 0.025215131648029857], ['e3', 0.43786784791305583], ['e4', 0.4017832186547394], ['e5', 0.2123375000356374], ['e6', 0.0005679765115312907], ['e7', 0.0], ['e8', 0.3268391874008882], ['e9', 0.5084835538739737]], 'object_sem': [['e0', 0.4798878324215645], ['e1', 0.07380340711139756], ['e2', 0.025215131648029857], ['e3', 0.43786784791305583], ['e4', 0.4017832186547394], ['e5', 0.2123375000356374], ['e6', 0.0005679765115312907], ['e7', 0.0], ['e8', 0.3268391874008882], ['e9', 0.5084835538739737]]}, {'higher_order': False, 'pred_name': 'fade', 'P': 'non_exist', 'object_name': 'obj', 'pred_sem': [['e0', 0.37040284158653425], ['e1', 0.29628707268249216], ['e2', 0.21412320877924132], ['e3', 0.5115592228272592], ['e4', 0.0229944234148856], ['e5', 0.3073509282620138], ['e6', 0.4248939763169253], ['e7', 0.30204760270517506], ['e8', 0.009020465459489358], ['e9', 0.3172251057043733]], 'object_sem': []}], 'name': 'P14', 'analog': 1}\n",
      "{'set': 'memory', 'RBs': [{'higher_order': False, 'pred_name': 'young', 'P': 'non_exist', 'object_name': 'kids', 'pred_sem': [['e0', 0.4552546236347022], ['e1', 0.2905279842630286], ['e2', 0.009043270663040599], ['e3', 0.13813908443770295], ['e4', 0.4835286682158543], ['e5', 0.2509966255823838], ['e6', 0.14802370850606153], ['e7', 0.5754133849163571], ['e8', 0.13319255224591253], ['e9', 0.14704221771588913]], 'object_sem': [['e0', 0.4552546236347022], ['e1', 0.2905279842630286], ['e2', 0.009043270663040599], ['e3', 0.13813908443770295], ['e4', 0.4835286682158543], ['e5', 0.2509966255823838], ['e6', 0.14802370850606153], ['e7', 0.5754133849163571], ['e8', 0.13319255224591253], ['e9', 0.14704221771588913]]}, {'higher_order': False, 'pred_name': 'play', 'P': 'non_exist', 'object_name': 'obj', 'pred_sem': [['e0', 0.43728279139357423], ['e1', 0.4823300957853745], ['e2', 0.26490212050887924], ['e3', 0.1586477565967399], ['e4', 0.3676232378160212], ['e5', 0.09276197538089868], ['e6', 0.5019061491898502], ['e7', 0.2821438643173313], ['e8', 0.03183793953363583], ['e9', 0.06722327504879974]], 'object_sem': []}], 'name': 'P15', 'analog': 1}\n",
      "{'set': 'memory', 'RBs': [{'higher_order': False, 'pred_name': 'tired', 'P': 'non_exist', 'object_name': 'chefs', 'pred_sem': [['e0', 0.30985104159164484], ['e1', 0.3672723004956025], ['e2', 0.4577688678340244], ['e3', 0.12385393841257483], ['e4', 0.4033354870093857], ['e5', 0.025593104888044747], ['e6', 0.20194902730721892], ['e7', 0.34214228482912473], ['e8', 0.13104003383731036], ['e9', 0.453718513650779]], 'object_sem': [['e0', 0.30985104159164484], ['e1', 0.3672723004956025], ['e2', 0.4577688678340244], ['e3', 0.12385393841257483], ['e4', 0.4033354870093857], ['e5', 0.025593104888044747], ['e6', 0.20194902730721892], ['e7', 0.34214228482912473], ['e8', 0.13104003383731036], ['e9', 0.453718513650779]]}, {'higher_order': False, 'pred_name': 'cook', 'P': 'non_exist', 'object_name': 'obj', 'pred_sem': [['e0', 0.3043617610254985], ['e1', 0.12694945676089808], ['e2', 0.0], ['e3', 0.41106500671361973], ['e4', 0.4428030804381603], ['e5', 0.0], ['e6', 0.39111145531265457], ['e7', 0.415120106842891], ['e8', 0.21669145107200746], ['e9', 0.3923655024512858]], 'object_sem': []}], 'name': 'P16', 'analog': 1}\n",
      "{'set': 'memory', 'RBs': [{'higher_order': False, 'pred_name': 'old', 'P': 'non_exist', 'object_name': 'friends', 'pred_sem': [['e0', 0.42023914917906163], ['e1', 0.25627575215347254], ['e2', 0.10295485404430894], ['e3', 0.33552796376106336], ['e4', 0.2272244469452891], ['e5', 0.48626546263135634], ['e6', 0.3177538307284633], ['e7', 0.33396764169099463], ['e8', 0.35665149033040927], ['e9', 0.08219405262341646]], 'object_sem': [['e0', 0.42023914917906163], ['e1', 0.25627575215347254], ['e2', 0.10295485404430894], ['e3', 0.33552796376106336], ['e4', 0.2272244469452891], ['e5', 0.48626546263135634], ['e6', 0.3177538307284633], ['e7', 0.33396764169099463], ['e8', 0.35665149033040927], ['e9', 0.08219405262341646]]}, {'higher_order': False, 'pred_name': 'bond', 'P': 'non_exist', 'object_name': 'obj', 'pred_sem': [['e0', 0.34944887696591653], ['e1', 0.06389928037765472], ['e2', 0.3611780686605596], ['e3', 0.48163852532985574], ['e4', 0.4920863715201077], ['e5', 0.0497072079213674], ['e6', 0.09077340547005573], ['e7', 0.47890195273974207], ['e8', 0.0], ['e9', 0.17079345115505656]], 'object_sem': []}], 'name': 'P17', 'analog': 1}\n",
      "{'set': 'memory', 'RBs': [{'higher_order': False, 'pred_name': 'non_exist', 'P': 'non_exist', 'object_name': 'hats', 'pred_sem': [], 'object_sem': [['e0', 0.6592590180934566], ['e1', 0.3225394266197766], ['e2', 0.03885290828550406], ['e3', 0.2515837842891815], ['e4', 0.08550220604291325], ['e5', 0.18658868779728646], ['e6', 0.09533635094625656], ['e7', 0.48407445341710725], ['e8', 0.06659830258733167], ['e9', 0.3264406845297924]]}, {'higher_order': False, 'pred_name': 'are', 'P': 'non_exist', 'object_name': 'protective', 'pred_sem': [['e0', 0.22701875637813881], ['e1', 0.48091478011076766], ['e2', 0.10177337746003438], ['e3', 0.15244806097304206], ['e4', 0.18833276273355185], ['e5', 0.14366924288612984], ['e6', 0.40444794945459767], ['e7', 0.46085036543628477], ['e8', 0.00700691203205826], ['e9', 0.5014627065317371]], 'object_sem': [['e0', 0.3058696725167198], ['e1', 0.13716541673539467], ['e2', 0.04479384116504364], ['e3', 0.3292424335271447], ['e4', 0.19484428965766806], ['e5', 0.04705819129852977], ['e6', 0.485043109679756], ['e7', 0.6309629179233341], ['e8', 0.2592095244370501], ['e9', 0.19097893460697876]]}], 'name': 'P19', 'analog': 1}\n",
      "{'set': 'memory', 'RBs': [{'higher_order': False, 'pred_name': 'non_exist', 'P': 'non_exist', 'object_name': 'friends', 'pred_sem': [], 'object_sem': [['e0', 0.42023914917906163], ['e1', 0.25627575215347254], ['e2', 0.10295485404430894], ['e3', 0.33552796376106336], ['e4', 0.2272244469452891], ['e5', 0.48626546263135634], ['e6', 0.3177538307284633], ['e7', 0.33396764169099463], ['e8', 0.35665149033040927], ['e9', 0.08219405262341646]]}, {'higher_order': False, 'pred_name': 'are', 'P': 'non_exist', 'object_name': 'necessary', 'pred_sem': [['e0', 0.22701875637813881], ['e1', 0.48091478011076766], ['e2', 0.10177337746003438], ['e3', 0.15244806097304206], ['e4', 0.18833276273355185], ['e5', 0.14366924288612984], ['e6', 0.40444794945459767], ['e7', 0.46085036543628477], ['e8', 0.00700691203205826], ['e9', 0.5014627065317371]], 'object_sem': [['e0', 0.3953365069372795], ['e1', 0.13653136753537606], ['e2', 0.42265785542922596], ['e3', 0.28970857101874603], ['e4', 0.4334199135941706], ['e5', 0.35736129235665043], ['e6', 0.3147653764961834], ['e7', 0.20029894712962976], ['e8', 0.1081999939360674], ['e9', 0.3098926413440841]]}], 'name': 'P110', 'analog': 1}\n",
      "{'set': 'memory', 'RBs': [{'higher_order': False, 'pred_name': 'non_exist', 'P': 'non_exist', 'object_name': 'kings', 'pred_sem': [], 'object_sem': [['e0', 0.1464494323908714], ['e1', 0.10458834543087714], ['e2', 0.5110721749011595], ['e3', 0.41956792798093295], ['e4', 0.005554064225384257], ['e5', 0.5125366662332265], ['e6', 0.24819494918721638], ['e7', 0.07220411358475023], ['e8', 0.2991765212804259], ['e9', 0.3336710776124556]]}, {'higher_order': False, 'pred_name': 'are', 'P': 'non_exist', 'object_name': 'respected', 'pred_sem': [['e0', 0.22701875637813881], ['e1', 0.48091478011076766], ['e2', 0.10177337746003438], ['e3', 0.15244806097304206], ['e4', 0.18833276273355185], ['e5', 0.14366924288612984], ['e6', 0.40444794945459767], ['e7', 0.46085036543628477], ['e8', 0.00700691203205826], ['e9', 0.5014627065317371]], 'object_sem': [['e0', 0.3342555442941809], ['e1', 0.2320561810016932], ['e2', 0.35285081607100244], ['e3', 0.34892930978942566], ['e4', 0.4701184661025868], ['e5', 0.15107501585032668], ['e6', 0.3698821719428776], ['e7', 0.2547051969609201], ['e8', 0.15703384524650582], ['e9', 0.3434902891168864]]}], 'name': 'P111', 'analog': 1}\n"
     ]
    }
   ],
   "source": [
    "for prop in symProps:\n",
    "    print(prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert symProps dict to text and write to python file\n",
    "test_sim_path = str(os.path.dirname(os.getcwd())) + '\\\\test_sim_8.py'\n",
    "write_file = open(test_sim_path, 'w')\n",
    "simType = 'sym_file'\n",
    "write_file.write(\"simType='sym_file'\\n\")\n",
    "write_file.write(\"symProps=\")\n",
    "write_file.write(str(symProps))\n",
    "write_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7863155084508869, 0.7320703115823899, 0.7571116311439076, 0.7073810089605608)\n"
     ]
    }
   ],
   "source": [
    "# tests : \n",
    "# hungry dogs bark -- deer are beautiful\n",
    "# hungry dogs is closer to are beautiful, than hungry dogs to happy cats\n",
    "# compare with word embedding distances\n",
    "print(np.dot(reduced_embeddings['hungry'],reduced_embeddings['happy']),\n",
    "np.dot(reduced_embeddings['hungry'],reduced_embeddings['are']),\n",
    "np.dot(reduced_embeddings['dogs'],reduced_embeddings['cats']),\n",
    "np.dot(reduced_embeddings['dogs'],reduced_embeddings['beautiful']),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('beautiful', ' : cats', 0.7426756152210802)\n",
      "('rats', ' : cats', 0.8601765094249539)\n",
      "('snakes', ' : cats', 0.8013988418599967)\n",
      "('deer', ' : cats', 0.8112851480358855)\n",
      "('worms', ' : cats', 0.6323906426647122)\n",
      "('sleep', ' : cats', 0.8041627339487587)\n",
      "('are', ' : cats', 0.8168678334121136)\n",
      "('majestic', ' : cats', 0.8479897601066844)\n",
      "('bark', ' : cats', 0.7853944977763743)\n",
      "('growl', ' : cats', 0.6732571596210443)\n",
      "('bite', ' : cats', 0.730776606365893)\n",
      "('cats', ' : cats', 1.0000000000000002)\n",
      "('lions', ' : cats', 0.7592189923185823)\n",
      "('hiss', ' : cats', 0.7654151056206537)\n",
      "('eagles', ' : cats', 0.8859131628666905)\n",
      "('dogs', ' : cats', 0.7571116311439076)\n",
      "('happy', ' : cats', 0.8563080085792507)\n",
      "('crawl', ' : cats', 0.7828140919851585)\n",
      "('eat', ' : cats', 0.7897654059165037)\n",
      "('hunt', ' : cats', 0.7169727508631525)\n",
      "('hungry', ' : cats', 0.8213940998429217)\n",
      "('slender', ' : cats', 0.8534662912461877)\n",
      "('small', ' : cats', 0.8467747922721409)\n",
      "('angry', ' : cats', 0.7705176790415731)\n"
     ]
    }
   ],
   "source": [
    "for word in unique_words:\n",
    "    print(word, ' : cats', np.dot(reduced_embeddings['cats'],reduced_embeddings[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
